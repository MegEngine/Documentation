.. _deployment:

=====================
将模型部署到 C++ 环境
=====================
.. note::

   MegEngine 的一大核心优势是 “训练推理一体化” ——

   * 其中 “训练” 在 Python 环境中进行，而 “推理” 特指在 C++ 环境下使用训练完成的模型进行推理；
   * 将模型迁移到无需依赖 Python 的环境中，使其能正常进行推理计算，被称为 “部署” ；
   * 部署的目的是简化除了模型推理所必需的一切其它依赖，使推理计算的耗时变得尽可能少。
     比如手机人脸识别场景下会需求毫秒级的优化，而这必须依赖于 C++ 环境才能实现。 😉


目前不再使用 MegEngine 的 C++ 接口直接作为推理框架，使用 MegEngine 的 C++ 接口作为推理框架有以下缺点：

* 接口过于复杂，MegEngine 的 C++ 接口分散在各个文件夹下面，内容和目录结构都很复杂。
* MegEngine 的 C++ 接口对于推理暴露了很多内部的概念，不单纯。
* MegEngine 的很多优化的功能使用 MegEngine 的 C++ 接口过于复杂。

所以目前推理都推荐使用 MegEngine Lite，使用 MegEngine Lite 参考 :ref:`lite-quick-start-cpp` 。
