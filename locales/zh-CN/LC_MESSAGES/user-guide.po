# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2021, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.3.0.dev\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-08 18:21+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/user-guide/advanced-parameter-optimization.rst:5
msgid "参数优化进阶配置"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:8
msgid "更细粒度的参数优化设置"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:10
msgid "在 :ref:`train_and_evaluation` 中网络使用如下优化器进行训练："
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:20
msgid "这个优化器对所有参数都使用同一学习速率进行优化，本章将介绍如何做到对不同的参数采用不同的学习速率。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:22
msgid ""
"本章沿用 :ref:`network_build` 中创建的 ``LeNet`` ，下述的优化器相关代码可以用于取代 "
":ref:`train_and_evaluation` 中对应的代码。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:25
msgid "不同参数使用不同的学习速率"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:27
msgid ""
":class:`~.megengine.optimizer.optimizer.Optimizer` "
"支持将网络的参数进行分组，不同的参数组可以采用不同的学习速率进行训练。 一个参数组由一个字典表示，这个字典中有"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:29
msgid "``'params': param_list``，用来指定参数组包含的参数。此键值对必须有。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:31
msgid "``'lr': learning_rate``，用来指定此参数组的学习速率。此键值对有时可省略，省略后参数组的学习速率由优化器指定。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:33
msgid ""
"所有待优化参数组的字典会组成一个列表作为 :class:`~.megengine.optimizer.optimizer.Optimizer` "
"实例化时的第一个参数传入。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:35
msgid ""
"为了更好的说明参数组，我们首先使用 :class:`~.megengine.module.module.Module` 提供的 "
":meth:`~.megengine.module.module.Module.named_parameters` "
"函数来对网络参数进行分组。这个函数返回一个包含网络所有参数并且以参数名字为键、参数变量为值的字典："
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:55
msgid "根据参数的名字我们可以将 ``LeNet`` 中所有卷积的参数分为一组，所有全连接层的参数分为另一组："
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:68
msgid "分组后即可根据下述代码对不同参数组设置不同的学习速率："
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:83
msgid ""
"优化器中设置的参数组列表对应于 :attr:`~.megengine.optimizer.Optimizer.param_groups` "
"属性。我们可以通过其获取不同参数组的学习速率。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:97
msgid "训练中对学习速率的更改"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:99
msgid ""
"MegEngine "
"也支持在训练过程中对学习速率进行修改，比如部分参数训练到一定程度后就不再需要优化，此时将对应参数组的学习速率设为零即可。我们修改 "
":ref:`train_and_evaluation` "
"中的训练代码进行示例说明。修改后的训练代码总共训练四个epoch，我们会在第二个epoch结束时将所有全连接层参数的学习速率置零，并在每个epoch当中输出"
" ``LeNet`` 中全连接层的部分参数值以显示是否被更新。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:129
msgid "从输出可以看到在学习速率设为0之前参数值是在不断更新的，但是在设为0之后参数值就不再变化。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:131
msgid "同时多数网络在训练当中会不断减小学习速率，如下代码展示了 MegEngine 是如何在训练过程中线性减小学习速率的："
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:144
msgid "不同参数使用不同的优化器"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:146
msgid ""
"对于不同的参数，也可以使用不同的优化器对它们分别优化。对参数的梯度置零（ "
":meth:`~.megengine.optimizer.optimizer.Optimizer.clear_grad` ）和更新（ "
":meth:`~.megengine.optimizer.optimizer.Optimizer.step` "
"）操作，如果所有优化器都是同时进行的，可以定义一个 ``MultipleOptimizer`` "
"类。在初始化时声明多个不同的优化器，在调用置零函数和更新函数时对所有优化器执行对应操作。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:162
msgid ""
"假设想用 :class:`~.megengine.optimizer.sgd.SGD` 优化所有卷积参数，用 "
":class:`~.megengine.optimizer.adam.Adam` "
"优化所有全连接层参数。可以按照如下方式定义优化器，不需要改变训练代码就可以达到不同的参数使用不同的优化器优化的效果。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:170
msgid "如果不同的参数梯度置零和更新不是同时进行的，你只需要定义多个优化器，在不同的时间调用对应的函数即可。"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:173
msgid "固定部分参数不优化"
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:175
msgid ""
"除了将不训练的参数分为一组并将学习速率设为零外，MegEngine "
"还提供了其他途径来固定参数不进行优化：仅将需要优化的参数与求导器和优化器绑定即可。如下代码所示，仅对 ``LeNet`` 中的卷积参数进行优化："
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:195
msgid "下述代码将上面的设置加入到了具体训练当中，能够更加直观的看到各个参数的梯度差异："
msgstr ""

#: ../../source/user-guide/advanced-parameter-optimization.rst:235
msgid "从输出可以看到除了卷积参数有梯度外其余参数均没有梯度也就不会更新。"
msgstr ""

#: ../../source/user-guide/codegen.rst:5
msgid "使用 codegen 减少访存操作"
msgstr ""

#: ../../source/user-guide/codegen.rst:8
msgid "如何使用 MegEngine 的 codegen"
msgstr ""

#: ../../source/user-guide/codegen.rst:10
msgid ""
"通常，模型中不仅含有计算受限的操作，还含有一些访存受限操作(如 Elemwsie)。MegEngine 内嵌了 codegen "
"优化机制，它可以在运行时将模型中多个操作融合起来并生成可以在目标机器上运行的代码，以此减少访存操作从而达到加速的目的。"
msgstr ""

#: ../../source/user-guide/codegen.rst:13
msgid "打开 codegen"
msgstr ""

#: ../../source/user-guide/codegen.rst:15
msgid ""
"我们在 :class:`~.megengine.jit.tracing.trace` 接口中传入 ``symbolic=True, "
"opt_level=3`` ，即可打开 MegEngine codegen 优化。"
msgstr ""

#: ../../source/user-guide/codegen.rst:19
msgid "指定 codegen 的后端"
msgstr ""

#: ../../source/user-guide/codegen.rst:21
msgid ""
"MegEngine 的 codegen 目前集成了三种后端，分别是 NVRTC, HALIDE 和 MLIR。其中 NVRTC 和 HALIDE "
"仅支持在 GPU 上使用，MLIR 则同时支持 GPU 和 CPU, 不同的后端生成代码的策略有所不同，所以运行效率也各异。"
msgstr ""

#: ../../source/user-guide/codegen.rst:23
msgid "我们可以通过设置如下的环境变量来改变 codegen 的后端，例如想要使用 NVRTC 后端，可以："
msgstr ""

#: ../../source/user-guide/codegen.rst:29
msgid ""
"该环境变量在 NVIDIA GPU 环境下可取的值为 NVRTC, HALIDE 和 ``MLIR``, 默认值为 HALIDE 。CPU "
"暂时仅支持 ``MLIR`` 后端。 (如果使用 ``MLIR`` 后端, 需要单独编译 MegEngine)"
msgstr ""

#: ../../source/user-guide/codegen.rst:33
msgid "使用 codegen 的 MLIR 后端"
msgstr ""

#: ../../source/user-guide/codegen.rst:35
msgid "由于 MLIR 默认关闭，所以需要源码编译安装 MegEngine, 详见 github 主页, cmake 时换成如下命令："
msgstr ""

#: ../../source/user-guide/codegen.rst:41
msgid "然后设置如下的环境变量："
msgstr ""

#: ../../source/user-guide/codegen.rst:48
msgid "代码示例"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:5
msgid "部署你的模型"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:8
msgid "模型部署"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:10
msgid ""
"MegEngine 的一大核心优势是“训练推理一体化”，其中“训练”是在 Python 环境中进行的，而“推理”则特指在 C++ "
"环境下使用训练完成的模型进行推理。而将模型迁移到无需依赖 Python 的环境中，使其能正常进行推理计算，被称为 **部署** "
"。部署的目的是简化除了模型推理所必需的一切其它依赖，使推理计算的耗时变得尽可能少，比如手机人脸识别场景下会需求毫秒级的优化，而这必须依赖于 C++"
" 环境才能实现。"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:12
msgid "本章从一个训练好的异或网络模型（见 `MegStudio 项目 `_ ）出发，讲解如何将其部署到 CPU（X86）环境下运行。主要分为以下步骤："
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:14
msgid "将模型序列化并导出到文件，详细介绍可见 :ref:`trace_and_dump`；"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:15
msgid "编写读取模型的 C++ 脚本；"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:16
msgid "编译 C++ 脚本成可执行文件。"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:19
#: ../../source/user-guide/dump.rst:8
msgid "模型序列化"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:21
msgid "这里我们使用 xor-deploy 的例子，模型定义与序列化代码可见 `xornet.py `_ 。"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:24
msgid "编写 C++ 程序读取模型"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:26
msgid ""
"接下来我们需要编写一个 C++ "
"程序，来实现我们期望在部署平台上完成的功能。在这里我们基于上面导出的异或网络模型，实现一个最简单的功能，即给定两个浮点数，输出对其做异或操作，结果为"
" 0 的概率以及为 1 的概率。"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:28
msgid ""
"在此之前，为了能够正常使用 MegEngine 底层 C++ 接口，需要先按照 MegeEngine 中提供的编译脚本( "
"`MegEngine/scripts `_ )从源码编译得到 MegEngine 的相关库, "
"通过这些脚本可以交叉编译安卓（ARMv7，ARMv8，ARMv8.2）版本、linux 版本（ARMv7，ARMv8，ARMv8.2）以及 ios"
" 相关库，也可以本机编译 windows/linux/macos 相关库文件。"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:30
msgid "实现上述异或计算的示例 C++ 代码如下（引自 `xor-deploy.cpp `_ ）："
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:35
msgid ""
"简单解释一下代码的意思，我们首先通过 "
":ref:`exhale_class_classmgb_1_1serialization_1_1GraphLoader` 将模型加载进来，接着通过"
" ``tensor_map`` 和上节指定的输入名称 ``data`` ，找到模型的输入指针，再将运行时提供的输入 ``x`` 和 ``y`` "
"赋值给输入指针，然后我们使用 ``network.graph->compile`` 将模型编译成一个函数接口，并调用执行，最后将得到的结果 "
"``predict`` 进行输出，该输出的两个值即为异或结果为 0 的概率以及为 1 的概率 。 另外可以配置上面加载模型时候的 "
"``config`` 来优化 inference 计算效率，为了加速一般在 ARM 上面配置 ``enable_nchw44_layout()``"
" ,在x86 CPU上面配置 ``enable_nchw88_layout()`` ，具体的配置方法参考 `load_and_run 源码 `_ "
"。"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:39
msgid "编译并执行"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:41
msgid ""
"为了更完整地实现“训练推理一体化”，我们还需要支持同一个 C++ "
"程序能够交叉编译到不同平台上执行，而不需要修改代码。之所以能够实现不同平台一套代码，是由于底层依赖的算子库（内部称作 "
"MegDNN）实现了对不同平台接口的封装，在编译时会自动根据指定的目标平台选择兼容的接口。"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:45
msgid ""
"目前发布的版本我们开放了对 CPU（X86、X64、ARMv7、ARMv8、ARMv8.2）和 GPU（CUDA）平台的 float 和量化 "
"int8 的支持。"
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:47
msgid ""
"我们在这里以 CPU（X86） 平台为例，如果目标平台是ARM则可以参考 :ref:`inference_chinese` 。首先直接使用 gcc"
" 或者 clang （用 ``$CXX`` 指代）进行编译即可："
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:53
msgid ""
"上面的 ``$MGE_INSTALL_PATH`` 指代了编译安装时通过 ``CMAKE_INSTALL_PREFIX`` "
"指定的安装路径。编译完成之后，通过以下命令执行即可："
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:59
msgid ""
"这里将 ``$MGE_INSTALL_PATH/lib`` 加进 ``LD_LIBRARY_PATH`` 环境变量，确保 MegEngine "
"库可以被编译器找到。上面命令对应的输出如下："
msgstr ""

#: ../../source/user-guide/deploy-your-model.rst:65
msgid ""
"至此我们便完成了从 Python 模型到 C++ 可执行文件的部署流程，如果需要快速的运行模型以及测试模型性能，请参考 "
":ref:`how_to_use_load_and_run` 。"
msgstr ""

#: ../../source/user-guide/distribution.rst:5
msgid "分布式训练"
msgstr ""

#: ../../source/user-guide/distribution.rst:7
msgid ""
"本章我们将介绍如何在 MegEngine 中高效地利用多 GPU 进行分布式训练。 分布式训练是指同时利用一台或者多台机器上的 GPU "
"进行并行计算。 在深度学习领域，最常见的并行计算方式是在数据层面进行的， 即每个 GPU 各自负责一部分数据，并需要跑通整个训练和推理流程。 "
"这种方式叫做 **数据并行** 。"
msgstr ""

#: ../../source/user-guide/distribution.rst:13
msgid "目前 MegEngine 开放的接口支持单机多卡和多机多卡的数据并行方式。"
msgstr ""

#: ../../source/user-guide/distribution.rst:17
msgid "单机多卡"
msgstr ""

#: ../../source/user-guide/distribution.rst:19
msgid "单机多卡是最为常用的方式，比如单机四卡、单机八卡，足以支持我们完成大部分模型的训练。"
msgstr ""

#: ../../source/user-guide/distribution.rst:21
msgid "本节我们按照以下顺序进行介绍："
msgstr ""

#: ../../source/user-guide/distribution.rst:23
#: ../../source/user-guide/distribution.rst:29
msgid "如何启动一个单机多卡的训练"
msgstr ""

#: ../../source/user-guide/distribution.rst:24
#: ../../source/user-guide/distribution.rst:51
msgid "数据处理流程"
msgstr ""

#: ../../source/user-guide/distribution.rst:25
msgid "进程间训练状态如何同步"
msgstr ""

#: ../../source/user-guide/distribution.rst:26
msgid "如何在多进程环境中将模型保存与加载"
msgstr ""

#: ../../source/user-guide/distribution.rst:31
msgid "我们提供了一个单机多卡的启动器。代码示例："
msgstr ""

#: ../../source/user-guide/distribution.rst:53
msgid ""
"用 :class:`~.distributed.launcher` 启动之后，我们便可以按照正常的流程进行训练了， "
"但是由于需要每个进程处理不同的数据，我们还需要在数据部分做一些额外的操作。"
msgstr ""

#: ../../source/user-guide/distribution.rst:56
msgid ""
"在这里我们以载入 MNIST 数据为例，展示如何对数据做切分，使得每个进程拿到不重叠的数据。 此处我们将整个数据集载入内存后再进行切分。 "
"这种方式比较低效，仅作为原理示意，更加高效的方式见 :ref:`dist_dataloader` 。"
msgstr ""

#: ../../source/user-guide/distribution.rst:71
msgid "至此我们便得到了每个进程各自负责的、互不重叠的数据部分。"
msgstr ""

#: ../../source/user-guide/distribution.rst:74
msgid "参数同步"
msgstr ""

#: ../../source/user-guide/distribution.rst:76
msgid "初始化模型的参数之后，我们可以调用 :func:`~.distributed.bcast_list_` 对进程间模型的参数进行广播同步："
msgstr ""

#: ../../source/user-guide/distribution.rst:85
msgid "在反向传播求梯度的步骤中，我们通过插入 callback 函数的形式， 对各个进程计算出的梯度进行累加，各个进程都拿到的是累加后的梯度。代码示例："
msgstr ""

#: ../../source/user-guide/distribution.rst:100
msgid "模型保存与加载"
msgstr ""

#: ../../source/user-guide/distribution.rst:102
msgid "在 MegEngine 中，依赖于上面提到的状态同步机制，我们保持了各个进程状态的一致， 因此可以很容易地实现模型的保存和加载。"
msgstr ""

#: ../../source/user-guide/distribution.rst:105
msgid ""
"对于加载，我们只要在主进程（rank 0 进程）中加载模型参数， 然后调用 :func:`~.distributed.bcast_list_` "
"对各个进程的参数进行同步，就保持了各个进程的状态一致。"
msgstr ""

#: ../../source/user-guide/distribution.rst:108
msgid "对于保存，由于我们在梯度计算中插入了 callback 函数对各个进程的梯度进行累加， 所以我们进行参数更新后的参数还是一致的，可以直接保存。"
msgstr ""

#: ../../source/user-guide/distribution.rst:111
msgid "可以参考以下示例代码实现："
msgstr ""

#: ../../source/user-guide/distribution.rst:135
msgid "使用 DataLoader 进行数据加载"
msgstr ""

#: ../../source/user-guide/distribution.rst:137
msgid ""
"在上一节，为了简单起见，我们将整个数据集全部载入内存。 实际中，我们可以通过 "
":class:`~.megengine.data.DataLoader` 来更高效地加载数据。"
msgstr ""

#: ../../source/user-guide/distribution.rst:140
msgid ""
":class:`~.megengine.data.DataLoader` 会自动帮我们处理分布式训练时数据相关的问题， "
"可以实现使用单卡训练时一样的数据加载代码，具体来说："
msgstr ""

#: ../../source/user-guide/distribution.rst:143
msgid ""
"所有采样器 :class:`~.megengine.data.Sampler` 都会自动地做类似上文中数据切分的操作， "
"使得所有进程都能获取互不重复的数据。"
msgstr ""

#: ../../source/user-guide/distribution.rst:145
msgid ""
"每个进程的 :class:`~.megengine.data.DataLoader` 还会自动调用分布式相关接口实现内存共享， "
"避免不必要的内存占用，从而显著加速数据读取。"
msgstr ""

#: ../../source/user-guide/distribution.rst:148
msgid ""
"总之，在分布式训练时，你无需对使用 :class:`~.megengine.data.DataLoader` 的方式进行任何修改， "
"一切都能无缝地切换。完整的例子见 MegEngine/Models_ 存储库。"
msgstr ""

#: ../../source/user-guide/distribution.rst:154
msgid "多机多卡"
msgstr ""

#: ../../source/user-guide/distribution.rst:156
msgid ""
"在 MegEngine 中，我们能很方便地将上面单机多卡的代码修改为多机多卡， 只需修改传给 "
":func:`~.megengine.distributed.launcher` 的参数就可以进行多机多卡训练"
msgstr ""

#: ../../source/user-guide/distribution.rst:180
msgid ""
"其中 ``world_size`` 是你训练的用到的总卡数， ``n_gpus`` 是你运行时这台物理机的卡数， ``rank_start`` "
"是这台机器的 rank 起始值，``master_ip`` 是 rank 0 所在机器的 ip 地址， ``port`` 是分布式训练 "
"master server 使用的端口号"
msgstr ""

#: ../../source/user-guide/distribution.rst:184
msgid "其它部分与单机版本完全相同。最终只需在每个机器上执行相同的 Python 程序，即可实现多机多卡的分布式训练。"
msgstr ""

#: ../../source/user-guide/distribution.rst:187
msgid "模型并行"
msgstr ""

#: ../../source/user-guide/distribution.rst:189
msgid "在 MegEngine 中，也支持模型并行的方式来做训练。"
msgstr ""

#: ../../source/user-guide/distribution.rst:191
msgid "最简单的模型并行就是把一个模型拆分成上下两个部分来做，在 MegEngine 中可以简单的实现。"
msgstr ""

#: ../../source/user-guide/distribution.rst:193
msgid "下面是一个简单的例子来展示怎么写一个模型并行的训练："
msgstr ""

#: ../../source/user-guide/dump.rst:5
msgid "导出序列化模型"
msgstr ""

#: ../../source/user-guide/dump.rst:10
msgid ""
"MegEngine 依赖 trace 来序列化（:meth:`dump "
"`）一个训练好的模型。并且为了把一些参数（比如卷积层的卷积核等）固化下来，需要在 trace 中多指定一项 ``capture_as_const "
"= True``。之后调用 ``dump`` 函数即可把模型序列化到一个文件或者文件对象中。如："
msgstr ""

#: ../../source/user-guide/dump.rst:23
msgid "``dump`` 函数可接受多个参数，其中最常用的有如下两个。"
msgstr ""

#: ../../source/user-guide/dump.rst:26
msgid "arg_names"
msgstr ""

#: ../../source/user-guide/dump.rst:27
msgid ""
"在序列化的时候统一设置模型输入 Tensor 的名字。由于不同的模型的差异，会导致输入 Tensor 的名字千差万别。 "
"为了减少理解和使用难度，可使用此参数统一设置模型输入为诸如 ``arg_0``, ``arg_1``, ..."
msgstr ""

#: ../../source/user-guide/dump.rst:31
msgid "optimize_for_inference"
msgstr ""

#: ../../source/user-guide/dump.rst:32
msgid ""
"训练出的模型往往在部署时不能发挥最优的性能，而我们提供 ``optimize_for_inference`` "
"来保证序列化出的模型是经过特定优化的。详细的键值参数可见 :meth:`~.megengine.jit.tracing.trace.dump`。 "
"使用上面的例子，通过指定 `enable_io16xc32` 来设置模型输入输出的 Tensor 的精度为 float16，但是运算的 "
"Tensor 精度为 float32。"
msgstr ""

#: ../../source/user-guide/dump.rst:45
msgid ""
"值得注意的是，optimize_for_inference 参数默认是 True， "
"所以即使不给任何键值优化参数，仍然会做一些基础的优化操作，这会导致序列化出来的模型相较之前的定义有细微的差别。"
msgstr ""

#: ../../source/user-guide/index.rst:5
msgid "用户指南"
msgstr ""

#: ../../source/user-guide/index.rst:23
msgid "内容正在建设中..."
msgstr ""

#: ../../source/user-guide/load-and-run.rst:5
msgid "模型正确性、速度验证与性能调试"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:8
msgid "如何使用 load_and_run"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:10
msgid ""
"load_and_run 是 MegEngine 中的加载并运行模型的工具，主要用来做模型正确性验证，速度验证及性能调试，源代码在 `load-"
"and-run `_ 。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:12
msgid "load_and_run 有以下功能："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:14
msgid "编译出对应各个平台的版本，可对比相同模型的速度；"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:15
msgid "测试验证不同模型优化方法的效果，直接执行 ./load_and_run 可得到对应的帮助文档；"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:16
msgid ""
"`dump_with_testcase_mge.py `_ "
"会把输入数据、运行脚本时计算出的结果都打包到模型里，便于比较相同模型在不同平台下的计算结果差异；"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:17
msgid ""
"同时支持 ``--input`` 选项直接设置 mge C++ 模型的输入，输入格式支持 .ppm/.pgm/.json/.npy "
"等文件格式和命令行。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:20
msgid "模型准备"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:22
msgid ""
"将 mge 模型序列化并导出到文件, 我们以 `ResNet50 `_ 为例。 因为 MegEngine 的模型训练都是动态图形式 "
"，所以我们需要先将模型转成静态图然后再部署。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:25
msgid "具体可参考如下代码片段:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:27
msgid "*代码片段:*"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:53
msgid "执行脚本，并完成模型转换后，我们就获得了 MegEngine C++ API 可识别的预训练模型文件 ``resnet50.mge``。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:56
msgid "输入准备"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:58
msgid "load_and_run 可以用 ``--input`` 选项直接设置模型文件的输入, 它支持 .ppm/.pgm/.json/.npy 等多种格式"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:60
msgid "测试输入图片如下:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:65
msgid "图1 猫"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:67
msgid "因为模型的输入是 float32, 且是 nchw, 需要先将图片转成 npy 格式。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:82
msgid "编译 load_and_run"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:86
msgid "目前发布的版本我们开放了对 cpu（x86, x64, arm, armv8.2）和 gpu（cuda）平台的支持。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:88
msgid "我们在这里以 x86 和 arm 交叉编译为例，来阐述一下如何编译一个 x86 和 arm 的 load_and_run。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:91
msgid "linux x86 平台编译 load_and_run"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:100
msgid "编译完成后，我们可以在 ``build/sdk/load_and_run`` 目录找到 ``load_and_run`` 。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:103
msgid "linux 下交叉编译 arm 版本 load_and_run"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:105
msgid "在 ubuntu(16.04/18.04) 上进行 arm-android 的交叉编译:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:107
msgid ""
"到 android 的官网下载 ndk 的相关工具，这里推荐 *android-ndk-r21* "
"以上的版本：https://developer.android.google.cn/ndk/downloads/"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:108
msgid "在 bash 中设置 NDK_ROOT 环境变量：``export NDK_ROOT=ndk_dir``"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:109
msgid "使用以下脚本进行 arm-android 的交叉编译"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:115
msgid ""
"编译完成后，我们可以在 "
"``build_dir/android/arm64-v8a/release/install/bin/load_and_run`` "
"目录下找到编译生成的可执行文件 ``load_and_run``。 默认没有开启 armv8.2-a+dotprod "
"的新指令集支持，如果在一些支持的设备，如 cortex-a76 等设备，可以开启相关选项(更多选项开关，可以直接看该脚本文件)。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:118
msgid "开启 armv8.2-a+dotprod 的代码如下:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:125
msgid "代码执行"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:127
msgid ""
"下面的实验是在某 android 平台，未开启 armv8.2 指令集(当前测试模型为 float 模型，量化模型推荐开启 "
"armv8.2+dotprod 支持，能够充分利用 dotprod 指令集硬件加速)。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:129
msgid "用 ``load_and_run`` 加载之前 dump 好的 ``resnet50.mge`` 模型，可以看到类似这样的输出："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:131
msgid "先将模型和 load_and_run (依赖 megengine.so )传到手机。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:141
msgid "之后直接在手机上运行 load_and_run， 可以得到如下输出:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:164
msgid "平台相关 layout 优化"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:166
msgid ""
"目前 MegEngine 的网络是 nchw 的 layout，但是这种 layout 不利于充分利用 simd 特性，且边界处理异常复杂。 "
"为此，我们针对 arm 开发了 nchw44 的 layout。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:169
msgid "这个命名主要是针对 conv 来定的。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:171
msgid "nchw: conv 的 feature map 为 (n, c, h, w), weights 为 (oc, ic, fh, fw)。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:172
msgid ""
"nchw44: conv 的 feature map 为 (n, c/4, h, w, 4), weights 为 (oc/4, ic/4, "
"fh, fw, 4(ic), 4(oc))。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:174
msgid ""
"这里从 channel 上取 4 个数排成连续主要方便利用 neon 优化，由于 neon 指令是 128 bit，刚好是 4 个 32 "
"bit，所以定义 nchw44，对于 x86 avx 下，我们同样定义了 nchw88 的 layout 优化。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:176
msgid "下面是开启 nchw44 的优化后的结果:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:200
msgid "fastrun 模式"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:202
msgid ""
"目前在 MegEngine 中，针对某些 opr，尤其是 conv ，存在很多种不同的算法，如 direct, winograd, 或者 "
"im2col 等。这些算法在不同的 shape "
"或者不同的硬件平台上，其性能表现差别极大，导致很难写出一个有效的搜索算法，在执行时选择到最快的执行方式。为此，我们 MegEngine 集成了 "
"fastrun 模式，也就是在执行模型的时候会将每个 opr 的可选所有算法都执行一遍，然后选择一个最优的算法记录下来。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:204
msgid "一般分为两个阶段，搜参和运行。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:206
msgid "搜参阶段: 开启 fastrun 模式，同时将输出的结果存储到一个 cache 文件中"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:207
msgid "执行阶段: 带上 cache 再次执行"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:209
msgid "搜参阶段:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:232
msgid "执行阶段:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:255
msgid "整体来讲 fastrun 大概有10%的性能提速。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:258
msgid "如何开 winograd 优化"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:260
msgid ""
"winograd 在 channel 较大的时候，能够有效提升卷积的计算速度，核心思想是加法换乘法。详细原理参考 `fast algorithms"
" for convolutional neural networks `_。 其在 ResNet 或者 VGG16 等网络, winograd "
"有非常大的加速效果。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:263
msgid ""
"因为对于 3x3 的卷积，有多种 winograd 算法，如 f(2,3), f(4,3), f(6,3)，从理论加速比来讲，f(6,3) > "
"f(4,3) > f(2,3)， 但是 f(6, 3) 的预处理开销更大，因为 MegEngine 内部是基于分块来处理的，feature map"
" 比较小的情况下，f(6,3) 可能会引入比较多的冗余计算，导致其性能不如 f(2,3)，所以可将 winograd 变换和 fastrun "
"模式结合，基于 fastrun 模式搜索的结果来决定做哪种 winograd 变换。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:266
msgid "具体命令如下:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:291
msgid "正确性验证"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:293
msgid "MegEngine 内置了多种正确性验证的方法，方便检查网络计算正确性。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:296
msgid "开启 asserteq 验证正确性"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:298
msgid ""
"可以基于脚本 `dump_with_testcase_mge.py `_ "
"将输入数据和运行脚本时使用当前默认的计算设备计算出的模型结果都打包到模型里， 这样在不同平台下就方便比较结果差异了。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:304
msgid ""
"在执行 load_and_run 的时候就不需要再带上 ``--input``，因为输入已经打包进 ``resnet50.mdl``, 同时在执行"
" ``dump_with_testcase_mge.py`` 脚本的时候，会在 xpu (如果有 gpu，就在 gpu 上执行，如果没有就在 "
"cpu 上执行)执行整个网络，将结果作为 ``ground-truth`` 写入模型中。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:306
msgid "我们在执行 load_and_run 的时候会看到:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:342
msgid "可以看到最大误差是 3.86273e-05."
msgstr ""

#: ../../source/user-guide/load-and-run.rst:345
msgid "dump 输出结果"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:347
msgid ""
"同时，我们可以使用 ``--bin-out-dump`` 在指定的文件夹内保存输出结果。这样就可以用 load-and-run "
"在目标设备上跑数据集了："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:354
msgid "然后可以在 python 里打开输出文件："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:365
msgid "dump 每层结果"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:367
msgid ""
"我们很多时候会遇到这种情况，就是模型输出结果不对，这个时候就需要打出网络每一层的结果作比对，看看是哪一层导致。目前有两中展现方式，一个是 io-"
"dump, 另一个是 bin-io-dump."
msgstr ""

#: ../../source/user-guide/load-and-run.rst:369
msgid ""
"为了对比结果，需要假定一个平台结果为 ``ground-truth`` ，下面假定以x86的结果为 ``ground-truth`` ，验证 "
"x86 和 cuda 上的误差产生的原因（下面会使用 ``host_build.sh`` 编译出来的 ``load_and_run`` 来演示）。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:372
msgid "文本形式对比结果"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:380
msgid ""
"文档形式只是显示了部分信息，比如 tensor 的前几个输出结果，整个 tensor 的平均值，标准差之类的，如果需要具体到哪个值错误，需要用 "
"bin-io-dump 会将每一层的结果都输出到一个文件。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:383
msgid "raw形式对比结果"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:393
msgid "性能调优"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:395
msgid "load-and-run 可以进行 profiling 并产生一个 json 文件："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:401
msgid "这个 model.json 文件可以后续用于 profile_analyze.py 分析。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:403
msgid "profile_analyze.py 的示例用法："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:423
msgid "示例输出："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:462
msgid "这个表格打印了前五个耗时最多的算子。每列的含义如下："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:464
msgid "``device self time`` 是算子在计算设备上（例如 GPU ）的运行时间"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:466
msgid "``cumulative`` 累加前面所有算子的时间"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:468
msgid "``operator info`` 打印算子的基本信息"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:470
msgid "``computation`` 是算子需要的浮点数操作数目"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:472
msgid "``FLOPS`` 是算子每秒执行的浮点操作数目，由 ``computation`` 除以 ``device self time`` 并转换单位得到"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:474
msgid "``memory`` 是算子使用的存储（例如 GPU 显存）大小"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:476
msgid "``bandwidth`` 是算子的带宽，由 ``memory`` 除以 ``device self time`` 并转换单位得到"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:478
msgid "``in_shapes`` 是算子输入张量的形状"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:480
msgid "``out_shapes`` 是算子输出张量的形状"
msgstr ""

#: ../../source/user-guide/midout.rst:5
msgid "使用 midout 进行端上裁剪"
msgstr ""

#: ../../source/user-guide/midout.rst:8
msgid "如何在端上裁剪MegEngine库"
msgstr ""

#: ../../source/user-guide/midout.rst:9
msgid ""
"`midout `_ "
"是MegEngine中用来减小生成的二进制文件体积的工具，有助于在空间受限的设备上部署应用。midout通过记录模型推理时用到的opr和执行流，使用if(0)关闭未被记录的代码段后重新编译，利用"
" ``-flto`` 链接参数，可以大幅度减少静态链接的可执行文件的大小。其原理请参考 `midout原理 `_。 "
"现在基于MegEngine提供模型验证工具 `load-and-run "
"`_，展示怎样在某Aarch64架构的Android端上裁剪MegEngine库。"
msgstr ""

#: ../../source/user-guide/midout.rst:13
msgid "编译静态链接的load_and_run"
msgstr ""

#: ../../source/user-guide/midout.rst:14
msgid ""
"端上裁剪MegEngine库需要一个静态连接MegEngine的可执行程序，编译方法详见 `load-and-run的编译 `_。 "
"稍有不同的是编译时需要先设置load_and_run静态链接MegEngine。"
msgstr ""

#: ../../source/user-guide/midout.rst:21
msgid "否则，MegEngine会自动编译成动态库。然后执行："
msgstr ""

#: ../../source/user-guide/midout.rst:27
msgid "查看一下load_and_run的大小："
msgstr ""

#: ../../source/user-guide/midout.rst:34
msgid "此时load_and_run大小超过20MB。load_and_run的执行，请参考 `代码执行 `_。"
msgstr ""

#: ../../source/user-guide/midout.rst:37
msgid "裁剪load_and_run"
msgstr ""

#: ../../source/user-guide/midout.rst:38
msgid "MegEngine的裁剪可以从两方面进行："
msgstr ""

#: ../../source/user-guide/midout.rst:40
msgid "1、通过opr裁剪。在dump模型时，可以同时将模型用到的opr信息以json文件的形式输出，midout在编译期裁掉没有被模型使用到的所有opr。"
msgstr ""

#: ../../source/user-guide/midout.rst:42
msgid "2、通过trace流裁剪。运行一次模型推理，根据代码的执行流生成trace文件，通过trace文件，在二次编译时将没有执行的代码段裁剪掉。"
msgstr ""

#: ../../source/user-guide/midout.rst:44
msgid ""
"整个裁剪过程分为两个步骤。第一步，dump模型，获得模型opr信息；通过一次推理，获得trace文件。第二步，使用MegEngine的头文件生成工具"
" `gen_header_for_bin_reduce.py `_ 将opr信息和trace文件作为输入，生成 "
"bin_reduce.h并将该头文件加入编译Release版的应用程序。当然，也可以单独使用模型opr信息或是trace文件来生成bin_reduce.h，单独使用opr信息时，默认保留所有kernel，单独使用trace文件时，默认保留所有opr。"
msgstr ""

#: ../../source/user-guide/midout.rst:48
msgid "dump模型获得opr类型名称"
msgstr ""

#: ../../source/user-guide/midout.rst:50
msgid ""
"一个模型通常不会用到所有的opr，根据模型使用的opr，可以裁掉那些模型没有使用的opr。在转换模型时，我们可以通过如下方式获得模型的opr信息。"
" 使用 `dump_with_testcase_mge.py `_ 准备模型时，加上--output-strip-info参数。"
msgstr ""

#: ../../source/user-guide/midout.rst:57
msgid "执行完毕后，会生成resnet50.mge和resnet50.mge.json。查看这个json文件，它记录了模型用到的opr名称。"
msgstr ""

#: ../../source/user-guide/midout.rst:65
msgid "执行模型获得trace文件"
msgstr ""

#: ../../source/user-guide/midout.rst:67
msgid "基于trace的裁剪需要通过一次推理获得模型的执行trace文件。具体步骤如下："
msgstr ""

#: ../../source/user-guide/midout.rst:69
msgid "1、CMake构建时，打开MGE_WITH_MIDOUT_PROFILE开关，编译load_and_run："
msgstr ""

#: ../../source/user-guide/midout.rst:76
msgid "编译完成后，将build_dir/android/arm64-v8a/Release/install/bin下的load_and_run推至设备并执行："
msgstr ""

#: ../../source/user-guide/midout.rst:82
msgid "得到如下输出："
msgstr ""

#: ../../source/user-guide/midout.rst:107
msgid "注意到执行模型后，生成了midout_trace.20717文件，该文件记录了模型在底层执行了哪些kernel。"
msgstr ""

#: ../../source/user-guide/midout.rst:109
msgid "2、生成bin_recude.h并再次编译load_and_run："
msgstr ""

#: ../../source/user-guide/midout.rst:111
msgid ""
"将生成的midout_trace.20717拷贝至本地，使用上文提到的头文件生成工具 `gen_header_for_bin_reduce.py "
"`_ 生成bin_reduce.h。"
msgstr ""

#: ../../source/user-guide/midout.rst:117
msgid "再次编译load_and_run，注意要将bin_reduce.h加入并编译Release版本。设置CMAKE编译选项："
msgstr ""

#: ../../source/user-guide/midout.rst:127
msgid "编译完成后，检查load_and_run的大小："
msgstr ""

#: ../../source/user-guide/midout.rst:134
msgid "此时load_and_run的大小减小到2MB多。推到设备上运行，得到如下输出："
msgstr ""

#: ../../source/user-guide/midout.rst:159
msgid "可以看到模型依然正常运行，并且运行速度正常。"
msgstr ""

#: ../../source/user-guide/midout.rst:162
msgid "使用裁剪后的load_and_run"
msgstr ""

#: ../../source/user-guide/midout.rst:163
msgid ""
"想要裁剪前后的应用能够正常运行，需要保证裁剪前后两次推理使用同样的命令行参数。如果使用上文裁剪的load_and_fun的fast-"
"run功能(详见 :ref:`how_to_use_load_and_run`)。"
msgstr ""

#: ../../source/user-guide/midout.rst:169
msgid "可能得到如下输出："
msgstr ""

#: ../../source/user-guide/midout.rst:180
msgid ""
"这是因为程序运行到了已经被裁剪掉的函数中，未被记录在trace文件中的函数的实现已经被替换成trap()，详见 `midout原理 `_"
"。如果想要裁剪与fast-run配合使用，需要按如下流程获得trace文件："
msgstr ""

#: ../../source/user-guide/midout.rst:182
msgid "1、开启fast-run模式，执行未裁剪的load_and_run获得.cache文件，注意本次执行生成的trace应该被丢弃："
msgstr ""

#: ../../source/user-guide/midout.rst:188
msgid "2、使用.cache文件，执行load_and_run获得trace文件："
msgstr ""

#: ../../source/user-guide/midout.rst:194
msgid "3、如上节，将trace文件拷贝回本机，生成bin_reduce.h，再次编译load_and_run并推至设备。"
msgstr ""

#: ../../source/user-guide/midout.rst:196
msgid "4、使用裁剪后的load_and_run的fast-run功能，执行同2的命令，得到如下输出："
msgstr ""

#: ../../source/user-guide/midout.rst:222
msgid "使用其他load_and_run提供的功能也是如此，想要裁剪前后的应用能够正常运行，需要保证裁剪前后两次推理使用同样的命令行参数。"
msgstr ""

#: ../../source/user-guide/midout.rst:225
msgid "多个模型合并裁剪"
msgstr ""

#: ../../source/user-guide/midout.rst:226
msgid ""
"多个模型的合并裁剪与单个模型流程相同。 `gen_header_for_bin_reduce.py `_ 接受多个输入。 "
"假设有模型A与模型B。已经获得A.mge.json,B.mge.json以及A.trace,B.trace。执行："
msgstr ""

#: ../../source/user-guide/midout.rst:234
msgid "编译选项"
msgstr ""

#: ../../source/user-guide/midout.rst:235
msgid "MegEngine的cmake中有一些开关是默认打开的，它们提供了RTTI、异常抛出等特性，可以在第二次构建时关闭它们，以获得体积更小的load_and_run。它们是："
msgstr ""

#: ../../source/user-guide/midout.rst:237
msgid "`MGB_WITH_FLATBUFFERS` : FLABUFFERS格式支持"
msgstr ""

#: ../../source/user-guide/midout.rst:239
msgid "`MGE_ENABLE_RTTI` : C++ RTTI特性"
msgstr ""

#: ../../source/user-guide/midout.rst:241
msgid "`MGE_ENABLE_LOGGING` : 日志功能"
msgstr ""

#: ../../source/user-guide/midout.rst:243
msgid "`MGE_ENABLE_EXCEPTIONS` : 异常功能"
msgstr ""

#: ../../source/user-guide/midout.rst:245
msgid ""
"MegEngine提供一个总开关 `MGE_WITH_MINIMUM_SIZE` "
"来关闭上述特性。需要注意的是，只有在MGE_BIN_REDUCE被设置时，此开关才会被检查并生效。"
msgstr ""

#: ../../source/user-guide/midout.rst:248
msgid "裁剪基于MegEngine的应用"
msgstr ""

#: ../../source/user-guide/midout.rst:249
msgid "可以通过如下几种方式集成MegEngine，对应的裁剪方法相差无几："
msgstr ""

#: ../../source/user-guide/midout.rst:251
msgid ""
"1、参照 `CMakeLists.txt `_，将应用集成到整个MegEngine的工程。 "
"假设已经将app.cpp集成到MegEngine，那么会编译出静态链接MegEngine的可执行程序 "
"`app`。只需要按照上文中裁剪load_and_run的流程裁剪 `app` 即可。"
msgstr ""

#: ../../source/user-guide/midout.rst:254
msgid ""
"2、可能一个应用想要通过静态库集成MegEngine。此时需要获得一个裁剪过的libmegengine.a。可以依然使用load_and_run运行模型获得trace文件，生成bin_reduce.h，并二次编译获得裁剪过的libmegengine.a。"
" 此时，用户使用自己编写的构建脚本构建应用程序，并静态链接libmegengine.a，加上链接参数 "
"``-flto=full``。即可得到裁剪过的基于MegEngine的应用。"
msgstr ""

#: ../../source/user-guide/midout.rst:257
msgid "3、上述流程亦可以用于libmegengine.so的裁剪，但是动态库的裁剪效果远不及静态库。原因在于动态库并不知道某段代码是否会被调用，因此链接器不会进行激进的优化。"
msgstr ""

#: ../../source/user-guide/quantization.rst:5
msgid "量化训练"
msgstr ""

#: ../../source/user-guide/quantization.rst:7
msgid ""
"量化指的是将浮点数模型（一般是 32 位浮点数）的权重或激活值用位数更少的数值类型（比如 8 位整数、16 位浮点数）来近似表示的过程。 "
"量化后的模型会占用更小的存储空间，还能够利用许多硬件平台上的专属算子进行提速。比如在 MegEngine 中使用 8 位整数来进行量化，相比默认的"
" 32 位浮点数，模型大小可以减少为 1/4，而运行在特定的设备上其计算速度也能提升为 2-4 倍。"
msgstr ""

#: ../../source/user-guide/quantization.rst:10
msgid ""
"量化的目的是为了追求极致的推理计算速度，为此舍弃了数值表示的精度，直觉上会带来较大的模型掉点，但是在使用一系列精细的量化处理之后，其掉点可以变得微乎其微，并能支持正常的部署使用。而且近年来随着专用神经网络加速芯片的兴起，低比特非浮点的运算方式越来越普及，因此如何把一个"
" GPU 上训练的浮点数模型转化为低比特的量化模型，就成为了工业界非常关心的话题。"
msgstr ""

#: ../../source/user-guide/quantization.rst:12
msgid "一般来说，得到量化模型的转换过程按代价从低到高可以分为以下 4 种："
msgstr ""

#: ../../source/user-guide/quantization.rst:14
msgid ""
"Type1 和 Type2 由于是在模型浮点模型训练之后介入，无需大量训练数据，故而转换代价更低，被称为后量化（Post "
"Quantization）；"
msgstr ""

#: ../../source/user-guide/quantization.rst:15
msgid ""
"Type3 和 Type4 "
"则需要在浮点模型训练时就插入一些假量化（FakeQuantize）算子，模拟计算过程中数值截断后精度降低的情形，故而称为量化感知训练（Quantization"
" Aware Training, QAT）。"
msgstr ""

#: ../../source/user-guide/quantization.rst:17
msgid "本文主要介绍 Type2 和 Type3 在 MegEngine 中的完整流程，事实上，除了 Type2 无需进行假量化，两者的整体流程完全一致。"
msgstr ""

#: ../../source/user-guide/quantization.rst:20
msgid "整体流程"
msgstr ""

#: ../../source/user-guide/quantization.rst:22
msgid ""
"以 Type3 为例，一般以一个训练完毕的浮点模型为起点，称为 Float 模型。包含假量化算子的用浮点操作来模拟量化过程的新模型，我们称之为 "
"Quantized-Float 模型，或者 QFloat 模型。可以直接在终端设备上运行的模型，称之为 Quantized 模型，简称 Q 模型。"
msgstr ""

#: ../../source/user-guide/quantization.rst:24
msgid "而三者的精度一般是 ``Float > QFloat > Q`` ，故而一般量化算法也就分为两步："
msgstr ""

#: ../../source/user-guide/quantization.rst:26
msgid "拉近 QFloat 和 Q，这样训练阶段的精度可以作为最终 Q 精度的代理指标，这一阶段偏工程；"
msgstr ""

#: ../../source/user-guide/quantization.rst:27
msgid "拔高 QFloat 逼近 Float，这样就可以将量化模型性能尽可能恢复到 Float 的精度，这一阶段偏算法。"
msgstr ""

#: ../../source/user-guide/quantization.rst:29
msgid "典型的三种模型在三个阶段的精度变化如下："
msgstr ""

#: ../../source/user-guide/quantization.rst:31
msgid "对应到具体的 MegEngine 接口中，三阶段如下："
msgstr ""

#: ../../source/user-guide/quantization.rst:33
msgid "基于 :class:`~.module.Module` 搭建网络模型，并按照正常的浮点模型方式进行训练；"
msgstr ""

#: ../../source/user-guide/quantization.rst:34
msgid ""
"使用 :func:`~.quantization.quantize.quantize_qat` 将浮点模型转换为 QFloat "
"模型，其中可被量化的关键 Module 会被转换为 :class:`~.module.qat.QATModule` ，并基于量化配置 "
":class:`~.quantization.QConfig` 设置好假量化算子和数值统计方式；"
msgstr ""

#: ../../source/user-guide/quantization.rst:35
msgid ""
"使用 :func:`~.quantization.quantize.quantize` 将 QFloat 模型转换为 Q 模型，对应的 "
"QATModule 则会被转换为 :class:`~.module.quantized.QuantizedModule` "
"，此时网络无法再进行训练，网络中的算子都会转换为低比特计算方式，即可用于部署了。"
msgstr ""

#: ../../source/user-guide/quantization.rst:37
msgid ""
"该流程是 Type3 对应 QAT 的步骤，Type2 对应的后量化则需使用不同 QConfig，且需使用 evaluation 模式运行 "
"QFloat 模型，而非训练模式。更多细节可以继续阅读下一节详细的接口介绍。"
msgstr ""

#: ../../source/user-guide/quantization.rst:40
msgid "接口介绍"
msgstr ""

#: ../../source/user-guide/quantization.rst:42
msgid ""
"在 MegEngine 中，最上层的接口是配置如何量化的 :class:`~.quantization.QConfig` 和模型转换模块里的 "
":func:`~.quantization.quantize.quantize_qat` 与 "
":func:`~.quantization.quantize.quantize` 。"
msgstr ""

#: ../../source/user-guide/quantization.rst:45
msgid "QConfig"
msgstr ""

#: ../../source/user-guide/quantization.rst:47
msgid ""
"QConfig 包括了 :class:`~.quantization.Observer` 和 "
":class:`~.quantization.FakeQuantize` "
"两部分。我们知道，对模型转换为低比特量化模型一般分为两步：一是统计待量化模型中参数和 activation "
"的数值范围（scale）和零点（zero_point），二是根据 scale 和 zero_point "
"将模型转换成指定的数值类型。而为了统计这两个值，我们需要使用 Observer。"
msgstr ""

#: ../../source/user-guide/quantization.rst:49
msgid ""
"Observer 继承自 :class:`~.module.Module` ，也会参与网络的前向传播，但是其 forward "
"的返回值就是输入，所以不会影响网络的反向梯度传播。其作用就是在前向时拿到输入的值，并统计其数值范围，并通过 "
":meth:`~.quantization.Observer.get_qparams` 来获取。所以在搭建网络时把需要统计数值范围的的 "
"Tensor 作为 Observer 的输入即可。"
msgstr ""

#: ../../source/user-guide/quantization.rst:63
msgid ""
"另外如果只观察而不模拟量化会导致模型掉点，于是我们需要有 FakeQuantize 来根据 Observer "
"观察到的数值范围模拟量化时的截断，使得参数在训练时就能提前“适应“这种操作。FakeQuantize 在前向时会根据传入的 scale 和 "
"zero_point 对输入 Tensor 做模拟量化的操作，即先做一遍数值转换再转换后的值还原成原类型，如下所示："
msgstr ""

#: ../../source/user-guide/quantization.rst:80
msgid "目前 MegEngine 支持对 weight/activation 两部分的量化，如下所示："
msgstr ""

#: ../../source/user-guide/quantization.rst:91
msgid "这里使用了两种 Observer 来统计信息，而 FakeQuantize 使用了默认的算子。"
msgstr ""

#: ../../source/user-guide/quantization.rst:93
msgid "如果是后量化，或者说 Calibration，由于无需进行 FakeQuantize，故而其 fake_quant 属性为 None 即可："
msgstr ""

#: ../../source/user-guide/quantization.rst:104
msgid ""
"除了使用在 :mod:`~.quantization.qconfig` 里提供的预设 QConfig，也可以根据需要灵活选择 Observer 和"
" FakeQuantize  实现自己的 QConfig。目前提供的 Observer 包括："
msgstr ""

#: ../../source/user-guide/quantization.rst:106
msgid ""
":class:`~.quantization.observer.MinMaxObserver` ，使用最简单的算法统计 "
"min/max，对见到的每批数据取 min/max 跟当前存的值比较并替换，基于 min/max 得到 scale 和 zero_point；"
msgstr ""

#: ../../source/user-guide/quantization.rst:107
msgid ""
":class:`~.quantization.observer.ExponentialMovingAverageObserver` "
"，引入动量的概念，对每批数据的 min/max 与现有 min/max 的加权和跟现有值比较；"
msgstr ""

#: ../../source/user-guide/quantization.rst:108
msgid ""
":class:`~.quantization.observer.HistogramObserver` ，更加复杂的基于直方图分布的 min/max"
" 统计算法，且在 forward 时持续更新该分布，并根据该分布计算得到 scale 和 zero_point。"
msgstr ""

#: ../../source/user-guide/quantization.rst:110
msgid ""
"对于 FakeQuantize，目前还提供了 :class:`~.quantization.fake_quant.TQT` 算子，另外还可以继承 "
"``_FakeQuant`` 基类实现自定义的假量化算子。"
msgstr ""

#: ../../source/user-guide/quantization.rst:112
msgid ""
"在实际使用过程中，可能需要在训练时让 Observer 统计并更新参数，但是在推理时则停止更新。 Observer 和 FakeQuantize "
"都支持 :meth:`~.quantization.observer.Observer.enable` 和 "
":meth:`~.quantization.observer.Observer.disable` 功能，且 Observer 会在 "
":meth:`~.module.module.Module.train` 和 "
":meth:`~.module.module.Module.train` 时自动分别调用 enable/disable。"
msgstr ""

#: ../../source/user-guide/quantization.rst:114
msgid ""
"所以一般在 Calibration 时，会先执行 ``net.eval()`` 保证网络的参数不被更新，然后再执行 "
":func:`enable_observer(net) ` 来手动开启 Observer 的统计修改功能。"
msgstr ""

#: ../../source/user-guide/quantization.rst:117
msgid "模型转换模块与相关基类"
msgstr ""

#: ../../source/user-guide/quantization.rst:119
msgid ""
"QConfig 提供了一系列如何对模型做量化的接口，而要使用这些接口，需要网络的 Module 能够在 forward "
"时给参数、activation 加上 Observer 和进行 FakeQuantize。转换模块的作用就是将模型中的普通 Module "
"替换为支持这一系列操作的 :class:`~.module.qat.module.QATModule` "
"，并能支持进一步替换成无法训练、专用于部署的 :class:`~.module.quantized.module.QuantizedModule`"
" 。"
msgstr ""

#: ../../source/user-guide/quantization.rst:121
msgid ""
"基于三种基类实现的 Module 是一一对应的关系，通过转换接口可以依次替换为不同实现的同名 "
"Module。同时考虑到量化与算子融合（Fuse）的高度关联，我们提供了一系列预先融合好的 Module，比如 "
":class:`~.module.conv.ConvRelu2d` 、 :class:`~.module.conv_bn.ConvBn2d` 和 "
":class:`~.module.conv_bn.ConvBnRelu2d` 等。除此之外还提供专用于量化的 "
":class:`~.module.quant_dequant.QuantStub` 、 "
":class:`~.module.quant_dequant.DequantStub` 等辅助模块。"
msgstr ""

#: ../../source/user-guide/quantization.rst:123
msgid ""
"转换的原理很简单，就是将父 Module 中可被量化（Quantable）的子 Module 替换为对应的新 Module。但是有一些 "
"Quantable Module 还包含 Quantable 子 Module，比如 ConvBn 就包含一个 Conv2d 和一个 "
"BatchNorm2d，转换过程并不会对这些子 Module 进一步转换，原因是父 Module 被替换之后，其 forward "
"计算过程已经完全不同了，不会再依赖于这些子 Module。"
msgstr ""

#: ../../source/user-guide/quantization.rst:127
msgid ""
"如果需要使一部分 Module 及其子 Module 保留 Float 状态，不进行转换，可以使用 "
":meth:`~.module.module.Module.disable_quantize` 来处理。"
msgstr ""

#: ../../source/user-guide/quantization.rst:129
msgid ""
"如果网络结构中涉及一些二元及以上的 ElementWise 操作符，比如加法乘法等，由于多个输入各自的 scale "
"并不一致，必须使用量化专用的算子，并指定好输出的 scale。实际使用中只需要把这些操作替换为 "
":class:`~.module.elemwise.Elemwise` 即可，比如 ``self.add_relu = "
"Elemwise(\"FUSE_ADD_RELU\")``"
msgstr ""

#: ../../source/user-guide/quantization.rst:131
msgid ""
"另外由于转换过程修改了原网络结构， :ref:`train_and_evaluation` "
"中提到的模型保存与加载无法直接适用于转换后的网络，读取新网络保存的参数时，需要先调用转换接口得到转换后的网络，才能用 "
"load_state_dict 将参数进行加载。"
msgstr ""

#: ../../source/user-guide/quantization.rst:134
msgid "实例讲解"
msgstr ""

#: ../../source/user-guide/quantization.rst:136
msgid "下面我们以 ResNet18 为例来讲解量化的完整流程，完整代码见 `MegEngine Models `_ 。主要分为以下几步："
msgstr ""

#: ../../source/user-guide/quantization.rst:138
msgid "修改网络结构，使用已经 Fuse 好的 ConvBn2d、ConvBnRelu2d、ElementWise 代替原先的 Module；"
msgstr ""

#: ../../source/user-guide/quantization.rst:139
msgid "在正常模式下预训练模型，并在每轮迭代保存网络检查点；"
msgstr ""

#: ../../source/user-guide/quantization.rst:140
msgid "调用 :func:`~.quantization.quantize.quantize_qat` 转换模型，并进行 finetune；"
msgstr ""

#: ../../source/user-guide/quantization.rst:141
msgid "调用 :func:`~.quantization.quantize.quantize` 转换为量化模型，并执行 dump 用于后续模型部署。"
msgstr ""

#: ../../source/user-guide/quantization.rst:143
msgid ""
"网络结构见 `resnet.py `_ ，相比惯常写法，我们修改了其中一些子 Module，将原先单独的 ``conv``, ``bn``, "
"``relu`` 替换为 Fuse 过的 Quantable Module。"
msgstr ""

#: ../../source/user-guide/quantization.rst:170
msgid "然后对该模型进行若干轮迭代训练，并保存检查点，这里省略细节："
msgstr ""

#: ../../source/user-guide/quantization.rst:189
msgid "再调用 :func:`~.quantization.quantize.quantize_qat` 来将网络转换为 QATModule："
msgstr ""

#: ../../source/user-guide/quantization.rst:200
msgid "这里使用默认的 ``ema_fakequant_qconfig`` 来进行 ``int8`` 量化。"
msgstr ""

#: ../../source/user-guide/quantization.rst:202
msgid ""
"然后我们继续使用上面相同的代码进行 finetune 训练。值得注意的是，如果这两步全在一次程序运行中执行，那么训练的 trace "
"函数需要用不一样的，因为模型的参数变化了，需要重新进行编译。示例代码中则是采用在新的执行中读取检查点重新编译的方法。"
msgstr ""

#: ../../source/user-guide/quantization.rst:204
msgid ""
"在 QAT 模式训练完成后，我们继续保存检查点，执行 `inference.py `_ 并设置 ``mode`` 为 ``quantized`` "
"，这里需要将原始 Float 模型转换为 QAT 模型之后再加载检查点。"
msgstr ""

#: ../../source/user-guide/quantization.rst:218
msgid "模型转换为量化模型包括以下几步："
msgstr ""

#: ../../source/user-guide/quantization.rst:249
msgid "至此便得到了一个可用于部署的量化模型。"
msgstr ""

#: ../../source/user-guide/sublinear-memory.rst:5
#: ../../source/user-guide/sublinear-memory.rst:8
msgid "亚线性内存优化"
msgstr ""

#: ../../source/user-guide/sublinear-memory.rst:10
msgid ""
"使用大 batch size 通常能够提升深度学习模型性能。然而，我们经常遇到的困境是有限的 GPU 内存资源无法满足大 batch size "
"模型训练。为了缓解这一问题， MegEngine 提供了亚线性内存 ( sublinear memory ) "
"优化技术用于降低网络训练的内存占用量。该技术基于 `gradient checkpointing `_ "
"算法，通过事先搜索最优的计算图节点作为前向传播和反向传播检查点（ checkpoints ），省去其它中间结果存储，大幅节约了内（显）存使用。"
msgstr ""

#: ../../source/user-guide/sublinear-memory.rst:12
msgid ""
"用户在编译静态图时使用 "
":class:`~.megengine.jit.sublinear_memory_config.SublinearMemoryConfig` 设置"
" :class:`~.megengine.jit.tracing.trace` 的参数 ``sublinear_memory_config`` "
"，就可以打开亚线性内存优化。"
msgstr ""

#: ../../source/user-guide/sublinear-memory.rst:24
msgid ""
"使用亚线性内存在编译计算图和训练模型时有少量的额外时间开销，但是可以大幅减少显存的开销。下面我们以 `ResNet50 `_ "
"为例，说明如何使用亚线性内存优化技术，突破显存瓶颈来训练更大batch size的模型。"
msgstr ""

#: ../../source/user-guide/trace.rst:5
msgid "动态图转静态图"
msgstr ""

#: ../../source/user-guide/trace.rst:8
msgid "动态图转化成静态图"
msgstr ""

#: ../../source/user-guide/trace.rst:10
msgid ""
"一般模型训练中我们推荐使用动态图，但仍存在一些场景静态图是必要的。 静态图相比起动态图，对全局的信息掌握更丰富，可做的优化也会更多。 比如下节 "
":ref:`sublinear` 技术，就依赖静态图。"
msgstr ""

#: ../../source/user-guide/trace.rst:14
msgid "我们使用 :class:`~.megengine.jit.tracing.trace` 功能实现动态图到静态图的转换。"
msgstr ""

#: ../../source/user-guide/trace.rst:17
msgid "symbolic 参数"
msgstr ""

#: ../../source/user-guide/trace.rst:19
msgid "用 trace 来装饰一个训练（或者测试）函数时，可以指定 ``symbolic`` 参数，示例代码如下:"
msgstr ""

#: ../../source/user-guide/trace.rst:28
msgid "``symbolic`` 的取值为 True 或者 False，其含义如下:"
msgstr ""

#: ../../source/user-guide/trace.rst:30
msgid ""
"True 表示“静态构造”或者“根据符号构造”。此时，计算图中的所有数据节点（即张量）被视为符号（即 "
"``symbolic``）。它们仅仅作为占位符（placeholder），不产生实际的内存分配，也没有实际的值。此时计算图的编译过程完全取决于计算图的结构，而不取决于张量的具体值，是真正的“静态”。"
msgstr ""

#: ../../source/user-guide/trace.rst:32
msgid ""
"False 表示“动态构造”或者“根据值构造”。此时，被 :class:`~.megengine.jit.tracing.trace` "
"装饰的函数在第一次被调用时，会根据输入的数据执行一次计算，这次计算会构建出一个动态图。然后，这个动态图会被编译为一个静态图。此后，该函数的所有调用都会运行这个静态图，而不再依赖调用时输入的值。此种模式可以视为“动态构建第一次，此后静态运行”。"
" **MegEngine 默认使用此模式。** 这也是 PyTorch 中的 trace 功能所采用的模式。"
msgstr ""

#: ../../source/user-guide/trace.rst:34
msgid "下面我们通过示例代码说明两种模式下构图过程的区别。"
msgstr ""

#: ../../source/user-guide/trace.rst:50 ../../source/user-guide/trace.rst:94
msgid "输出为："
msgstr ""

#: ../../source/user-guide/trace.rst:56
msgid ""
"如上所示，当 ``symbolic=True`` 时，网络的输出 Tensor 并未被赋值。如果我们将 ``symbolic`` 改为 "
"False，重新执行上面的代码将得到："
msgstr ""

#: ../../source/user-guide/trace.rst:62
msgid "可以看到，此时网络的输出 Tensor 是有结果值的。也就说，计算图确实被构造和执行了。"
msgstr ""

#: ../../source/user-guide/trace.rst:64
msgid "在绝大部分情况下，两种模式下构造出的静态图并没有区别，使用中也没有分别。然而，它们有一些细微的区别需要注意。"
msgstr ""

#: ../../source/user-guide/trace.rst:66
msgid ""
"``symbolic=False`` "
"的模式下，由于第一次运行和构建计算图的过程依赖于输入，这提供了一定的“动态灵活性”。根据第一次运行时信息的不同，可以构建出不同的静态图。这种灵活性是"
" ``symbolic=True`` "
"的模式无法提供的。例如，可以在网络搭建中写诸如“如果条件满足，则执行分支1，否则执行分支2”的语句。注意，如果这样的条件语句在循环中，那么在循环的第一次执行中构造出的静态图将固定不再改变，即使在循环的后续执行中，该条件语句的结果发生了变化。这是容易造成问题和误解的地方。"
msgstr ""

#: ../../source/user-guide/trace.rst:68
msgid ""
"``symbolic=False`` "
"的模式的一个缺点是，由于第一次的运行在动态图模式下，无法利用静态图的内存优化，通常会耗费更大的内存。这可能导致本来在静态图模式下可以运行的网络，在第一次运行时由于内存不够而失败。"
msgstr ""

#: ../../source/user-guide/trace.rst:70
msgid ""
"与之相对，``symbolic=True`` "
"的模式具有静态图完全的优点和缺点：始终高效，但缺乏灵活性。如果网络中包含了需要运行时动态信息才能计算的条件语句，该模式将会失败。"
msgstr ""

#: ../../source/user-guide/trace.rst:72
msgid "具体应用中，用户需要根据情况灵活选择使用哪种模式。"
msgstr ""

#: ../../source/user-guide/trace.rst:75
msgid "使用 exclude_from_trace"
msgstr ""

#: ../../source/user-guide/trace.rst:76
msgid ""
"exclude_from_trace 中的代码不会被 trace，而且其中的代码允许访问静态区域的 Tensor。 可在模型中关键位置用来打印 "
"tensor 信息，或者做多卡参数同步。"
msgstr ""

#: ../../source/user-guide/trace.rst:102
msgid "由于 exclude_from_trace 会把整体的执行序列分割为多个子序列，因此不建议在内部插入影响执行状态的语句。"
msgstr ""

