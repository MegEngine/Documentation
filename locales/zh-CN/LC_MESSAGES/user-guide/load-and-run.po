# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2021, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.3.0.dev\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-09 13:56+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/user-guide/load-and-run.rst:5
msgid "模型正确性、速度验证与性能调试"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:8
msgid "如何使用 load_and_run"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:10
msgid ""
"load_and_run 是 MegEngine 中的加载并运行模型的工具，主要用来做模型正确性验证，速度验证及性能调试，源代码在 `load-"
"and-run `_ 。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:12
msgid "load_and_run 有以下功能："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:14
msgid "编译出对应各个平台的版本，可对比相同模型的速度；"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:15
msgid "测试验证不同模型优化方法的效果，直接执行 ./load_and_run 可得到对应的帮助文档；"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:16
msgid ""
"`dump_with_testcase_mge.py `_ "
"会把输入数据、运行脚本时计算出的结果都打包到模型里，便于比较相同模型在不同平台下的计算结果差异；"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:17
msgid ""
"同时支持 ``--input`` 选项直接设置 mge C++ 模型的输入，输入格式支持 .ppm/.pgm/.json/.npy "
"等文件格式和命令行。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:20
msgid "模型准备"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:22
msgid ""
"将 mge 模型序列化并导出到文件, 我们以 `ResNet50 `_ 为例。 因为 MegEngine 的模型训练都是动态图形式 "
"，所以我们需要先将模型转成静态图然后再部署。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:25
msgid "具体可参考如下代码片段:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:27
msgid "*代码片段:*"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:53
msgid "执行脚本，并完成模型转换后，我们就获得了 MegEngine C++ API 可识别的预训练模型文件 ``resnet50.mge``。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:56
msgid "输入准备"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:58
msgid "load_and_run 可以用 ``--input`` 选项直接设置模型文件的输入, 它支持 .ppm/.pgm/.json/.npy 等多种格式"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:60
msgid "测试输入图片如下:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:65
msgid "图1 猫"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:67
msgid "因为模型的输入是 float32, 且是 nchw, 需要先将图片转成 npy 格式。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:82
msgid "编译 load_and_run"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:86
msgid "目前发布的版本我们开放了对 cpu（x86, x64, arm, armv8.2）和 gpu（cuda）平台的支持。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:88
msgid "我们在这里以 x86 和 arm 交叉编译为例，来阐述一下如何编译一个 x86 和 arm 的 load_and_run。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:91
msgid "linux x86 平台编译 load_and_run"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:100
msgid "编译完成后，我们可以在 ``build/sdk/load_and_run`` 目录找到 ``load_and_run`` 。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:103
msgid "linux 下交叉编译 arm 版本 load_and_run"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:105
msgid "在 ubuntu(16.04/18.04) 上进行 arm-android 的交叉编译:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:107
msgid ""
"到 android 的官网下载 ndk 的相关工具，这里推荐 *android-ndk-r21* "
"以上的版本：https://developer.android.google.cn/ndk/downloads/"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:108
msgid "在 bash 中设置 NDK_ROOT 环境变量：``export NDK_ROOT=ndk_dir``"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:109
msgid "使用以下脚本进行 arm-android 的交叉编译"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:115
msgid ""
"编译完成后，我们可以在 "
"``build_dir/android/arm64-v8a/release/install/bin/load_and_run`` "
"目录下找到编译生成的可执行文件 ``load_and_run``。 默认没有开启 armv8.2-a+dotprod "
"的新指令集支持，如果在一些支持的设备，如 cortex-a76 等设备，可以开启相关选项(更多选项开关，可以直接看该脚本文件)。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:118
msgid "开启 armv8.2-a+dotprod 的代码如下:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:125
msgid "代码执行"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:127
msgid ""
"下面的实验是在某 android 平台，未开启 armv8.2 指令集(当前测试模型为 float 模型，量化模型推荐开启 "
"armv8.2+dotprod 支持，能够充分利用 dotprod 指令集硬件加速)。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:129
msgid "用 ``load_and_run`` 加载之前 dump 好的 ``resnet50.mge`` 模型，可以看到类似这样的输出："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:131
msgid "先将模型和 load_and_run (依赖 megengine.so )传到手机。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:141
msgid "之后直接在手机上运行 load_and_run， 可以得到如下输出:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:164
msgid "平台相关 layout 优化"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:166
msgid ""
"目前 MegEngine 的网络是 nchw 的 layout，但是这种 layout 不利于充分利用 simd 特性，且边界处理异常复杂。 "
"为此，我们针对 arm 开发了 nchw44 的 layout。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:169
msgid "这个命名主要是针对 conv 来定的。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:171
msgid "nchw: conv 的 feature map 为 (n, c, h, w), weights 为 (oc, ic, fh, fw)。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:172
msgid ""
"nchw44: conv 的 feature map 为 (n, c/4, h, w, 4), weights 为 (oc/4, ic/4, "
"fh, fw, 4(ic), 4(oc))。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:174
msgid ""
"这里从 channel 上取 4 个数排成连续主要方便利用 neon 优化，由于 neon 指令是 128 bit，刚好是 4 个 32 "
"bit，所以定义 nchw44，对于 x86 avx 下，我们同样定义了 nchw88 的 layout 优化。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:176
msgid "下面是开启 nchw44 的优化后的结果:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:200
msgid "fastrun 模式"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:202
msgid ""
"目前在 MegEngine 中，针对某些 opr，尤其是 conv ，存在很多种不同的算法，如 direct, winograd, 或者 "
"im2col 等。这些算法在不同的 shape "
"或者不同的硬件平台上，其性能表现差别极大，导致很难写出一个有效的搜索算法，在执行时选择到最快的执行方式。为此，我们 MegEngine 集成了 "
"fastrun 模式，也就是在执行模型的时候会将每个 opr 的可选所有算法都执行一遍，然后选择一个最优的算法记录下来。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:204
msgid "一般分为两个阶段，搜参和运行。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:206
msgid "搜参阶段: 开启 fastrun 模式，同时将输出的结果存储到一个 cache 文件中"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:207
msgid "执行阶段: 带上 cache 再次执行"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:209
msgid "搜参阶段:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:232
msgid "执行阶段:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:255
msgid "整体来讲 fastrun 大概有10%的性能提速。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:258
msgid "如何开 winograd 优化"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:260
msgid ""
"winograd 在 channel 较大的时候，能够有效提升卷积的计算速度，核心思想是加法换乘法。详细原理参考 `fast algorithms"
" for convolutional neural networks `_。 其在 ResNet 或者 VGG16 等网络, winograd "
"有非常大的加速效果。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:263
msgid ""
"因为对于 3x3 的卷积，有多种 winograd 算法，如 f(2,3), f(4,3), f(6,3)，从理论加速比来讲，f(6,3) > "
"f(4,3) > f(2,3)， 但是 f(6, 3) 的预处理开销更大，因为 MegEngine 内部是基于分块来处理的，feature map"
" 比较小的情况下，f(6,3) 可能会引入比较多的冗余计算，导致其性能不如 f(2,3)，所以可将 winograd 变换和 fastrun "
"模式结合，基于 fastrun 模式搜索的结果来决定做哪种 winograd 变换。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:266
msgid "具体命令如下:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:291
msgid "正确性验证"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:293
msgid "MegEngine 内置了多种正确性验证的方法，方便检查网络计算正确性。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:296
msgid "开启 asserteq 验证正确性"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:298
msgid ""
"可以基于脚本 `dump_with_testcase_mge.py `_ "
"将输入数据和运行脚本时使用当前默认的计算设备计算出的模型结果都打包到模型里， 这样在不同平台下就方便比较结果差异了。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:304
msgid ""
"在执行 load_and_run 的时候就不需要再带上 ``--input``，因为输入已经打包进 ``resnet50.mdl``, 同时在执行"
" ``dump_with_testcase_mge.py`` 脚本的时候，会在 xpu (如果有 gpu，就在 gpu 上执行，如果没有就在 "
"cpu 上执行)执行整个网络，将结果作为 ``ground-truth`` 写入模型中。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:306
msgid "我们在执行 load_and_run 的时候会看到:"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:342
msgid "可以看到最大误差是 3.86273e-05."
msgstr ""

#: ../../source/user-guide/load-and-run.rst:345
msgid "dump 输出结果"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:347
msgid ""
"同时，我们可以使用 ``--bin-out-dump`` 在指定的文件夹内保存输出结果。这样就可以用 load-and-run "
"在目标设备上跑数据集了："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:354
msgid "然后可以在 python 里打开输出文件："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:365
msgid "dump 每层结果"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:367
msgid ""
"我们很多时候会遇到这种情况，就是模型输出结果不对，这个时候就需要打出网络每一层的结果作比对，看看是哪一层导致。目前有两中展现方式，一个是 io-"
"dump, 另一个是 bin-io-dump."
msgstr ""

#: ../../source/user-guide/load-and-run.rst:369
msgid ""
"为了对比结果，需要假定一个平台结果为 ``ground-truth`` ，下面假定以x86的结果为 ``ground-truth`` ，验证 "
"x86 和 cuda 上的误差产生的原因（下面会使用 ``host_build.sh`` 编译出来的 ``load_and_run`` 来演示）。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:372
msgid "文本形式对比结果"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:380
msgid ""
"文档形式只是显示了部分信息，比如 tensor 的前几个输出结果，整个 tensor 的平均值，标准差之类的，如果需要具体到哪个值错误，需要用 "
"bin-io-dump 会将每一层的结果都输出到一个文件。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:383
msgid "raw形式对比结果"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:393
msgid "性能调优"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:395
msgid "load-and-run 可以进行 profiling 并产生一个 json 文件："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:401
msgid "这个 model.json 文件可以后续用于 profile_analyze.py 分析。"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:403
msgid "profile_analyze.py 的示例用法："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:423
msgid "示例输出："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:462
msgid "这个表格打印了前五个耗时最多的算子。每列的含义如下："
msgstr ""

#: ../../source/user-guide/load-and-run.rst:464
msgid "``device self time`` 是算子在计算设备上（例如 GPU ）的运行时间"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:466
msgid "``cumulative`` 累加前面所有算子的时间"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:468
msgid "``operator info`` 打印算子的基本信息"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:470
msgid "``computation`` 是算子需要的浮点数操作数目"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:472
msgid "``FLOPS`` 是算子每秒执行的浮点操作数目，由 ``computation`` 除以 ``device self time`` 并转换单位得到"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:474
msgid "``memory`` 是算子使用的存储（例如 GPU 显存）大小"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:476
msgid "``bandwidth`` 是算子的带宽，由 ``memory`` 除以 ``device self time`` 并转换单位得到"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:478
msgid "``in_shapes`` 是算子输入张量的形状"
msgstr ""

#: ../../source/user-guide/load-and-run.rst:480
msgid "``out_shapes`` 是算子输出张量的形状"
msgstr ""

