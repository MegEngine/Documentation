# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2021, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.3.0.dev\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-09 13:56+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/user-guide/distribution.rst:5
msgid "分布式训练"
msgstr ""

#: ../../source/user-guide/distribution.rst:7
msgid ""
"本章我们将介绍如何在 MegEngine 中高效地利用多 GPU 进行分布式训练。 分布式训练是指同时利用一台或者多台机器上的 GPU "
"进行并行计算。 在深度学习领域，最常见的并行计算方式是在数据层面进行的， 即每个 GPU 各自负责一部分数据，并需要跑通整个训练和推理流程。 "
"这种方式叫做 **数据并行** 。"
msgstr ""

#: ../../source/user-guide/distribution.rst:13
msgid "目前 MegEngine 开放的接口支持单机多卡和多机多卡的数据并行方式。"
msgstr ""

#: ../../source/user-guide/distribution.rst:17
msgid "单机多卡"
msgstr ""

#: ../../source/user-guide/distribution.rst:19
msgid "单机多卡是最为常用的方式，比如单机四卡、单机八卡，足以支持我们完成大部分模型的训练。"
msgstr ""

#: ../../source/user-guide/distribution.rst:21
msgid "本节我们按照以下顺序进行介绍："
msgstr ""

#: ../../source/user-guide/distribution.rst:23
#: ../../source/user-guide/distribution.rst:29
msgid "如何启动一个单机多卡的训练"
msgstr ""

#: ../../source/user-guide/distribution.rst:24
#: ../../source/user-guide/distribution.rst:51
msgid "数据处理流程"
msgstr ""

#: ../../source/user-guide/distribution.rst:25
msgid "进程间训练状态如何同步"
msgstr ""

#: ../../source/user-guide/distribution.rst:26
msgid "如何在多进程环境中将模型保存与加载"
msgstr ""

#: ../../source/user-guide/distribution.rst:31
msgid "我们提供了一个单机多卡的启动器。代码示例："
msgstr ""

#: ../../source/user-guide/distribution.rst:53
msgid ""
"用 :class:`~.distributed.launcher` 启动之后，我们便可以按照正常的流程进行训练了， "
"但是由于需要每个进程处理不同的数据，我们还需要在数据部分做一些额外的操作。"
msgstr ""

#: ../../source/user-guide/distribution.rst:56
msgid ""
"在这里我们以载入 MNIST 数据为例，展示如何对数据做切分，使得每个进程拿到不重叠的数据。 此处我们将整个数据集载入内存后再进行切分。 "
"这种方式比较低效，仅作为原理示意，更加高效的方式见 :ref:`dist_dataloader` 。"
msgstr ""

#: ../../source/user-guide/distribution.rst:71
msgid "至此我们便得到了每个进程各自负责的、互不重叠的数据部分。"
msgstr ""

#: ../../source/user-guide/distribution.rst:74
msgid "参数同步"
msgstr ""

#: ../../source/user-guide/distribution.rst:76
msgid "初始化模型的参数之后，我们可以调用 :func:`~.distributed.bcast_list_` 对进程间模型的参数进行广播同步："
msgstr ""

#: ../../source/user-guide/distribution.rst:85
msgid "在反向传播求梯度的步骤中，我们通过插入 callback 函数的形式， 对各个进程计算出的梯度进行累加，各个进程都拿到的是累加后的梯度。代码示例："
msgstr ""

#: ../../source/user-guide/distribution.rst:100
msgid "模型保存与加载"
msgstr ""

#: ../../source/user-guide/distribution.rst:102
msgid "在 MegEngine 中，依赖于上面提到的状态同步机制，我们保持了各个进程状态的一致， 因此可以很容易地实现模型的保存和加载。"
msgstr ""

#: ../../source/user-guide/distribution.rst:105
msgid ""
"对于加载，我们只要在主进程（rank 0 进程）中加载模型参数， 然后调用 :func:`~.distributed.bcast_list_` "
"对各个进程的参数进行同步，就保持了各个进程的状态一致。"
msgstr ""

#: ../../source/user-guide/distribution.rst:108
msgid "对于保存，由于我们在梯度计算中插入了 callback 函数对各个进程的梯度进行累加， 所以我们进行参数更新后的参数还是一致的，可以直接保存。"
msgstr ""

#: ../../source/user-guide/distribution.rst:111
msgid "可以参考以下示例代码实现："
msgstr ""

#: ../../source/user-guide/distribution.rst:135
msgid "使用 DataLoader 进行数据加载"
msgstr ""

#: ../../source/user-guide/distribution.rst:137
msgid ""
"在上一节，为了简单起见，我们将整个数据集全部载入内存。 实际中，我们可以通过 "
":class:`~.megengine.data.DataLoader` 来更高效地加载数据。"
msgstr ""

#: ../../source/user-guide/distribution.rst:140
msgid ""
":class:`~.megengine.data.DataLoader` 会自动帮我们处理分布式训练时数据相关的问题， "
"可以实现使用单卡训练时一样的数据加载代码，具体来说："
msgstr ""

#: ../../source/user-guide/distribution.rst:143
msgid ""
"所有采样器 :class:`~.megengine.data.Sampler` 都会自动地做类似上文中数据切分的操作， "
"使得所有进程都能获取互不重复的数据。"
msgstr ""

#: ../../source/user-guide/distribution.rst:145
msgid ""
"每个进程的 :class:`~.megengine.data.DataLoader` 还会自动调用分布式相关接口实现内存共享， "
"避免不必要的内存占用，从而显著加速数据读取。"
msgstr ""

#: ../../source/user-guide/distribution.rst:148
msgid ""
"总之，在分布式训练时，你无需对使用 :class:`~.megengine.data.DataLoader` 的方式进行任何修改， "
"一切都能无缝地切换。完整的例子见 MegEngine/Models_ 存储库。"
msgstr ""

#: ../../source/user-guide/distribution.rst:154
msgid "多机多卡"
msgstr ""

#: ../../source/user-guide/distribution.rst:156
msgid ""
"在 MegEngine 中，我们能很方便地将上面单机多卡的代码修改为多机多卡， 只需修改传给 "
":func:`~.megengine.distributed.launcher` 的参数就可以进行多机多卡训练"
msgstr ""

#: ../../source/user-guide/distribution.rst:180
msgid ""
"其中 ``world_size`` 是你训练的用到的总卡数， ``n_gpus`` 是你运行时这台物理机的卡数， ``rank_start`` "
"是这台机器的 rank 起始值，``master_ip`` 是 rank 0 所在机器的 ip 地址， ``port`` 是分布式训练 "
"master server 使用的端口号"
msgstr ""

#: ../../source/user-guide/distribution.rst:184
msgid "其它部分与单机版本完全相同。最终只需在每个机器上执行相同的 Python 程序，即可实现多机多卡的分布式训练。"
msgstr ""

#: ../../source/user-guide/distribution.rst:187
msgid "模型并行"
msgstr ""

#: ../../source/user-guide/distribution.rst:189
msgid "在 MegEngine 中，也支持模型并行的方式来做训练。"
msgstr ""

#: ../../source/user-guide/distribution.rst:191
msgid "最简单的模型并行就是把一个模型拆分成上下两个部分来做，在 MegEngine 中可以简单的实现。"
msgstr ""

#: ../../source/user-guide/distribution.rst:193
msgid "下面是一个简单的例子来展示怎么写一个模型并行的训练："
msgstr ""

