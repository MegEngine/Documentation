
msgid ""
msgstr ""
"Project-Id-Version:  megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-04-19 16:51+0800\n"
"PO-Revision-Date: 2022-02-17 05:30+0000\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/reference/api/megengine.functional.distributed.all_reduce_min.rst:2
msgid "megengine.functional.distributed.all\\_reduce\\_min"
msgstr ""

#: megengine.distributed.functional.all_reduce_min:1 of
msgid ""
"Reduce tensors with min operation on each value across the specified "
"group."
msgstr ""

#: megengine.distributed.functional.all_reduce_min:3 of
msgid ""
"``inp`` tensor must have identical shape in all processes across the "
"group."
msgstr ""

#: megengine.distributed.functional.all_reduce_min of
msgid "参数"
msgstr ""

#: megengine.distributed.functional.all_reduce_min:6 of
msgid "tensor to be reduced."
msgstr ""

#: megengine.distributed.functional.all_reduce_min of
msgid "关键字参数"
msgstr ""

#: megengine.distributed.functional.all_reduce_min:9 of
msgid ""
"the process group to work on. Default: ``WORLD``. ``WORLD`` group selects"
" all processes available. list of process rank as parameter will create a"
" new group to work on."
msgstr ""

#: megengine.distributed.functional.all_reduce_min:13 of
msgid ""
"the specific device to execute this operator. Default: ``None`` ``None`` "
"will select the device of ``inp`` to execute. Specially, ``GPU`` device "
"can assign a different stream to execute by adding a number right after a"
" colon following the device name while ``:0`` denotes default stream of "
"GPU, otherwise will use default stream."
msgstr ""

#: megengine.distributed.functional.all_reduce_min of
msgid "返回类型"
msgstr ""

#: megengine.distributed.functional.all_reduce_min:20 of
msgid ":py:class:`~megengine.tensor.Tensor`"
msgstr ""

#: megengine.distributed.functional.all_reduce_min of
msgid "返回"
msgstr ""

#: megengine.distributed.functional.all_reduce_min:21 of
msgid ""
"A tensor with min operation on each value across the group.  The shape of"
" the output tensor must be the same as ``inp``, and the output tensor is "
"going to be bitwise identical in all processes across the group."
msgstr ""

#: megengine.distributed.functional.all_reduce_min:27 of
msgid "实际案例"
msgstr ""

#~ msgid "返回类型"
#~ msgstr "返回类型"

#~ msgid ":py:class:`~megengine.tensor.Tensor`"
#~ msgstr ":py:class:`~megengine.tensor.Tensor`"

#~ msgid "Create all_reduce_min operator for collective communication."
#~ msgstr "创建用于聚合通信的 all_reduce_min 算子。"

#~ msgid "communication group."
#~ msgstr "通信组。"

#~ msgid "execution device."
#~ msgstr "执行设备。"

#~ msgid "Reduce tensors across the specified group by min."
#~ msgstr ""

#~ msgid "Input tensor."
#~ msgstr ""

#~ msgid ""
#~ "The process group to work on. The"
#~ " default group is WORLD which means"
#~ " all processes available. You can use"
#~ " a list of process ranks to "
#~ "create new group to work on it,"
#~ " e.g. [1, 3, 5]."
#~ msgstr ""

#~ msgid ""
#~ "The specific device to execute this "
#~ "operator. None default device means the"
#~ " device of inp will be used. "
#~ "Specify \"gpu0:1\" to execute this "
#~ "operator on diffrent cuda stream, 1 "
#~ "is stream id, and default stream "
#~ "id is 0."
#~ msgstr ""

#~ msgid "Result tensor."
#~ msgstr ""

#~ msgid "A tensor with min operation on each value across the group."
#~ msgstr ""

#~ msgid ""
#~ "The shape of the output tensor "
#~ "must be the same as ``inp``, and"
#~ " the output tensor is going to "
#~ "be bitwise identical in all processes"
#~ " across the group."
#~ msgstr ""

