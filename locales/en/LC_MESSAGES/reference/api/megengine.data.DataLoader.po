msgid ""
msgstr ""
"Project-Id-Version: megengine-documentation\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-12-24 23:51+0800\n"
"PO-Revision-Date: 2023-04-20 08:27\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"X-Crowdin-Project: megengine-documentation\n"
"X-Crowdin-Project-ID: 582157\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /dev/locales/zh_CN/LC_MESSAGES/reference/api/megengine.data.DataLoader.po\n"
"X-Crowdin-File-ID: 221\n"

#: ../../source/reference/api/megengine.data.DataLoader.rst:5
msgid "DataLoader"
msgstr ""

#: megengine.data.dataloader.DataLoader:1 of
msgid "Provides a convenient way to iterate on a given dataset. The process is as follows:"
msgstr ""

#: megengine.data.dataloader.DataLoader:13 of
msgid "DataLoader combines a :class:`~.Dataset` with :class:`~.Sampler`, :class:`~.Transform` and :class:`~.Collator`, make it flexible to get minibatch continually from a dataset. See :ref:`data-guide` for more details."
msgstr ""

#: megengine.data.dataloader.DataLoader of
msgid "参数"
msgstr ""

#: megengine.data.dataloader.DataLoader:19 of
msgid "dataset from which to load the minibatch."
msgstr ""

#: megengine.data.dataloader.DataLoader:21 of
msgid "defines the strategy to sample data from the dataset. If ``None``, it will sequentially sample from the dataset one by one."
msgstr ""

#: megengine.data.dataloader.DataLoader:24 of
msgid "defined the transforming strategy for a sampled batch."
msgstr ""

#: megengine.data.dataloader.DataLoader:26 of
msgid "defined the merging strategy for a transformed batch."
msgstr ""

#: megengine.data.dataloader.DataLoader:28 of
msgid "the number of sub-process to load, transform and collate the batch. ``0`` means using single-process. Default: 0"
msgstr ""

#: megengine.data.dataloader.DataLoader:31 of
msgid "if positive, means the timeout value(second) for collecting a batch from workers. Default: 0"
msgstr ""

#: megengine.data.dataloader.DataLoader:34 of
msgid "whether to enable the preloading strategy of the dataloader. When enabling, the dataloader will preload one batch to the device memory to speed up the whole training process."
msgstr ""

#: megengine.data.dataloader.DataLoader:37 of
msgid "whether to splitting workload across all workers when dataset is streamdataset and num_workers > 0. When enabling, each worker will collect data from different dataset in order to speed up the whole loading process. See ref:`streamdataset-example` for more details"
msgstr ""

#: megengine.data.dataloader.DataLoader:41 of
msgid "The effect of enabling preload"
msgstr ""

#: megengine.data.dataloader.DataLoader:44 of
msgid "All elements in :class:`map`, :class:`list`, and :class:`tuple` will be converted to :class:`~.Tensor` by preloading, and you will get :class:`~.Tensor` instead of the original Numpy array or Python built-in data structrure."
msgstr ""

#: megengine.data.dataloader.DataLoader:46 of
msgid "Tensors' host2device copy and device kernel execution will be overlapped, which will improve the training speed at the cost of **higher device memory usage** (due to one more batch data on device memory). This feature saves more time when your NN training time is short or your machine's host PCIe bandwidth for each device is low."
msgstr ""

#~ msgid "基类：:class:`object`"
#~ msgstr "基类：:class:`object`"

#~ msgid ""
#~ ":obj:`__init__ <megengine.data.DataLoader.__init__>`\\ "
#~ "\\(dataset\\[\\, sampler\\, transform\\, ...\\]\\)"
#~ msgstr ""
#~ ":obj:`__init__ <megengine.data.DataLoader.__init__>`\\ "
#~ "\\(dataset\\[\\, sampler\\, transform\\, ...\\]\\)"

#~ msgid ""
#~ "`DataLoader` combines a dataset with "
#~ "`sampler`, `transform` and `collator`, make"
#~ " it flexible to get minibatch "
#~ "continually from a dataset."
#~ msgstr ""
#~ "DataLoader 将数据集与sampler 、transform 、 collator"
#~ " 结合起来，从而能够灵活地从数据集中连续获得拼成 minibatch 的数据。"

#~ msgid "megengine.data.DataLoader"
#~ msgstr "megengine.data.DataLoader"

#~ msgid "Methods"
#~ msgstr "方法"

#~ msgid ""
#~ "Defines whether to apply the preloading"
#~ " strategy of dataloader, and parallelize"
#~ " the copy of host2device while kernal"
#~ " is executed to improve the loading"
#~ " speed. default is seted False the"
#~ " output will change from np.ndarry to"
#~ " dtype tensor. the support dtypes for"
#~ " preload are "
#~ "int,float,list[int,float],tuple[int,float],and another "
#~ "type is not supported."
#~ msgstr ""
#~ "定义是否应用 dataloader 的预加载策略，并在 kernel 执行时并行化 "
#~ "host2device 以提高加载速度。 默认设置为 False，输出将从 "
#~ "np.ndarry 更改为 dtype 张量。 预加载支持的 dtype "
#~ "是 int,float,list[int,float],tuple[int,float]，不支持其他类型。"

#~ msgid ""
#~ "By enabling preload, tensors' host2device "
#~ "copy and device kernel execution will"
#~ " be overlapped, which will improve "
#~ "the training speed at the cost of"
#~ " higher device memory usage (due to"
#~ " one more batch data on device "
#~ "memory). This feature saves more time"
#~ " when your NN training time is "
#~ "short or your machine's host PCIe "
#~ "bandwidth for each device is low."
#~ msgstr ""
#~ "通过开启预加载策略， tensor 的 host2device 拷贝和 "
#~ "device 上 kernel 执行将会部分重叠， 这将会加速训练， "
#~ "代价是较高的 device 内存占用（因为会多出一个 batch 的数据）。 "
#~ "当你的训练时间很短或者你的每个 device 的 PCIe 带宽很低的时候， "
#~ "这个策略将会省很多训练的时间。"

#~ msgid "callback function triggered by timeout, default to raise runtime error."
#~ msgstr "超时触发的回调函数，默认引发运行时错误。"

#~ msgid ""
#~ "define the paralleling strategy in "
#~ "multi-processing mode. ``True`` means one"
#~ " batch is divided into :attr:`num_workers`"
#~ " pieces, and the workers will process"
#~ " these pieces parallelly. ``False`` means"
#~ " different sub-process will process "
#~ "different batch. Default: False"
#~ msgstr ""
#~ "定义多进程模式下的并行策略。 ``True`` 表示一个批次被分成 "
#~ ":attr:`num_workers` 块，worker 将并行处理这些块。 ``False`` "
#~ "表示不同的子进程会处理不同的批次。 默认：False"

