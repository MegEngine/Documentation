
msgid ""
msgstr ""
"Project-Id-Version:  megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-17 20:15+0800\n"
"PO-Revision-Date: 2021-06-03 10:20+0000\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/reference/api/megengine.functional.nn.logsoftmax.rst:2
msgid "megengine.functional.nn.logsoftmax"
msgstr ""

#: megengine.functional.nn.logsoftmax:1 of
msgid ""
"Applies the :math:`\\log(\\text{softmax}(x))` function to an "
"n-dimensional input tensor. The :math:`\\text{logsoftmax}(x)` formulation"
" can be simplified as:"
msgstr ""

#: megengine.functional.nn.logsoftmax:4 of
msgid ""
"\\text{logsoftmax}(x_{i}) = \\log(\\frac{\\exp(x_i) }{ \\sum_j "
"\\exp(x_j)} )\n"
"\n"
msgstr ""

#: megengine.functional.nn.logsoftmax:7 of
msgid "For numerical stability the implementation follows this transformation:"
msgstr ""

#: megengine.functional.nn.logsoftmax:9 of
msgid ""
"\\text{logsoftmax}(x)\n"
"= \\log (\\frac{\\exp (x)}{\\sum_{i}(\\exp (x_{i}))})\n"
"= x - \\log (\\sum_{i}(\\exp (x_{i})))\n"
"= x - \\text{logsumexp}(x)\n"
"\n"
msgstr ""

#: megengine.functional.nn.logsoftmax of
msgid "参数"
msgstr ""

#: megengine.functional.nn.logsoftmax:16 of
msgid "input tensor."
msgstr ""

#: megengine.functional.nn.logsoftmax:18 of
msgid "axis along which :math:`\\text{logsoftmax}(x)` will be applied."
msgstr ""

#: megengine.functional.nn.logsoftmax:20 of
msgid "Examples:"
msgstr ""

#: megengine.functional.nn.logsoftmax:32 of
msgid "Outputs:"
msgstr ""

#: megengine.functional.nn.logsoftmax of
msgid "返回类型"
msgstr ""

#: megengine.functional.nn.logsoftmax:41 of
msgid ":py:class:`~megengine.tensor.Tensor`"
msgstr ""

#~ msgid ""
#~ "Applies the :math:`\\log(\\text{softmax}(x))` "
#~ "function to an n-dimensional input "
#~ "tensor. The :math:`\\text{logsoftmax}(x)` "
#~ "formulation can be simplified as:"
#~ msgstr ""

#~ msgid ""
#~ "\\text{logsoftmax}(x_{i}) = \\log(\\frac{\\exp(x_i) "
#~ "}{ \\sum_j \\exp(x_j)} )\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "\\text{logsoftmax}(x)\n"
#~ "= \\log (\\frac{\\exp (x)}{\\sum_{i}(\\exp (x_{i}))})\n"
#~ "= x - \\log (\\sum_{i}(\\exp (x_{i})))\n"
#~ "= x - \\text{logsumexp}(x)\n"
#~ "\n"
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "axis along which :math:`\\text{logsoftmax}(x)` will be applied."
#~ msgstr ""

#~ msgid ""
#~ "Applies the :math:`\\log(    ext{softmax}(x))` "
#~ "function to an n-dimensional input "
#~ "tensor. The :math:`   ext{logsoftmax}(x)` "
#~ "formulation can be simplified as:"
#~ msgstr ""

#~ msgid ""
#~ "ext{logsoftmax}(x_{i}) = \\log(\n"
#~ "\n"
#~ msgstr ""

#~ msgid "rac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} )"
#~ msgstr ""

#~ msgid ""
#~ "ext{logsoftmax}(x)\n"
#~ "= \\log (\n"
#~ "\n"
#~ msgstr ""

#~ msgid "rac{\\exp (x)}{\\sum_{i}(\\exp (x_{i}))})"
#~ msgstr ""

#~ msgid "= x - \\log (\\sum_{i}(\\exp (x_{i}))) = x -   ext{logsumexp}(x)"
#~ msgstr ""

#~ msgid "param inp"
#~ msgstr ""

#~ msgid "param axis"
#~ msgstr ""

#~ msgid "axis along which :math:`       ext{logsoftmax}(x)` will be applied."
#~ msgstr ""

