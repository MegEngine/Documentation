
msgid ""
msgstr ""
"Project-Id-Version:  megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-28 09:14+0000\n"
"PO-Revision-Date: 2021-11-09 13:29+0000\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/reference/api/megengine.optimizer.Adam.rst:5
msgid "Adam"
msgstr ""

#: megengine.optimizer.adam.Adam:1 of
msgid ""
"Implements Adam algorithm proposed in `\"Adam: A Method for Stochastic "
"Optimization\" <https://arxiv.org/abs/1412.6980>`_."
msgstr ""

#: megengine.optimizer.adam.Adam of
msgid "参数"
msgstr ""

#: megengine.optimizer.adam.Adam:4 of
msgid "iterable of parameters to optimize or dicts defining parameter groups."
msgstr ""

#: megengine.optimizer.adam.Adam:7 of
msgid ""
"learning rate. betas: coefficients used for computing running averages of"
" gradient and its square. Default: (0.9, 0.999)"
msgstr ""

#: megengine.optimizer.adam.Adam:11 of
msgid ""
"term added to the denominator to improve numerical stability. Default: "
"1e-8"
msgstr ""

#: megengine.optimizer.adam.Adam:13 of
msgid "weight decay (L2 penalty). Default: 0"
msgstr ""

#~ msgid "基类：:class:`megengine.optimizer.optimizer.Optimizer`"
#~ msgstr "基类：:class:`megengine.optimizer.optimizer.Optimizer`"

#~ msgid ""
#~ ":obj:`__init__ <megengine.optimizer.Adam.__init__>`\\ "
#~ "\\(params\\, lr\\[\\, betas\\, eps\\, "
#~ "weight\\_decay\\]\\)"
#~ msgstr ""
#~ ":obj:`__init__ <megengine.optimizer.Adam.__init__>`\\ "
#~ "\\(params\\, lr\\[\\, betas\\, eps\\, "
#~ "weight\\_decay\\]\\)"

#~ msgid "Initialize self."
#~ msgstr "初始化方法。"

#~ msgid "learning rate."
#~ msgstr "学习率(learning rate)。"

#~ msgid "megengine.optimizer.Adam"
#~ msgstr ""

#~ msgid "Methods"
#~ msgstr ""

#~ msgid ""
#~ ":obj:`add_param_group "
#~ "<megengine.optimizer.Adam.add_param_group>`\\ "
#~ "\\(param\\_group\\)"
#~ msgstr ""

#~ msgid ""
#~ "Add a param group to ``param_groups``"
#~ " of the :class:`~megengine.optim.optimizer.Optimizer`."
#~ msgstr ""

#~ msgid ":obj:`backward <megengine.optimizer.Adam.backward>`\\ \\(loss\\)"
#~ msgstr ""

#~ msgid ":obj:`bcast_param <megengine.optimizer.Adam.bcast_param>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":obj:`clear_grad <megengine.optimizer.Adam.clear_grad>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Set the grad attribute to None for all parameters."
#~ msgstr ""

#~ msgid ""
#~ ":obj:`load_state_dict "
#~ "<megengine.optimizer.Adam.load_state_dict>`\\ \\(state\\)"
#~ msgstr ""

#~ msgid "Loads the optimizer state."
#~ msgstr ""

#~ msgid ""
#~ ":obj:`state_dict <megengine.optimizer.Adam.state_dict>`\\ "
#~ "\\(\\[keep\\_var\\]\\)"
#~ msgstr ""

#~ msgid "Export the optimizer state."
#~ msgstr ""

#~ msgid ":obj:`step <megengine.optimizer.Adam.step>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Performs a single optimization step."
#~ msgstr ""

#~ msgid ":obj:`zero_grad <megengine.optimizer.Adam.zero_grad>`\\ \\(\\)"
#~ msgstr ""

