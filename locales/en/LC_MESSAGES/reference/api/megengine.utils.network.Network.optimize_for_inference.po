
msgid ""
msgstr ""
"Project-Id-Version:  megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-11-08 21:51+0800\n"
"PO-Revision-Date: 2021-08-18 08:45+0000\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/reference/api/megengine.utils.network.Network.optimize_for_inference.rst:2
msgid "megengine.utils.network.Network.optimize\\_for\\_inference"
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:1 of
msgid "Applies optimize_for_inference pass for operator graph."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference of
msgid "参数"
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:3 of
msgid "list of output vars in the operator graph"
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:5 of
msgid "Keyword Arguments:"
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:7 of
msgid ""
"enable_io16xc32 -- whether to use float16 for I/O between oprs and use "
"float32 as internal computation precision. Note the output var would be "
"changed to float16."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:11 of
msgid ""
"enable_ioc16 -- whether to use float16 for both I/O and computation "
"precision."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:14 of
msgid ""
"enable_hwcd4 -- whether to use NHWCD4 data layout. This is faster on some"
" OpenCL backend."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:17 of
msgid ""
"enable_nchw88 -- whether to use NCHW88 data layout, currently used in X86"
" AVX backend."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:20 of
msgid ""
"enable_nchw44 -- whether to use NCHW44 data layout, currently used in arm"
" backend."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:23 of
msgid ""
"enable_nchw44_dot -- whether to use NCHW44_dot data layout, currently "
"used in armv8.2+dotprod backend."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:26 of
msgid ""
"enable_nchw4 -- whether to use NCHW4 data layout, currently used in "
"nvidia backend(based on cudnn)."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:29 of
msgid ""
"enable_nchw32 -- whether to use NCHW32 data layout, currently used in "
"nvidia backend with tensorcore(based on cudnn)."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:32 of
msgid ""
"enable_chwn4 -- whether to use CHWN4 data layout, currently used in "
"nvidia backend with tensorcore."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:35 of
msgid ""
"enable_nchw64 -- whether to use NCHW64 data layout, used for fast int4 "
"support on Nvidia GPU."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:38 of
msgid ""
"enable_fuse_conv_bias_nonlinearity: whether to fuse conv+bias+nonlinearty"
" into one opr."
msgstr ""

#: megengine.utils.network.Network.optimize_for_inference:40 of
msgid ""
"enable_fuse_conv_bias_with_z: whether to fuse conv_bias with z input for "
"inference on nvidia backend(this optimization pass will result in "
"mismatch of the precision of output of training and inference)"
msgstr ""

#~ msgid "param dest_vars"
#~ msgstr ""

#~ msgid "Keyword Arguments"
#~ msgstr ""

#~ msgid "enable_io16xc32 --"
#~ msgstr ""

#~ msgid ""
#~ "whether to use float16 for I/O "
#~ "between oprs and use float32 as "
#~ "internal computation precision. Note the "
#~ "output var would be changed to "
#~ "float16."
#~ msgstr ""

#~ msgid "enable_ioc16 --"
#~ msgstr ""

#~ msgid "whether to use float16 for both I/O and computation precision."
#~ msgstr ""

#~ msgid "enable_hwcd4 --"
#~ msgstr ""

#~ msgid ""
#~ "whether to use NHWCD4 data layout. "
#~ "This is faster on some OpenCL "
#~ "backend."
#~ msgstr ""

#~ msgid "enable_nchw88 --"
#~ msgstr ""

#~ msgid "whether to use NCHW88 data layout, currently used in X86 AVX backend."
#~ msgstr ""

#~ msgid "enable_nchw44 --"
#~ msgstr ""

#~ msgid "whether to use NCHW44 data layout, currently used in arm backend."
#~ msgstr ""

#~ msgid "enable_nchw44_dot --"
#~ msgstr ""

#~ msgid ""
#~ "whether to use NCHW44_dot data layout,"
#~ " currently used in armv8.2+dotprod backend."
#~ msgstr ""

#~ msgid "enable_nchw4 --"
#~ msgstr ""

#~ msgid ""
#~ "whether to use NCHW4 data layout, "
#~ "currently used in nvidia backend(based "
#~ "on cudnn)."
#~ msgstr ""

#~ msgid "enable_nchw32 --"
#~ msgstr ""

#~ msgid ""
#~ "whether to use NCHW32 data layout, "
#~ "currently used in nvidia backend with"
#~ " tensorcore(based on cudnn)."
#~ msgstr ""

#~ msgid "enable_chwn4 --"
#~ msgstr ""

#~ msgid ""
#~ "whether to use CHWN4 data layout, "
#~ "currently used in nvidia backend with"
#~ " tensorcore."
#~ msgstr ""

#~ msgid ""
#~ "enable_fuse_conv_bias_nonlinearity: whether to fuse"
#~ " conv+bias+nonlinearty"
#~ msgstr ""

#~ msgid "into one opr."
#~ msgstr ""

#~ msgid "enable_fuse_conv_bias_with_z: whether to fuse conv_bias with z"
#~ msgstr ""

#~ msgid ""
#~ "input for inference on nvidia "
#~ "backend(this optimization pass will result "
#~ "in mismatch of the precision of "
#~ "output of training and inference)"
#~ msgstr ""

