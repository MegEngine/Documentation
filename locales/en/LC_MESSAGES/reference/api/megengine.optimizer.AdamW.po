msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-05-12 09:02+0800\n"
"PO-Revision-Date: 2021-06-03 08:23\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/zh_CN/LC_MESSAGES/reference/api/megengine.optimizer.AdamW.po\n"
"X-Crowdin-File-ID: 5910\n"
"Language: en_US\n"

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:2
msgid "megengine.optimizer.AdamW"
msgstr ""

#: megengine.optimizer.adamw.AdamW:1 of
msgid "Implements AdamW algorithm proposed in `\"Decoupled Weight Decay Regularization\" <https://arxiv.org/abs/1711.05101>`_."
msgstr ""

#: megengine.optimizer.adamw.AdamW of
msgid "参数"
msgstr ""

#: megengine.optimizer.adamw.AdamW:3 of
msgid "iterable of parameters to optimize or dicts defining parameter groups."
msgstr ""

#: megengine.optimizer.adamw.AdamW:5 of
msgid "learning rate."
msgstr ""

#: megengine.optimizer.adamw.AdamW:6 of
msgid "coefficients used for computing running averages of gradient and its square. Default: (0.9, 0.999)"
msgstr ""

#: megengine.optimizer.adamw.AdamW:8 of
msgid "term added to the denominator to improve numerical stability Default: 1e-8"
msgstr ""

#: megengine.optimizer.adamw.AdamW:10 of
msgid "weight decay (L2 penalty). Default: 1e-2"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:11
msgid "Methods"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid ":obj:`add_param_group <megengine.optimizer.AdamW.add_param_group>`\\ \\(param\\_group\\)"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid "Add a param group to ``param_groups`` of the :class:`~megengine.optim.optimizer.Optimizer`."
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid ":obj:`backward <megengine.optimizer.AdamW.backward>`\\ \\(loss\\)"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid ":obj:`bcast_param <megengine.optimizer.AdamW.bcast_param>`\\ \\(\\)"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid ":obj:`clear_grad <megengine.optimizer.AdamW.clear_grad>`\\ \\(\\)"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid "Set the grad attribute to None for all parameters."
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid ":obj:`load_state_dict <megengine.optimizer.AdamW.load_state_dict>`\\ \\(state\\)"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid "Loads the optimizer state."
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid ":obj:`state_dict <megengine.optimizer.AdamW.state_dict>`\\ \\(\\[keep\\_var\\]\\)"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid "Export the optimizer state."
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid ":obj:`step <megengine.optimizer.AdamW.step>`\\ \\(\\)"
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid "Performs a single optimization step."
msgstr ""

#: ../../source/reference/api/megengine.optimizer.AdamW.rst:38:<autosummary>:1
msgid ":obj:`zero_grad <megengine.optimizer.AdamW.zero_grad>`\\ \\(\\)"
msgstr ""

