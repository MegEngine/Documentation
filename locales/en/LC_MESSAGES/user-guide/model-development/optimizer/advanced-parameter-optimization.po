msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-21 10:52+0800\n"
"PO-Revision-Date: 2021-11-12 00:51\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/model-development/optimizer/advanced-parameter-optimization.po\n"
"X-Crowdin-File-ID: 8171\n"
"Language: en_US\n"

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:5
msgid "参数优化进阶配置"
msgstr "Parameter optimization advanced configuration"

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:7
msgid "假定网络使用如下优化器进行训练："
msgstr "Assume that the network uses the following optimizer for training："

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:17
msgid "这个优化器对所有参数都使用同一学习速率进行优化，本章将介绍如何做到对不同的参数采用不同的学习速率。"
msgstr "This optimizer uses the same learning rate to optimize all parameters. This chapter will introduce how to use different learning rates for different parameters."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:21
msgid "不同参数使用不同的学习速率"
msgstr "Different parameters use different learning rates"

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:23
msgid ":class:`~.megengine.optimizer.Optimizer` 支持将网络的参数进行分组， 不同的参数组可以采用不同的学习速率进行训练。 一个参数组由一个字典表示，这个字典中有"
msgstr ":class:`~.megengine.optimizer.Optimizer` supports grouping the parameters of the network, and different parameter groups can be trained with different learning rates. A parameter group is represented by a dictionary with"

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:26
msgid "``'params': param_list`` ，用来指定参数组包含的参数。此键值对必须有。"
msgstr "``'params': param_list``, used to specify the parameters included in the parameter group. This key-value pair must be present."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:28
msgid "``'lr': learning_rate`` ，用来指定此参数组的学习速率。此键值对有时可省略，省略后参数组的学习速率由优化器指定。"
msgstr "``'lr': learning_rate`` is used to specify the learning rate of this parameter group. This key-value pair can sometimes be omitted, and the learning rate of the parameter group after it is omitted is specified by the optimizer."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:30
msgid "所有待优化参数组的字典会组成一个列表作为 :class:`~.megengine.optimizer.Optimizer` 实例化时的第一个参数传入。"
msgstr "The dictionaries of all parameter groups to be optimized will form a list as :class:`~.megengine.optimizer.Optimizer` is instantiated."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:32
msgid "为了更好的说明参数组，我们首先使用 :class:`~.megengine.module.Module` 提供的 :meth:`~.megengine.module.Module.named_parameters` 函数来对网络参数进行分组。 这个函数返回一个包含网络所有参数并且以参数名字为键、参数变量为值的字典："
msgstr "In order to better illustrate parameters, we first use the :class:`~ .megengine.module.Module` provided :meth:` ~ .megengine.module.Module.named_parameters` function to group network parameters. This function returns a dictionary that contains all the parameters of the network and uses the parameter name as the key and the parameter variable as the value："

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:54
msgid "根据参数的名字我们可以将 ``LeNet`` 中所有卷积的参数分为一组，所有全连接层的参数分为另一组："
msgstr "The parameter name `` LeNet`` we can convolution of all the parameters into a set of parameters for all layers fully connected into another group："

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:67
msgid "分组后即可根据下述代码对不同参数组设置不同的学习速率："
msgstr "According to the following code to set the different parameters grouped set different learning rate："

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:82
msgid "优化器中设置的参数组列表对应于 :attr:`~.megengine.Optimizer.param_groups` 属性。 我们可以通过其获取不同参数组的学习速率。"
msgstr "The list of parameter groups set in the optimizer corresponds to the :attr:`~.megengine.Optimizer.param_groups` attribute. We can get the learning rate of different parameter groups through it."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:97
msgid "训练中对学习速率的更改"
msgstr "Changes to the learning rate during training"

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:99
msgid "MegEngine 也支持在训练过程中对学习速率进行修改，比如部分参数训练到一定程度后就不再需要优化， 此时将对应参数组的学习速率设为零即可。我们修改训练代码进行示例说明。 修改后的训练代码总共训练四个epoch，我们会在第二个epoch结束时将所有全连接层参数的学习速率置零， 并在每个epoch当中输出 ``LeNet`` 中全连接层的部分参数值以显示是否被更新。"
msgstr "MegEngine also supports the modification of the learning rate during the training process. For example, some parameters do not need to be optimized after training to a certain level. At this time, the learning rate of the corresponding parameter group can be set to zero. We modify the training code to illustrate with examples. The modified training code trains a total of four epochs. At the end of the second epoch, we will zero the learning rate of all fully connected layer parameters, and output the part of the fully connected layer in ``LeNet`` in each epoch The parameter value shows whether it is updated."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:132
msgid "从输出可以看到在学习速率设为0之前参数值是在不断更新的，但是在设为0之后参数值就不再变化。"
msgstr "It can be seen from the output that the parameter value is continuously updated before the learning rate is set to 0, but the parameter value does not change after it is set to 0."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:134
msgid "同时多数网络在训练当中会不断减小学习速率，如下代码展示了 MegEngine 是如何在训练过程中线性减小学习速率的："
msgstr "While most networks in the training which will continue to reduce the learning rate, the following code shows how MegEngine learning rate decreases linearly in the training process："

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:147
msgid "不同参数使用不同的优化器"
msgstr "Use different optimizers for different parameters"

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:149
msgid "对于不同的参数，也可以使用不同的优化器对它们分别优化。 对参数的梯度置零（ :meth:`~.megengine.optimizer.Optimizer.clear_grad` ） 和更新（ :meth:`~.megengine.optimizer.Optimizer.step` ）操作， 如果所有优化器都是同时进行的，可以定义一个 ``MultipleOptimizer`` 类。 在初始化时声明多个不同的优化器，在调用置零函数和更新函数时对所有优化器执行对应操作。"
msgstr "For different parameters, you can also use different optimizers to optimize them separately. Zero the parameter gradient ( :meth:`~.megengine.optimizer.Optimizer.clear_grad`) and update ( :meth:`~.megengine.optimizer.Optimizer.step`) operations, if all optimizers are performed at the same time, you can define A ``MultipleOptimizer`` class. Declare multiple different optimizers during initialization, and perform corresponding operations on all optimizers when calling the zeroing function and the update function."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:169
msgid "假设想用 :class:`~.megengine.optimizer.SGD` 优化所有卷积参数， 用 :class:`~.megengine.optimizer.adam.Adam` 优化所有全连接层参数。 可以按照如下方式定义优化器，不需要改变训练代码就可以达到不同的参数使用不同的优化器优化的效果。"
msgstr "Suppose you want to use :class:`~.megengine.optimizer.SGD` to optimize all convolution parameters, and use :class:`~.megengine.optimizer.adam.Adam` to optimize all fully connected layer parameters. The optimizer can be defined as follows, and the effect of using different optimizers to optimize different parameters can be achieved without changing the training code."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:179
msgid "如果不同的参数梯度置零和更新不是同时进行的，你只需要定义多个优化器，在不同的时间调用对应的函数即可。"
msgstr "If different parameter gradients are zeroed and updated at the same time, you only need to define multiple optimizers and call the corresponding functions at different times."

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:182
msgid "固定部分参数不优化"
msgstr "Fixed some parameters not optimized"

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:184
msgid "除了将不训练的参数分为一组并将学习速率设为零外， MegEngine 还提供了其他途径来固定参数不进行优化： 仅将需要优化的参数与求导器和优化器绑定即可。 如下代码所示，仅对 ``LeNet`` 中的卷积参数进行优化："
msgstr "In addition to the training parameters will not be grouped together and learning rate is set to zero outside, MegEngine also offers other ways to fix parameters are not optimized： only the parameters to be optimized and the derivation and optimizer can bind. As shown in the following code, only the convolution parameters in ``LeNet'' are optimized："

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:207
msgid "下述代码将上面的设置加入到了具体训练当中，能够更加直观的看到各个参数的梯度差异："
msgstr "The following code is added to the above provided specific training them, a gradient of the difference can be seen more intuitive respective parameters："

#: ../../source/user-guide/model-development/optimizer/advanced-parameter-optimization.rst:247
msgid "从输出可以看到除了卷积参数有梯度外其余参数均没有梯度也就不会更新。"
msgstr "From the output, it can be seen that except for the convolution parameter, the other parameters have no gradient and will not be updated."

