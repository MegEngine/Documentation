msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-11-08 21:51+0800\n"
"PO-Revision-Date: 2021-11-12 00:50\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/model-development/quantization/index.po\n"
"X-Crowdin-File-ID: 8185\n"
"Language: en_US\n"

#: ../../source/user-guide/model-development/quantization/index.rst:5
msgid "量化（Quantization）"
msgstr "Quantization"

#: ../../source/user-guide/model-development/quantization/index.rst:13
msgid "常见神经网络模型所用的 :ref:`tensor-dtype` 一般是 ``float32`` 类型， 而工业界出于对特定场景的需求，需要把模型转换为像 ``int8`` 这样的低精度/比特类型 —— 整个过程被称为量化（Quantization）。"
msgstr ":ref:`tensor-dtype` used by common neural network models is generally the ``float32'' type, and the industry needs to convert the model to a low-precision/bit type like ``int8'' for specific scenarios. —— The whole process is called Quantization."

#: ../../source/user-guide/model-development/quantization/index.rst:24
msgid "量化能将 32 位的浮点数转换成 8 位甚至是 4 位定点数，具有更少的运行时内存和缓存要求； 另外由于大部分的硬件对于定点运算都有特定的优化，所以在运行速度上也会有较大的提升。 相较于普通模型， **量化模型有着更小的内存容量与带宽占用、更低的功耗和更快的推理速度等优点。**"
msgstr "Quantization can convert 32-bit floating-point numbers into 8-bit or even 4-bit fixed-point numbers, which has less runtime memory and cache requirements; in addition, since most hardware has specific optimizations for fixed-point operations, it is in terms of operating speed. There will also be a big improvement. Compared with ordinary models, **quantized models have the advantages of smaller memory capacity and bandwidth usage, lower power consumption and faster inference speed. **"

#: ../../source/user-guide/model-development/quantization/index.rst:27
msgid "某些计算设备只支持做定点运算。为了让模型可以在这些设备上正常运行，我们需要进行量化处理。"
msgstr "Some computing devices only support fixed-point operations. In order for the model to run normally on these devices, we need to perform quantitative processing."

#: ../../source/user-guide/model-development/quantization/index.rst:29
msgid "“为了追求极致的推理计算速度，从而舍弃了数值表示的精度”，直觉上会带来较大的模型掉点， 但是在通过一系列精妙的量化处理之后，其掉点可以变得微乎其微，并能支持正常的部署使用。"
msgstr "\"In order to pursue the ultimate inference calculation speed, thereby giving up the precision of numerical representation\", intuition will bring a larger model drop point, but after a series of sophisticated quantization processing, the drop point can become minimal, and Can support normal deployment and use."

#: ../../source/user-guide/model-development/quantization/index.rst:32
msgid "用户无需了解背后的实现细节，使用 MegEngine 的 :mod:`~.quantization` 所提供的解决方案，就能满足基本量化需求。 我们为感兴趣的用户提供了更多有关量化基本原理的介绍，可参考 :ref:`quantization-basic-concept` 。"
msgstr "Users do not need to understand the implementation details behind them, and use :mod:`~.quantization` to meet the basic quantization needs. We provide more introduction to the basic principles of quantization for interested users, please refer to :ref:`quantization-basic-concept`."

#: ../../source/user-guide/model-development/quantization/index.rst:35
msgid "熟悉基本原理的用户可直接跳转到 :ref:`megengine-quantization` ↩ 查看基本用法。"
msgstr "Users who are familiar with the basic principles can jump directly to :ref:`megengine-quantization` ↩ to see the basic usage."

#: ../../source/user-guide/model-development/quantization/index.rst:39
msgid "请不要将 “量化” 与 “混合精度（Mixed precision）” 混淆，可参考 :ref:`amp-guide` 文档。"
msgstr "Please do not confuse \"quantization\" with \"Mixed precision\", please refer to the :ref:`amp-guide` document."

#: ../../source/user-guide/model-development/quantization/index.rst:44
msgid "量化基本流程介绍"
msgstr "Introduction to the basic quantification process"

#: ../../source/user-guide/model-development/quantization/index.rst:46
msgid "目前工业界主要应用有两类量化技术，在 MegEngine 中都进行了支持："
msgstr "Currently there are two types of industry applications mainly quantitative techniques, it is carried out in MegEngine support："

#: ../../source/user-guide/model-development/quantization/index.rst:48
msgid "训练后量化（Post-Training Quantization, PTQ）；"
msgstr "Post-Training Quantization (PTQ);"

#: ../../source/user-guide/model-development/quantization/index.rst:49
msgid "量化感知训练（Quantization-Aware Training, QAT）。"
msgstr "Quantization-Aware Training (QAT)."

#: ../../source/user-guide/model-development/quantization/index.rst:51
msgid "训练后量化是将已经训练好的浮点模型转换成低精度/比特模型所使用的通用技术，常见的做法是对模型的权重（weight）和激活值（activation）进行处理， 将它们转换成精度更低的类型。在转换过程中，需要用到待量化模型中权重和激活的一些统计信息，如缩放因子（scale）和零点（zero_point）。 尽管精度转换发生在训练后，但为了获取这些统计信息，我们仍需要在模型训练时 —— 即前向计算的过程中，插入一名观察者（Observer）。"
msgstr "Post-training quantization is a general technique used to convert a trained floating-point model into a low-precision/bit model. A common approach is to process the weight and activation value of the model and convert them to precision Lower type. In the conversion process, some statistical information about the weights and activations in the model to be quantified, such as the scale and zero_point, need to be used. Although the accuracy conversion occurs after training, in order to obtain these statistics, we still need to insert an observer (Observer) during model training, that is, during the forward calculation process."

#: ../../source/user-guide/model-development/quantization/index.rst:55
msgid "使用训练后量化技术，会导致量化后的模型掉点（即预测正确率下降）。严重情况下会导致量化模型不可用。 一种可行的做法是使用小批量数据来在量化前对 Observer 进行校准（Calibration），也叫 Calibration 后量化。"
msgstr "The use of post-training quantization technology will cause the quantized model to drop points (that is, the prediction accuracy rate decreases). In severe cases, the quantitative model will be unavailable. A feasible approach is to use small batches of data to calibrate the Observer before quantification (Calibration), also called post-Calibration quantification."

#: ../../source/user-guide/model-development/quantization/index.rst:58
msgid "另一种可行的改善方案是使用量化感知训练技术，向浮点模型中插入一些伪量化（FakeQuantize）算子作为改造， 在训练时伪量化算子会根据 Observer 观察到的信息进行量化模拟， 即模拟计算过程中数值截断后精度降低的情形，先做一遍数值转换，再将转换后的值还原成原类型。 这样可以让被量化对象在训练时 “提前适应” 量化操作，缓解在训练后量化时带来的掉点影响。"
msgstr "Another feasible improvement plan is to use quantization perception training technology to insert some fake quantization (FakeQuantize) operators into the floating-point model as a transformation. During training, the fake quantize operator will perform quantization simulation based on the information observed by the Observer, that is, When the accuracy of the numerical value is truncated during the simulation calculation, the precision is reduced, and the numerical conversion is performed first, and then the converted value is restored to the original type. This allows the quantized object to \"adapt in advance\" to the quantization operation during training, and alleviate the impact of drop points during quantization after training."

#: ../../source/user-guide/model-development/quantization/index.rst:63
msgid "新增的 FakeQuantize 算子会引入大量的训练开销，为了节省总用时，模型量化更通常的思路是："
msgstr "The new FakeQuantize operator will introduce a lot of training overhead. In order to save the total time, the more general idea of model quantization is："

#: ../../source/user-guide/model-development/quantization/index.rst:65
msgid "按照平时训练模型的流程，设计好 Float 模型并进行训练（等同于得到一个预训练模型）；"
msgstr "According to the usual training model process, the Float model is designed and trained (equivalent to obtaining a pre-training model);"

#: ../../source/user-guide/model-development/quantization/index.rst:66
msgid "插入 Observer 和 FakeQuantize 算子，得到 Quantized-Float 模型（简称 QFloat 模型），量化感知训练；"
msgstr "Insert Observer and FakeQuantize operators to obtain the Quantized-Float model (QFloat model for short), and quantize perception training;"

#: ../../source/user-guide/model-development/quantization/index.rst:67
msgid "进行训练后量化，得到真正的 Quantized 模型（简称 Q 模型），即最终被用作推理的低比特模型。"
msgstr "After training and quantization, the real Quantized model (Q model for short) is obtained, which is the low-bit model that is finally used for inference."

#: ../../source/user-guide/model-development/quantization/index.rst:82
msgid "根据实际情景的一些差异，量化流程可以有灵活的变化，例如："
msgstr "According to some differences in actual scenarios, the quantification process can be flexibly changed, such as："

#: ../../source/user-guide/model-development/quantization/index.rst:84
msgid "在不考虑训练开销的情况下，为了简化整体流程，可以直接构造 QFloat 模型，并进行训练与后量化："
msgstr "After spending without regard to training, in order to simplify the overall process can be constructed directly QFloat model, and training and quantify："

#: ../../source/user-guide/model-development/quantization/index.rst:95
msgid "在构造 QFloat 模型时，如果不插入 FakeQuantize 算子，也可以相应地减少训练开销，提升速度。"
msgstr "When constructing the QFloat model, if the FakeQuantize operator is not inserted, the training overhead can be reduced accordingly and the speed can be increased."

#: ../../source/user-guide/model-development/quantization/index.rst:97
msgid "但是这时等同于未进行量化感知，只进行了数据校准 Calibration, 模型可能会掉点严重："
msgstr "But this time is not equivalent to quantify perception, were only the calibration data Calibration, the model may point out serious："

#: ../../source/user-guide/model-development/quantization/index.rst:108
msgid "对于上述不同情景，在 MegEngine 中可以使用一套统一的接口来对不同的情况进行灵活配置。"
msgstr "For the above different scenarios, a unified set of interfaces can be used in MegEngine to flexibly configure different situations."

#: ../../source/user-guide/model-development/quantization/index.rst:113
msgid "Megengine 量化步骤"
msgstr "Megengine quantification steps"

#: ../../source/user-guide/model-development/quantization/index.rst:115
msgid "在 MegEngine 中，最上层的量化接口是配置如何量化的 :class:`~.quantization.QConfig` 和模型转换模块里的 :func:`~.quantization.quantize_qat` 与 :func:`~.quantization.quantize` . 通过配置 :class:`~.quantization.QConfig` 中所使用的 Observer 和 FakeQuantize 算子，我们可以对量化方案进行自定义。 进一步的说明请参考 :ref:`qconfig-guide` 小节，下面将展示 QAT 量化流程所需的步骤:"
msgstr "In MegEngine, the top-level quantization interface is to configure how to quantize :class: :func:`~.quantization.quantize_qat` and :func:`~.quantization.quantize` in the model conversion module :class:` The Observer and FakeQuantize operators used in ~.quantization.QConfig`, we can customize the quantization scheme. For further instructions, please refer to :ref:`qconfig-guide`, the following will show the steps required for the QAT quantification process:"

#: ../../source/user-guide/model-development/quantization/index.rst:133
msgid ":ref:`module-guide` ，并按照正常的浮点模型方式进行训练，得到预训练模型；"
msgstr ":ref:`module-guide`, and train according to the normal floating-point model to get the pre-trained model;"

#: ../../source/user-guide/model-development/quantization/index.rst:134
msgid "使用 :func:`~.quantization.quantize_qat` 将 Float 模型转换为 QFloat 模型， 这一步会基于量化配置 :class:`~.quantization.QConfig` 设置好 Observer 和 FakeQuantize 算子 （在 MegEngine 中提供了常见的 QConfig :ref:`预设 <qconfig-list>`, 这里使用了 EMA 算法）；"
msgstr "Use :func:`~.quantization.quantize_qat` to convert the Float model to the QFloat model. This step will be based on the quantization configuration :class:`~.quantization.QConfig` to set up the Observer and FakeQuantize operators (common QConfig :ref:Suppose <qconfig-list>`, the EMA algorithm is used here);"

#: ../../source/user-guide/model-development/quantization/index.rst:137
msgid "使用 QFloat 模型继续训练（微调），此时 Obersever 统计信息, FakeQuantize 进行伪量化；"
msgstr "Use the QFloat model to continue training (fine-tuning), at this time Obersever statistics, FakeQuantize for pseudo-quantization;"

#: ../../source/user-guide/model-development/quantization/index.rst:138
msgid "使用 :func:`~.quantization.quantize` 将 QFloat 模型转换为 Q 模型，这一步也叫 “真量化”（相较于伪量化）。 此时网络无法再进行训练，网络中的算子都会转换为低比特计算方式，即可用于部署了。"
msgstr "Use :func:`~.quantization.quantize` to convert the QFloat model to the Q model. This step is also called \"true quantization\" (compared to pseudo quantization). At this time, the network can no longer be trained, and the operators in the network will be converted to low-bit calculation methods, which can be used for deployment."

#: ../../source/user-guide/model-development/quantization/index.rst:152
msgid "我们也可以使用 Calibration 后量化方案，需准备校准数据集（参考代码示范）；"
msgstr "We can also use the post-Calibration quantification scheme, which requires a calibration data set (reference code demonstration);"

#: ../../source/user-guide/model-development/quantization/index.rst:153
msgid "MegEngine 的量化模型可被直接导出用于推理部署，参考 :ref:`dump` 。"
msgstr "The quantitative model of MegEngine can be directly exported for inference deployment, refer to :ref:`dump`."

#: ../../source/user-guide/model-development/quantization/index.rst:155
msgid "完整的 MegEngine 模型量化代码示范可在 :models:`official/quantization` 找到。"
msgstr "The complete MegEngine model quantization code demonstration can be found at :models:`official/quantization`."

#: ../../source/user-guide/model-development/quantization/index.rst:159
msgid "从宏观上看，量化是在 Model 级别之间的转换操作，但掰开细节，则都是对 Module 的处理。"
msgstr "From a macro point of view, quantification is a conversion operation between Model levels, but apart from the details, it is all about the processing of Module."

#: ../../source/user-guide/model-development/quantization/index.rst:161
msgid "对应 Float, QFloat 和 Q Model, MegEngine 中的 Module 可被整理成以下三种："
msgstr "Corresponding to Float, QFloat and Q Model, MegEngine the Module can be organized into three："

#: ../../source/user-guide/model-development/quantization/index.rst:163
msgid "进行正常浮点运算的默认 :class:`~.module.Module` （也即 Float Module ）"
msgstr "The default for normal floating-point operations is :class:`~.module.Module` (also called Float Module)"

#: ../../source/user-guide/model-development/quantization/index.rst:164
msgid "带有 Observer 和 FakeQuantize 算子的 :class:`.qat.QATModule`"
msgstr ":class:`.qat.QATModule` with Observer and FakeQuantize operators"

#: ../../source/user-guide/model-development/quantization/index.rst:165
msgid "无法训练、专门用于部署的 :class:`.quantized.QuantizedModule`"
msgstr ":class:`.quantized.QuantizedModule` that cannot be trained and specifically used for deployment"

#: ../../source/user-guide/model-development/quantization/index.rst:167
msgid "对于其中比较常见的可以被量化的算子，分别有同名的实现如 ——"
msgstr "For the more common operators that can be quantified, there are implementations of the same name, such as-"

#: ../../source/user-guide/model-development/quantization/index.rst:169
msgid ":class:`.module.Linear`, :class:`.module.qat.Linear` 和 :class:`.module.quantized.Linear`"
msgstr ":class:`.module.Linear`, :class:`.module.qat.Linear` and :class:`.module.quantized.Linear`"

#: ../../source/user-guide/model-development/quantization/index.rst:170
msgid ":class:`.module.Conv2d`, :class:`.module.qat.Conv2d` 和 :class:`.module.quantized.Conv2d`"
msgstr ":class:`.module.Conv2d`, :class:`.module.qat.Conv2d` and :class:`.module.quantized.Conv2d`"

#: ../../source/user-guide/model-development/quantization/index.rst:172
msgid "对 Module 的处理用户无需感知，通过调用模型转换接口 :func:`~.quantization.quantize_qat` 和 :func:`~.quantization.quantize`, 框架会完成相应算子的批量替换操作，感兴趣的用户可以阅读相应的源码逻辑， 在 :ref:`module-convert` 小节中也会进行更具体的介绍。"
msgstr "Users do not need to perceive the processing of Module. By calling the model conversion interface :func:`~.quantization.quantize_qat` and :func:`~.quantization.quantize`, the framework will complete the batch replacement operation of the corresponding operators. Interested users can read the corresponding The source code logic will be introduced in more detail in section :ref:"

#: ../../source/user-guide/model-development/quantization/index.rst:179
msgid "量化配置 QConfig 说明"
msgstr "Quantitative configuration QConfig description"

#: ../../source/user-guide/model-development/quantization/index.rst:181
msgid ":class:`~.quantization.QConfig` 包括 :class:`~.quantization.Observer` 和 :class:`~.quantization.FakeQuantize` 两部分，用户可 1.使用预设 2.自定义配置。"
msgstr ":class:`~.quantization.QConfig` includes :class:`~.quantization.Observer` and :class:`~.quantization.FakeQuantize` two parts, users can 1. Use the preset 2. Customize the configuration."

#: ../../source/user-guide/model-development/quantization/index.rst:192
msgid "使用预设配置"
msgstr "Use preset configuration"

#: ../../source/user-guide/model-development/quantization/index.rst:194
msgid "MegEngine 中提供了类似 ``ema_fakequant_qconfig`` 这样的预设，可用作 :func:`~.quantization.quantize_qat` 的 ``qconfig``:"
msgstr "MegEngine provides a preset similar to ``ema_fakequant_qconfig``, which can be used as the ``qconfig`` of :func:"

#: ../../source/user-guide/model-development/quantization/index.rst:199
msgid "实际上它等同于使用以下 :class:`~.quantization.Qconfig` （以下即源码写法），以进行量化感知训练："
msgstr "In fact, it is equivalent to using the following :class:`~.quantization.Qconfig` (the following is the source code) for quantization perception training："

#: ../../source/user-guide/model-development/quantization/index.rst:210
msgid "这里使用了两种 Observer 来统计信息，而 FakeQuantize 使用了默认的算子。"
msgstr "Two Observers are used here for statistics, and FakeQuantize uses the default operator."

#: ../../source/user-guide/model-development/quantization/index.rst:212
msgid "如果仅做后量化，或者说 Calibration, 由于无需进行 FakeQuantize, 故而其 ``fake_quant`` 属性为 None 即可："
msgstr "If you only do post-quantization, or calibration, since FakeQuantize is not needed, the ``fake_quant'' attribute is None, then："

#: ../../source/user-guide/model-development/quantization/index.rst:225
msgid "这里的 ``calibration_qconfig`` 也是可以直接使用的 Qconfig 预设配置；"
msgstr "The ``calibration_qconfig`` here is also the Qconfig preset configuration that can be used directly;"

#: ../../source/user-guide/model-development/quantization/index.rst:226
msgid "所有可用的 Qconfig 预设可以在 :ref:`量化 API 参考 <qconfig-list>` 中找到。"
msgstr "All available Qconfig presets can be :ref:quantify API reference ` <qconfig-list>` found."

#: ../../source/user-guide/model-development/quantization/index.rst:229
msgid "自定义 Observer 和 FakeQuantize"
msgstr "Custom Observer and FakeQuantize"

#: ../../source/user-guide/model-development/quantization/index.rst:231
msgid "除了使用预设配置，用户也可以根据需要灵活选择 Observer 和 FakeQuantize, 实现自己的 QConfig."
msgstr "In addition to using the preset configuration, users can also choose Observer and FakeQuantize flexibly according to their needs to implement their own QConfig."

#: ../../source/user-guide/model-development/quantization/index.rst:235
msgid "Observer 举例：:class:`~.quantization.MinMaxObserver` / :class:`~.HistogramObserver` / :class:`~.ExponentialMovingAverageObserver` ..."
msgstr "Observer example：:class:`~.quantization.MinMaxObserver` / :class:`~.HistogramObserver` / :class:`~.ExponentialMovingAverageObserver` ..."

#: ../../source/user-guide/model-development/quantization/index.rst:236
msgid "FakeQuantize 举例：:class:`~.FakeQuantize` / :class:`~.TQT` / :class:`~.LSQ` ..."
msgstr "FakeQuantize example：:class:`~.FakeQuantize` / :class:`~.TQT` / :class:`~.LSQ` ..."

#: ../../source/user-guide/model-development/quantization/index.rst:237
msgid "所有可选的 Observer 和 FakeQuantize 已经列举在 :ref:`量化 API 参考 <qconfig-obsever>` 页面。"
msgstr "All optional Observer and it has been cited in FakeQuantize :ref:quantization API Reference ` <qconfig-obsever>'page."

#: ../../source/user-guide/model-development/quantization/index.rst:241
msgid "在实际使用过程中，可能需要在训练时让 Observer 统计并更新参数，但是在推理时则停止更新。 Observer 和 FakeQuantize 自身都支持 :meth:`~.quantization.Observer.enable` 和 :meth:`~.quantization.Observer.disable` 方法，且 Observer 会在模型调用 :meth:`~.module.Module.train` 和 :meth:`~.module.Module.eval` 方法时自动分别调用对应的 ``Observer.enable/disable`` 方法。"
msgstr "In actual use, it may be necessary to let Observer count and update parameters during training, but stop updating during inference. Observer and FakeQuantize both support :meth:`~.quantization.Observer.enable` and :meth:`~.quantization.Observer.disable` methods, and Observer will call :meth:`~.module.Module.train` and :meth:`~ in the model. Module.Module.eval` method automatically calls the corresponding ``Observer.enable/disable`` methods."

#: ../../source/user-guide/model-development/quantization/index.rst:246
msgid "一般在进行数据校准时，会先执行 ``net.eval()`` 保证网络的参数不被更新， 然后再调用 :func:`~.quantization.enable_observer` 函数来手动开启 Module 中 Observer 的统计修改功能 （即先全局关闭，再开启特定的部分）："
msgstr "Generally, during data calibration, ``net.eval()'' will be executed first to ensure that the network parameters are not updated, and then the :func:`~.quantization.enable_observer` function will be called to manually enable the statistical modification function of the Observer in the Module ( That is to turn off the global first, and then turn on the specific part)："

#: ../../source/user-guide/model-development/quantization/index.rst:257
msgid "注意这些开关处理都是递归进行的。类似接口还有 :func:`~.quantization.disable_observer`, :func:`~.quantization.enable_fake_quant`, :func:`~.quantization.disable_fake_quant` 等，可在 :ref:`quantize-operation` 中找到。"
msgstr "Note that these switch processing are performed recursively. Similar interfaces include :func:`~.quantization.disable_observer`, :func:`~.quantization.enable_fake_quant`, :func:`~.quantization.disable_fake_quant`, etc., which can be found in :ref:`quantize-operation`."

#: ../../source/user-guide/model-development/quantization/index.rst:263
msgid "模型转换模块与相关基类"
msgstr "Model conversion module and related base classes"

#: ../../source/user-guide/model-development/quantization/index.rst:265
msgid "QConfig 提供了一系列如何对模型做量化的接口，而要使用这些接口， 需要网络的 Module 能够在 forward 时给权重、激活值加上 Observer 和进行 FakeQuantize. 转换模块的作用就是将模型中的普通 :class:`~.module.Module` 替换为支持这一系列操作的 :class:`~.module.qat.QATModule` ， 并能支持进一步替换成无法训练、专用于部署的 :class:`~.module.quantized.QuantizedModule` ."
msgstr "QConfig provides a series of interfaces on how to quantify the model, and to use these interfaces, the module of the network needs to be able to add Observer to the weight and activation value and perform FakeQuantize when forwarding. The function of the conversion module is to change the ordinary :class:`~.module.Module` is replaced with :class: :class:`~.module.quantized.QuantizedModule` that cannot be trained and dedicated to deployment."

#: ../../source/user-guide/model-development/quantization/index.rst:270
msgid "这三种 Module 与 Model 对应，通过转换接口可以依次替换为不同实现的同名 Module."
msgstr "These three modules correspond to the Model, and can be replaced by modules of the same name with different implementations through the conversion interface."

#: ../../source/user-guide/model-development/quantization/index.rst:279
msgid "同时考虑到量化与推理优化时常用的算子融合（Fuse）技术高度关联，MegEngine 中提供了一系列预先融合好的 Module， 比如 :class:`~.module.ConvRelu2d` 、 :class:`~.module.ConvBn2d` 和 :class:`~.module.ConvBnRelu2d` 等。 显式地使用融合算子可以保证过程更加可控，其对应的 QuantizedModule 版本都会直接调用底层实现好的融合算子； 否则框架需要自己根据网络结构进行自动匹配和融合优化。 这样实现的缺点在于用户在使用时需要修改原先的网络结构，使用融合好的 Module 搭建网络。 而好处则是用户能更直接地控制网络如何转换，比如同时存在需要融合和不需要融合的 Conv 算子， 相比提供一个冗长的白名单，我们更倾向于在网络结构中显式地控制；而一些默认会进行转换的算子， 也可以通过 :meth:`~.module.Module.disable_quantize` 方法来控制其不进行转换（下面有举例）。"
msgstr "At the same time, considering the high correlation between quantization and inference optimization commonly used operator fusion (Fuse) technology, MegEngine provides a series of pre-fused Modules, such as :class:`~.module.ConvRelu2d`, :class:`~.module.ConvBn2d` And :class:`~.module.ConvBnRelu2d` etc. Explicit use of the fusion operator can ensure that the process is more controllable, and its corresponding QuantizedModule version will directly call the underlying implementation of the fusion operator; otherwise the framework needs to automatically match and optimize the fusion according to the network structure. The disadvantage of this implementation is that the user needs to modify the original network structure when using it, and use the integrated Module to build the network. The advantage is that users can more directly control how the network transforms. For example, there are Conv operators that need to be merged and those that do not need to be merged. Compared to providing a lengthy whitelist, we are more inclined to control explicitly in the network structure; Some operators that will perform conversion by default can also be :meth:`~.module.Module.disable_quantize` method (there are examples below)."

#: ../../source/user-guide/model-development/quantization/index.rst:288
msgid "除此之外还提供专用于量化的 :class:`~.module.QuantStub` 、 :class:`~.module.DequantStub` 等辅助模块。"
msgstr "In addition, auxiliary modules such as :class:`~.module.QuantStub` and :class:`~.module.DequantStub` dedicated to quantification are also provided."

#: ../../source/user-guide/model-development/quantization/index.rst:290
msgid "转换的原理很简单，就是将父 Module 中可被量化（Quantable）的子 Module 替换为对应的新 Module. 但是有一些 Quantable Module 还包含 Quantable 子 Module，比如 ConvBn 就包含一个 Conv2d 和一个 BatchNorm2d， 转换过程并不会对这些子 Module 进一步转换，原因是父 Module 被替换之后， 其 forward 计算过程已经完全不同了，不会再依赖于这些子 Module."
msgstr "The principle of conversion is very simple, that is, replace the Quantable sub-Modules in the parent Module with the corresponding new Modules. However, some Quantable Modules also contain Quantable sub-Modules. For example, ConvBn contains a Conv2d and a BatchNorm2d. The conversion process These child modules will not be further converted, because after the parent module is replaced, its forward calculation process is completely different and will no longer depend on these child modules."

#: ../../source/user-guide/model-development/quantization/index.rst:297
msgid "如果需要使一部分 Module 及其子 Module 保留 Float 状态，不进行转换， 可以使用 :meth:`~.module.Module.disable_quantize` 来处理。 比如当你发现对 fc 层进行量化后，模型会掉点，则可以关闭该层的量化处理："
msgstr "If you need to keep a part of Module and its sub-modules in Float state without conversion, you can use :meth:`~.module.Module.disable_quantize` to process. For example, when you find that after quantizing the fc layer, the model will drop points, you can turn off the quantization processing of this layer："

#: ../../source/user-guide/model-development/quantization/index.rst:303
msgid "该接口也可以被当作装饰器进行使用，方便对多个 Module 进行处理。"
msgstr "This interface can also be used as a decorator to facilitate the processing of multiple Modules."

#: ../../source/user-guide/model-development/quantization/index.rst:307
msgid "如果网络结构中涉及一些二元及以上的 ElementWise 操作符，比如加法乘法等， 由于多个输入各自的 scale 并不一致，必须使用量化专用的算子，并指定好输出的 scale. 实际使用中只需要把这些操作替换为 :class:`~.module.Elemwise` 即可， 比如 ``self.add_relu = Elemwise(\"FUSE_ADD_RELU\")``"
msgstr "If some binary and above ElementWise operators are involved in the network structure, such as addition and multiplication, etc., since the scales of multiple inputs are not consistent, you must use a special quantization operator and specify the output scale. In actual use, only need Just replace these operations with :class:`~.module.Elemwise`, such as ``self.add_relu = Elemwise(\"FUSE_ADD_RELU\")''"

#: ../../source/user-guide/model-development/quantization/index.rst:312
msgid "目前支持的量化 Elemwise 算子可在 :src:`dnn/scripts/opr_param_defs.py` 中找到："
msgstr "The currently supported quantized Elemwise operators can be found in :src:`dnn/scripts/opr_param_defs.py`："

#: ../../source/user-guide/model-development/quantization/index.rst:324
msgid "注意：在量化模型过程中，使用 Elemwise 算子不用加上前置 Q."
msgstr "Note：In the process of quantizing the model, the use of the Elemwise operator does not need to add the pre-Q."

#: ../../source/user-guide/model-development/quantization/index.rst:326
msgid "另外由于转换过程修改了原网络结构，模型保存与加载无法直接适用于转换后的网络， 读取新网络保存的参数时，需要先调用转换接口得到转换后的网络， 才能用 :meth:`~.module.Module.load_state_dict` 将参数进行加载。"
msgstr "In addition, because the original network structure is modified in the conversion process, model saving and loading cannot be directly applied to the converted network. When reading the parameters saved in the new network, you need to call the conversion interface to get the converted network before you can use :meth:`~.module .Module.load_state_dict` loads the parameters."

#: ../../source/user-guide/model-development/quantization/index.rst:331
msgid "ResNet 实例讲解"
msgstr "ResNet example explanation"

#: ../../source/user-guide/model-development/quantization/index.rst:333
msgid "下面我们以 ResNet18 为例来讲解量化的完整流程。主要分为以下几步："
msgstr "Let's take ResNet18 as an example to explain the complete quantification process. In the following steps："

#: ../../source/user-guide/model-development/quantization/index.rst:335
msgid "修改网络结构，使用已经融合好的 ConvBn2d、ConvBnRelu2d、ElementWise 代替原先的 Module. 在正常模式下预训练模型，并在每轮迭代保存网络检查点；"
msgstr "Modify the network structure and use the already fused ConvBn2d, ConvBnRelu2d, ElementWise to replace the original Module. Pre-train the model in normal mode, and save network checkpoints in each iteration;"

#: ../../source/user-guide/model-development/quantization/index.rst:337
msgid "调用 :func:`~.quantization.quantize_qat` 转换模型，并进行量化感知训练微调（或校准，取决于 QConfig）；"
msgstr "Call :func:`~.quantization.quantize_qat` to convert the model, and perform quantization perception training fine-tuning (or calibration, depending on QConfig);"

#: ../../source/user-guide/model-development/quantization/index.rst:338
msgid "调用 :func:`~.quantization.quantize` 转换为量化模型，导出模型用于后续模型部署。"
msgstr "Call :func:`~.quantization.quantize` to convert to a quantized model, and export the model for subsequent model deployment."

#: ../../source/user-guide/model-development/quantization/index.rst:342
msgid "这里对代码进行了简化，完整的 MegEngine 官方量化示例代码见： :models:`official/quantization`"
msgstr "The code is simplified here. For the complete MegEngine official quantization example code, see： :models:`official/quantization`"

#: ../../source/user-guide/model-development/quantization/index.rst:345
msgid "训练 Float 模型"
msgstr "Train Float model"

#: ../../source/user-guide/model-development/quantization/index.rst:347
msgid "我们修改了模型结构中的一些子 Module, 将原先单独的 ``Conv``, ``BN``, ``ReLU`` 替换为融合后的可被量化的 Module."
msgstr "We have modified some of the sub-modules in the model structure, replacing the original individual ``Conv'', ``BN``, and ``ReLU'' with quantifiable modules after fusion."

#: ../../source/user-guide/model-development/quantization/index.rst:349
msgid "修改前的模型结构： :models:`official/vision/classification/resnet/model.py`"
msgstr "Model structure before modification： :models:`official/vision/classification/resnet/model.py`"

#: ../../source/user-guide/model-development/quantization/index.rst:350
msgid "修改后的模型结构： :models:`official/quantization/models/resnet.py`"
msgstr "Modified model structure： :models:`official/quantization/models/resnet.py`"

#: ../../source/user-guide/model-development/quantization/index.rst:352
msgid "以 ``BasicBlock`` 模块的修改前后作为例子对比："
msgstr "To modify `` BasicBlock`` front module Comparative Examples："

#: ../../source/user-guide/model-development/quantization/index.rst:402
msgid "然后对该模型进行若干轮迭代训练，并保存检查点，这里省略细节："
msgstr "The model is then trained a number of iterations, and saving the checkpoint, details will be omitted here："

#: ../../source/user-guide/model-development/quantization/index.rst:424
msgid "Train - :models:`official/quantization/train.py`"
msgstr "Train- :models:`official/quantization/train.py`"

#: ../../source/user-guide/model-development/quantization/index.rst:427
msgid "转换成 QFloat 模型"
msgstr "Convert to QFloat model"

#: ../../source/user-guide/model-development/quantization/index.rst:429
msgid "调用 :func:`~.quantization.quantize_qat` 来将网络转换为 QFloat 模型:"
msgstr "Call :func:`~.quantization.quantize_qat` to convert the network to a QFloat model:"

#: ../../source/user-guide/model-development/quantization/index.rst:443
msgid "读取预训练 Float 模型保存的检查点，继续使用上面相同的代码进行微调 / 校准。"
msgstr "Read the checkpoints saved by the pre-trained Float model, and continue to use the same code above for fine-tuning/calibration."

#: ../../source/user-guide/model-development/quantization/index.rst:456
msgid "最后也需要保存此时 QFloat 模型的检查点，以便在测试和推理进行 QFloat 模型的加载和转换。"
msgstr "Finally, it is also necessary to save the checkpoints of the QFloat model at this time, so that the QFloat model can be loaded and converted during testing and inference."

#: ../../source/user-guide/model-development/quantization/index.rst:460
msgid "需要将原始 Float 模型转换为 QFloat 模型之后再加载检查点；"
msgstr "You need to convert the original Float model to a QFloat model before loading checkpoints;"

#: ../../source/user-guide/model-development/quantization/index.rst:461
msgid "如果这两次训练全在同一个脚本中执行，那么训练的 traced 函数需要用不一样的， 因为此时模型的参数变化了，需要重新进行编译。"
msgstr "If these two trainings are all executed in the same script, the trained traced function needs to be different, because the parameters of the model have changed at this time and need to be recompiled."

#: ../../source/user-guide/model-development/quantization/index.rst:466
msgid "Finetune - :models:`official/quantization/finetune.py`"
msgstr "Finetune- :models:`official/quantization/finetune.py`"

#: ../../source/user-guide/model-development/quantization/index.rst:467
msgid "Calibration - :models:`official/quantization/calibration.py`"
msgstr "Calibration- :models:`official/quantization/calibration.py`"

#: ../../source/user-guide/model-development/quantization/index.rst:470
msgid "转换成 Q 模型"
msgstr "Convert to Q model"

#: ../../source/user-guide/model-development/quantization/index.rst:472
msgid "将 QFloat 模型转换为 Q 模型并导出，共包括以下几步："
msgstr "The Q QFloat model to model and derived, comprising the following steps were："

#: ../../source/user-guide/model-development/quantization/index.rst:494
msgid "定义 trace 函数，打开 ``capture_as_const`` 以进行模型导出；"
msgstr "Define the trace function and open ``capture_as_const'' to export the model;"

#: ../../source/user-guide/model-development/quantization/index.rst:495
msgid "调用 :func:`~.quantization.quantize` 将 QAT 模型转换为 Quantized 模型；"
msgstr "Call :func:`~.quantization.quantize` to convert QAT model to Quantized model;"

#: ../../source/user-guide/model-development/quantization/index.rst:496
msgid "准备数据并执行一次推理，调用 :meth:`~.trace.dump` 将模型导出。"
msgstr "Prepare data and perform an inference, call :meth:`~.trace.dump` to export the model."

#: ../../source/user-guide/model-development/quantization/index.rst:498
msgid "至此便得到了一个可用于部署的量化模型。"
msgstr "At this point, a quantitative model that can be used for deployment is obtained."

#: ../../source/user-guide/model-development/quantization/index.rst:502
msgid "Inference and dump - :models:`official/quantization/inference.py`"
msgstr "Inference and dump- :models:`official/quantization/inference.py`"

#~ msgid ""
#~ "量化指的是将浮点数模型（一般是 32 位浮点数）的权重或激活值用位数更少的数值类型 （比如 "
#~ "8 位整数、16 位浮点数）来近似表示的过程。 "
#~ "量化后的模型会占用更小的存储空间，还能够利用许多硬件平台上的专属算子进行提速。 比如在 MegEngine "
#~ "中使用 8 位整数来进行量化，相比默认的 32 位浮点数， 模型大小可以减少为"
#~ " 1/4，而运行在特定的设备上其计算速度也能提升为 2-4 倍。"
#~ msgstr ""

#~ msgid ""
#~ "量化的目的是为了追求极致的推理计算速度，为此舍弃了数值表示的精度，直觉上会带来较大的模型掉点， "
#~ "但是在使用一系列精细的量化处理之后，其掉点可以变得微乎其微，并能支持正常的部署使用。 "
#~ "而且近年来随着专用神经网络加速芯片的兴起，低比特非浮点的运算方式越来越普及， 因此如何把一个 GPU "
#~ "上训练的浮点数模型转化为低比特的量化模型，就成为了工业界非常关心的话题。"
#~ msgstr ""

#~ msgid "一般来说，得到量化模型的转换过程按代价从低到高可以分为以下 4 种："
#~ msgstr ""

#~ msgid ""
#~ "Type1 和 Type2 由于是在模型浮点模型训练之后介入，无需大量训练数据， "
#~ "故而转换代价更低，被称为后量化（Post Quantization）；"
#~ msgstr ""

#~ msgid ""
#~ "Type3 和 Type4 则需要在浮点模型训练时就插入一些假量化（FakeQuantize）算子，"
#~ " 模拟计算过程中数值截断后精度降低的情形，故而称为量化感知训练（Quantization Aware "
#~ "Training, QAT）。"
#~ msgstr ""

#~ msgid ""
#~ "本文主要介绍 Type2 和 Type3 在 MegEngine "
#~ "中的完整流程。 事实上，除了 Type2 无需进行假量化，两者的整体流程完全一致。"
#~ msgstr ""

#~ msgid "整体流程"
#~ msgstr ""

#~ msgid ""
#~ "以 Type3 为例，一般以一个训练完毕的浮点模型为起点，称为 Float 模型。 "
#~ "包含假量化算子的用浮点操作来模拟量化过程的新模型，我们称之为 Quantized-Float 模型，或者"
#~ " QFloat 模型。 可以直接在终端设备上运行的模型，称之为 Quantized "
#~ "模型，简称 Q 模型。"
#~ msgstr ""

#~ msgid "而三者的精度一般是 ``Float > QFloat > Q`` ，故而一般量化算法也就分为两步："
#~ msgstr ""

#~ msgid "拉近 QFloat 和 Q，这样训练阶段的精度可以作为最终 Q 精度的代理指标，这一阶段偏工程；"
#~ msgstr ""

#~ msgid "拔高 QFloat 逼近 Float，这样就可以将量化模型性能尽可能恢复到 Float 的精度，这一阶段偏算法。"
#~ msgstr ""

#~ msgid "典型的三种模型在三个阶段的精度变化如下："
#~ msgstr ""

#~ msgid "对应到具体的 MegEngine 接口中，三阶段如下："
#~ msgstr ""

#~ msgid "基于 :class:`~.module.Module` 搭建网络模型，并按照正常的浮点模型方式进行训练；"
#~ msgstr ""

#~ msgid ""
#~ "使用 :func:`~.quantization.quantize_qat` 将浮点模型转换为 "
#~ "QFloat 模型， 其中可被量化的关键 Module 会被转换为 "
#~ ":class:`~.module.qat.QATModule` ， 并基于量化配置 "
#~ ":class:`~.quantization.QConfig` 设置好假量化算子和数值统计方式；"
#~ msgstr ""

#~ msgid ""
#~ "使用 :func:`~.quantization.quantize` 将 QFloat "
#~ "模型转换为 Q 模型， 对应的 QATModule 则会被转换为 "
#~ ":class:`~.module.quantized.QuantizedModule` ， "
#~ "此时网络无法再进行训练，网络中的算子都会转换为低比特计算方式，即可用于部署了。"
#~ msgstr ""

#~ msgid ""
#~ "该流程是 Type3 对应 QAT 的步骤，Type2 对应的后量化则需使用不同"
#~ " QConfig， 且需使用 evaluation 模式运行 QFloat "
#~ "模型，而非训练模式。更多细节可以继续阅读下一节详细的接口介绍。"
#~ msgstr ""

#~ msgid "接口介绍"
#~ msgstr ""

#~ msgid ""
#~ "在 MegEngine 中，最上层的接口是配置如何量化的 "
#~ ":class:`~.quantization.QConfig` 和模型转换模块里的 "
#~ ":func:`~.quantization.quantize_qat` 与 "
#~ ":func:`~.quantization.quantize` 。"
#~ msgstr ""

#~ msgid "QConfig"
#~ msgstr ""

#~ msgid ""
#~ "QConfig 包括了 :class:`~.quantization.Observer` 和 "
#~ ":class:`~.quantization.FakeQuantize` 两部分。 "
#~ "我们知道，对模型转换为低比特量化模型一般分为两步： 一是统计待量化模型中参数和 activation "
#~ "的数值范围（scale）和零点（zero_point）， 二是根据 scale 和 "
#~ "zero_point 将模型转换成指定的数值类型。而为了统计这两个值，我们需要使用 Observer."
#~ msgstr ""

#~ msgid ""
#~ "Observer 继承自 :class:`~.module.Module` ，也会参与网络的前向传播，"
#~ " 但是其 forward 的返回值就是输入，所以不会影响网络的反向梯度传播。 "
#~ "其作用就是在前向时拿到输入的值，并统计其数值范围，并通过 "
#~ ":meth:`~.quantization.Observer.get_qparams` 来获取。 "
#~ "所以在搭建网络时把需要统计数值范围的的 Tensor 作为 Observer 的输入即可。"
#~ msgstr ""

#~ msgid ""
#~ "另外如果只观察而不模拟量化会导致模型掉点，于是我们需要有 FakeQuantize 来根据 "
#~ "Observer 观察到的数值范围模拟量化时的截断，使得参数在训练时就能提前“适应“这种操作。 "
#~ "FakeQuantize 在前向时会根据传入的 scale 和 zero_point "
#~ "对输入 Tensor 做模拟量化的操作， 即先做一遍数值转换再转换后的值还原成原类型，如下所示："
#~ msgstr ""

#~ msgid "目前 MegEngine 支持对 weight/activation 两部分的量化，如下所示："
#~ msgstr ""

#~ msgid "如果是后量化，或者说 Calibration，由于无需进行 FakeQuantize，故而其 fake_quant 属性为 None 即可："
#~ msgstr ""

#~ msgid ""
#~ "除了使用在 :class:`~.quantization.Qconfig` 里提供的预设 "
#~ "QConfig， 也可以根据需要灵活选择 Observer 和 FakeQuantize"
#~ "  实现自己的 QConfig。目前提供的 Observer 包括："
#~ msgstr ""

#~ msgid ""
#~ ":class:`~.quantization.MinMaxObserver` ， 使用最简单的算法统计 "
#~ "min/max，对见到的每批数据取 min/max 跟当前存的值比较并替换， 基于 "
#~ "min/max 得到 scale 和 zero_point；"
#~ msgstr ""

#~ msgid ""
#~ ":class:`~.quantization.ExponentialMovingAverageObserver` ， "
#~ "引入动量的概念，对每批数据的 min/max 与现有 min/max 的加权和跟现有值比较；"
#~ msgstr ""

#~ msgid ""
#~ ":class:`~.quantization.HistogramObserver` ， 更加复杂的基于直方图分布的"
#~ " min/max 统计算法，且在 forward 时持续更新该分布， "
#~ "并根据该分布计算得到 scale 和 zero_point。"
#~ msgstr ""

#~ msgid ""
#~ "对于 FakeQuantize，目前还提供了 :class:`~.quantization.TQT` "
#~ "算子， 另外还可以继承 ``_FakeQuant`` 基类实现自定义的假量化算子。"
#~ msgstr ""

#~ msgid ""
#~ "在实际使用过程中，可能需要在训练时让 Observer 统计并更新参数，但是在推理时则停止更新。 "
#~ "Observer 和 FakeQuantize 都支持 "
#~ ":meth:`~.quantization.Observer.enable` 和 "
#~ ":meth:`~.quantization.Observer.disable` 功能， 且 "
#~ "Observer 会在 :meth:`~module.Module.train` 和 "
#~ ":meth:`~module.Module.eval` 时自动分别调用 enable/disable。"
#~ msgstr ""

#~ msgid ""
#~ "所以一般在 Calibration 时，会先执行 ``net.eval()`` "
#~ "保证网络的参数不被更新， 然后再执行 :``enable_observer(net)`` 来手动开启"
#~ " Observer 的统计修改功能。"
#~ msgstr ""

#~ msgid ""
#~ "QConfig 提供了一系列如何对模型做量化的接口，而要使用这些接口， 需要网络的 Module "
#~ "能够在 forward 时给参数、activation 加上 Observer "
#~ "和进行 FakeQuantize. 转换模块的作用就是将模型中的普通 Module "
#~ "替换为支持这一系列操作的 :class:`~.module.qat.QATModule` ， "
#~ "并能支持进一步替换成无法训练、专用于部署的 "
#~ ":class:`~.module.quantized.QuantizedModule` 。"
#~ msgstr ""

#~ msgid ""
#~ "基于三种基类实现的 Module 是一一对应的关系，通过转换接口可以依次替换为不同实现的同名 "
#~ "Module。 同时考虑到量化与算子融合（Fuse）的高度关联，我们提供了一系列预先融合好的 Module， "
#~ "比如 :class:`~.module.ConvRelu2d` 、 "
#~ ":class:`~.module.ConvBn2d` 和 "
#~ ":class:`~.module.ConvBnRelu2d` 等。 除此之外还提供专用于量化的 "
#~ ":class:`~.module.QuantStub` 、 "
#~ ":class:`~.module.DequantStub` 等辅助模块。"
#~ msgstr ""

#~ msgid ""
#~ "转换的原理很简单，就是将父 Module 中可被量化（Quantable）的子 Module "
#~ "替换为对应的新 Module. 但是有一些 Quantable Module "
#~ "还包含 Quantable 子 Module，比如 ConvBn 就包含一个"
#~ " Conv2d 和一个 BatchNorm2d， 转换过程并不会对这些子 Module"
#~ " 进一步转换，原因是父 Module 被替换之后， 其 forward "
#~ "计算过程已经完全不同了，不会再依赖于这些子 Module。"
#~ msgstr ""

#~ msgid ""
#~ "如果需要使一部分 Module 及其子 Module 保留 Float "
#~ "状态，不进行转换， 可以使用 :meth:`~.module.Module.disable_quantize`"
#~ " 来处理。"
#~ msgstr ""

#~ msgid ""
#~ "另外由于转换过程修改了原网络结构，模型保存与加载无法直接适用于转换后的网络， "
#~ "读取新网络保存的参数时，需要先调用转换接口得到转换后的网络，才能用 load_state_dict 将参数进行加载。"
#~ msgstr ""

#~ msgid "实例讲解"
#~ msgstr ""

#~ msgid ""
#~ "下面我们以 ResNet18 为例来讲解量化的完整流程，完整代码见 `MegEngine/Models"
#~ " "
#~ "<https://github.com/MegEngine/Models/tree/master/official/quantization>`_"
#~ " . 主要分为以下几步："
#~ msgstr ""

#~ msgid "修改网络结构，使用已经 Fuse 好的 ConvBn2d、ConvBnRelu2d、ElementWise 代替原先的 Module；"
#~ msgstr ""

#~ msgid "在正常模式下预训练模型，并在每轮迭代保存网络检查点；"
#~ msgstr ""

#~ msgid "调用 :func:`~.quantization.quantize_qat` 转换模型，并进行 finetune；"
#~ msgstr ""

#~ msgid "调用 :func:`~.quantization.quantize` 转换为量化模型，并执行 dump 用于后续模型部署。"
#~ msgstr ""

#~ msgid ""
#~ "网络结构见 ``resnet.py`` ，相比惯常写法，我们修改了其中一些子 Module， "
#~ "将原先单独的 ``conv``, ``bn``, ``relu`` 替换为 "
#~ "Fuse 过的 Quantable Module。"
#~ msgstr ""

#~ msgid "再调用 :func:`~.quantization.quantize_qat` 来将网络转换为 QATModule："
#~ msgstr ""

#~ msgid "这里使用默认的 ``ema_fakequant_qconfig`` 来进行 ``int8`` 量化。"
#~ msgstr ""

#~ msgid ""
#~ "然后我们继续使用上面相同的代码进行 finetune 训练。 "
#~ "值得注意的是，如果这两步全在一次程序运行中执行，那么训练的 trace 函数需要用不一样的， "
#~ "因为模型的参数变化了，需要重新进行编译。 示例代码中则是采用在新的执行中读取检查点重新编译的方法。"
#~ msgstr ""

#~ msgid ""
#~ "在 QAT 模式训练完成后，我们继续保存检查点，执行 ``inference.py`` "
#~ "并设置 ``mode`` 为 ``quantized`` ， 这里需要将原始"
#~ " Float 模型转换为 QAT 模型之后再加载检查点。"
#~ msgstr ""

#~ msgid "模型转换为量化模型包括以下几步："
#~ msgstr ""

