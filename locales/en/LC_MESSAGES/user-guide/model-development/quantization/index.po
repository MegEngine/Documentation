# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2021, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.4.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-11-08 21:51+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/user-guide/model-development/quantization/index.rst:5
msgid "量化（Quantization）"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:13
msgid ""
"常见神经网络模型所用的 :ref:`tensor-dtype` 一般是 ``float32`` 类型， "
"而工业界出于对特定场景的需求，需要把模型转换为像 ``int8`` 这样的低精度/比特类型 —— 整个过程被称为量化（Quantization）。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:24
msgid ""
"量化能将 32 位的浮点数转换成 8 位甚至是 4 位定点数，具有更少的运行时内存和缓存要求； "
"另外由于大部分的硬件对于定点运算都有特定的优化，所以在运行速度上也会有较大的提升。 相较于普通模型， "
"**量化模型有着更小的内存容量与带宽占用、更低的功耗和更快的推理速度等优点。**"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:27
msgid "某些计算设备只支持做定点运算。为了让模型可以在这些设备上正常运行，我们需要进行量化处理。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:29
msgid ""
"“为了追求极致的推理计算速度，从而舍弃了数值表示的精度”，直觉上会带来较大的模型掉点， "
"但是在通过一系列精妙的量化处理之后，其掉点可以变得微乎其微，并能支持正常的部署使用。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:32
msgid ""
"用户无需了解背后的实现细节，使用 MegEngine 的 :mod:`~.quantization` 所提供的解决方案，就能满足基本量化需求。 "
"我们为感兴趣的用户提供了更多有关量化基本原理的介绍，可参考 :ref:`quantization-basic-concept` 。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:35
msgid "熟悉基本原理的用户可直接跳转到 :ref:`megengine-quantization` ↩ 查看基本用法。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:39
msgid "请不要将 “量化” 与 “混合精度（Mixed precision）” 混淆，可参考 :ref:`amp-guide` 文档。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:44
msgid "量化基本流程介绍"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:46
msgid "目前工业界主要应用有两类量化技术，在 MegEngine 中都进行了支持："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:48
msgid "训练后量化（Post-Training Quantization, PTQ）；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:49
msgid "量化感知训练（Quantization-Aware Training, QAT）。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:51
msgid ""
"训练后量化是将已经训练好的浮点模型转换成低精度/比特模型所使用的通用技术，常见的做法是对模型的权重（weight）和激活值（activation）进行处理，"
" 将它们转换成精度更低的类型。在转换过程中，需要用到待量化模型中权重和激活的一些统计信息，如缩放因子（scale）和零点（zero_point）。"
" 尽管精度转换发生在训练后，但为了获取这些统计信息，我们仍需要在模型训练时 —— 即前向计算的过程中，插入一名观察者（Observer）。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:55
msgid ""
"使用训练后量化技术，会导致量化后的模型掉点（即预测正确率下降）。严重情况下会导致量化模型不可用。 一种可行的做法是使用小批量数据来在量化前对 "
"Observer 进行校准（Calibration），也叫 Calibration 后量化。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:58
msgid ""
"另一种可行的改善方案是使用量化感知训练技术，向浮点模型中插入一些伪量化（FakeQuantize）算子作为改造， 在训练时伪量化算子会根据 "
"Observer 观察到的信息进行量化模拟， 即模拟计算过程中数值截断后精度降低的情形，先做一遍数值转换，再将转换后的值还原成原类型。 "
"这样可以让被量化对象在训练时 “提前适应” 量化操作，缓解在训练后量化时带来的掉点影响。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:63
msgid "新增的 FakeQuantize 算子会引入大量的训练开销，为了节省总用时，模型量化更通常的思路是："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:65
msgid "按照平时训练模型的流程，设计好 Float 模型并进行训练（等同于得到一个预训练模型）；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:66
msgid "插入 Observer 和 FakeQuantize 算子，得到 Quantized-Float 模型（简称 QFloat 模型），量化感知训练；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:67
msgid "进行训练后量化，得到真正的 Quantized 模型（简称 Q 模型），即最终被用作推理的低比特模型。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:82
msgid "根据实际情景的一些差异，量化流程可以有灵活的变化，例如："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:84
msgid "在不考虑训练开销的情况下，为了简化整体流程，可以直接构造 QFloat 模型，并进行训练与后量化："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:95
msgid "在构造 QFloat 模型时，如果不插入 FakeQuantize 算子，也可以相应地减少训练开销，提升速度。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:97
msgid "但是这时等同于未进行量化感知，只进行了数据校准 Calibration, 模型可能会掉点严重："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:108
msgid "对于上述不同情景，在 MegEngine 中可以使用一套统一的接口来对不同的情况进行灵活配置。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:113
msgid "Megengine 量化步骤"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:115
msgid ""
"在 MegEngine 中，最上层的量化接口是配置如何量化的 :class:`~.quantization.QConfig` 和模型转换模块里的 "
":func:`~.quantization.quantize_qat` 与 :func:`~.quantization.quantize` . "
"通过配置 :class:`~.quantization.QConfig` 中所使用的 Observer 和 FakeQuantize "
"算子，我们可以对量化方案进行自定义。 进一步的说明请参考 :ref:`qconfig-guide` 小节，下面将展示 QAT 量化流程所需的步骤:"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:133
msgid ":ref:`module-guide` ，并按照正常的浮点模型方式进行训练，得到预训练模型；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:134
msgid ""
"使用 :func:`~.quantization.quantize_qat` 将 Float 模型转换为 QFloat 模型， "
"这一步会基于量化配置 :class:`~.quantization.QConfig` 设置好 Observer 和 FakeQuantize 算子"
" （在 MegEngine 中提供了常见的 QConfig :ref:`预设 <qconfig-list>`, 这里使用了 EMA 算法）；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:137
msgid "使用 QFloat 模型继续训练（微调），此时 Obersever 统计信息, FakeQuantize 进行伪量化；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:138
msgid ""
"使用 :func:`~.quantization.quantize` 将 QFloat 模型转换为 Q 模型，这一步也叫 "
"“真量化”（相较于伪量化）。 此时网络无法再进行训练，网络中的算子都会转换为低比特计算方式，即可用于部署了。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:152
msgid "我们也可以使用 Calibration 后量化方案，需准备校准数据集（参考代码示范）；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:153
msgid "MegEngine 的量化模型可被直接导出用于推理部署，参考 :ref:`dump` 。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:155
msgid "完整的 MegEngine 模型量化代码示范可在 :models:`official/quantization` 找到。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:159
msgid "从宏观上看，量化是在 Model 级别之间的转换操作，但掰开细节，则都是对 Module 的处理。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:161
msgid "对应 Float, QFloat 和 Q Model, MegEngine 中的 Module 可被整理成以下三种："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:163
msgid "进行正常浮点运算的默认 :class:`~.module.Module` （也即 Float Module ）"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:164
msgid "带有 Observer 和 FakeQuantize 算子的 :class:`.qat.QATModule`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:165
msgid "无法训练、专门用于部署的 :class:`.quantized.QuantizedModule`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:167
msgid "对于其中比较常见的可以被量化的算子，分别有同名的实现如 ——"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:169
msgid ""
":class:`.module.Linear`, :class:`.module.qat.Linear` 和 "
":class:`.module.quantized.Linear`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:170
msgid ""
":class:`.module.Conv2d`, :class:`.module.qat.Conv2d` 和 "
":class:`.module.quantized.Conv2d`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:172
msgid ""
"对 Module 的处理用户无需感知，通过调用模型转换接口 :func:`~.quantization.quantize_qat` 和 "
":func:`~.quantization.quantize`, 框架会完成相应算子的批量替换操作，感兴趣的用户可以阅读相应的源码逻辑， 在 "
":ref:`module-convert` 小节中也会进行更具体的介绍。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:179
msgid "量化配置 QConfig 说明"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:181
msgid ""
":class:`~.quantization.QConfig` 包括 :class:`~.quantization.Observer` 和 "
":class:`~.quantization.FakeQuantize` 两部分，用户可 1.使用预设 2.自定义配置。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:192
msgid "使用预设配置"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:194
msgid ""
"MegEngine 中提供了类似 ``ema_fakequant_qconfig`` 这样的预设，可用作 "
":func:`~.quantization.quantize_qat` 的 ``qconfig``:"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:199
msgid "实际上它等同于使用以下 :class:`~.quantization.Qconfig` （以下即源码写法），以进行量化感知训练："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:210
msgid "这里使用了两种 Observer 来统计信息，而 FakeQuantize 使用了默认的算子。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:212
msgid ""
"如果仅做后量化，或者说 Calibration, 由于无需进行 FakeQuantize, 故而其 ``fake_quant`` 属性为 None"
" 即可："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:225
msgid "这里的 ``calibration_qconfig`` 也是可以直接使用的 Qconfig 预设配置；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:226
msgid "所有可用的 Qconfig 预设可以在 :ref:`量化 API 参考 <qconfig-list>` 中找到。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:229
msgid "自定义 Observer 和 FakeQuantize"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:231
msgid "除了使用预设配置，用户也可以根据需要灵活选择 Observer 和 FakeQuantize, 实现自己的 QConfig."
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:235
msgid ""
"Observer 举例：:class:`~.quantization.MinMaxObserver` / "
":class:`~.HistogramObserver` / "
":class:`~.ExponentialMovingAverageObserver` ..."
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:236
msgid ""
"FakeQuantize 举例：:class:`~.FakeQuantize` / :class:`~.TQT` / :class:`~.LSQ`"
" ..."
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:237
msgid "所有可选的 Observer 和 FakeQuantize 已经列举在 :ref:`量化 API 参考 <qconfig-obsever>` 页面。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:241
msgid ""
"在实际使用过程中，可能需要在训练时让 Observer 统计并更新参数，但是在推理时则停止更新。 Observer 和 FakeQuantize "
"自身都支持 :meth:`~.quantization.Observer.enable` 和 "
":meth:`~.quantization.Observer.disable` 方法，且 Observer 会在模型调用 "
":meth:`~.module.Module.train` 和 :meth:`~.module.Module.eval` 方法时自动分别调用对应的"
" ``Observer.enable/disable`` 方法。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:246
msgid ""
"一般在进行数据校准时，会先执行 ``net.eval()`` 保证网络的参数不被更新， 然后再调用 "
":func:`~.quantization.enable_observer` 函数来手动开启 Module 中 Observer 的统计修改功能 "
"（即先全局关闭，再开启特定的部分）："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:257
msgid ""
"注意这些开关处理都是递归进行的。类似接口还有 :func:`~.quantization.disable_observer`, "
":func:`~.quantization.enable_fake_quant`, "
":func:`~.quantization.disable_fake_quant` 等，可在 :ref:`quantize-operation` "
"中找到。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:263
msgid "模型转换模块与相关基类"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:265
msgid ""
"QConfig 提供了一系列如何对模型做量化的接口，而要使用这些接口， 需要网络的 Module 能够在 forward 时给权重、激活值加上 "
"Observer 和进行 FakeQuantize. 转换模块的作用就是将模型中的普通 :class:`~.module.Module` "
"替换为支持这一系列操作的 :class:`~.module.qat.QATModule` ， 并能支持进一步替换成无法训练、专用于部署的 "
":class:`~.module.quantized.QuantizedModule` ."
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:270
msgid "这三种 Module 与 Model 对应，通过转换接口可以依次替换为不同实现的同名 Module."
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:279
msgid ""
"同时考虑到量化与推理优化时常用的算子融合（Fuse）技术高度关联，MegEngine 中提供了一系列预先融合好的 Module， 比如 "
":class:`~.module.ConvRelu2d` 、 :class:`~.module.ConvBn2d` 和 "
":class:`~.module.ConvBnRelu2d` 等。 显式地使用融合算子可以保证过程更加可控，其对应的 "
"QuantizedModule 版本都会直接调用底层实现好的融合算子； 否则框架需要自己根据网络结构进行自动匹配和融合优化。 "
"这样实现的缺点在于用户在使用时需要修改原先的网络结构，使用融合好的 Module 搭建网络。 "
"而好处则是用户能更直接地控制网络如何转换，比如同时存在需要融合和不需要融合的 Conv 算子， "
"相比提供一个冗长的白名单，我们更倾向于在网络结构中显式地控制；而一些默认会进行转换的算子， 也可以通过 "
":meth:`~.module.Module.disable_quantize` 方法来控制其不进行转换（下面有举例）。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:288
msgid ""
"除此之外还提供专用于量化的 :class:`~.module.QuantStub` 、 :class:`~.module.DequantStub`"
" 等辅助模块。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:290
msgid ""
"转换的原理很简单，就是将父 Module 中可被量化（Quantable）的子 Module 替换为对应的新 Module. 但是有一些 "
"Quantable Module 还包含 Quantable 子 Module，比如 ConvBn 就包含一个 Conv2d 和一个 "
"BatchNorm2d， 转换过程并不会对这些子 Module 进一步转换，原因是父 Module 被替换之后， 其 forward "
"计算过程已经完全不同了，不会再依赖于这些子 Module."
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:297
msgid ""
"如果需要使一部分 Module 及其子 Module 保留 Float 状态，不进行转换， 可以使用 "
":meth:`~.module.Module.disable_quantize` 来处理。 比如当你发现对 fc "
"层进行量化后，模型会掉点，则可以关闭该层的量化处理："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:303
msgid "该接口也可以被当作装饰器进行使用，方便对多个 Module 进行处理。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:307
msgid ""
"如果网络结构中涉及一些二元及以上的 ElementWise 操作符，比如加法乘法等， 由于多个输入各自的 scale "
"并不一致，必须使用量化专用的算子，并指定好输出的 scale. 实际使用中只需要把这些操作替换为 "
":class:`~.module.Elemwise` 即可， 比如 ``self.add_relu = "
"Elemwise(\"FUSE_ADD_RELU\")``"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:312
msgid "目前支持的量化 Elemwise 算子可在 :src:`dnn/scripts/opr_param_defs.py` 中找到："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:324
msgid "注意：在量化模型过程中，使用 Elemwise 算子不用加上前置 Q."
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:326
msgid ""
"另外由于转换过程修改了原网络结构，模型保存与加载无法直接适用于转换后的网络， 读取新网络保存的参数时，需要先调用转换接口得到转换后的网络， 才能用"
" :meth:`~.module.Module.load_state_dict` 将参数进行加载。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:331
msgid "ResNet 实例讲解"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:333
msgid "下面我们以 ResNet18 为例来讲解量化的完整流程。主要分为以下几步："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:335
msgid ""
"修改网络结构，使用已经融合好的 ConvBn2d、ConvBnRelu2d、ElementWise 代替原先的 Module. "
"在正常模式下预训练模型，并在每轮迭代保存网络检查点；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:337
msgid "调用 :func:`~.quantization.quantize_qat` 转换模型，并进行量化感知训练微调（或校准，取决于 QConfig）；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:338
msgid "调用 :func:`~.quantization.quantize` 转换为量化模型，导出模型用于后续模型部署。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:342
msgid "这里对代码进行了简化，完整的 MegEngine 官方量化示例代码见： :models:`official/quantization`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:345
msgid "训练 Float 模型"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:347
msgid ""
"我们修改了模型结构中的一些子 Module, 将原先单独的 ``Conv``, ``BN``, ``ReLU`` 替换为融合后的可被量化的 "
"Module."
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:349
msgid "修改前的模型结构： :models:`official/vision/classification/resnet/model.py`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:350
msgid "修改后的模型结构： :models:`official/quantization/models/resnet.py`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:352
msgid "以 ``BasicBlock`` 模块的修改前后作为例子对比："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:402
msgid "然后对该模型进行若干轮迭代训练，并保存检查点，这里省略细节："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:424
msgid "Train - :models:`official/quantization/train.py`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:427
msgid "转换成 QFloat 模型"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:429
msgid "调用 :func:`~.quantization.quantize_qat` 来将网络转换为 QFloat 模型:"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:443
msgid "读取预训练 Float 模型保存的检查点，继续使用上面相同的代码进行微调 / 校准。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:456
msgid "最后也需要保存此时 QFloat 模型的检查点，以便在测试和推理进行 QFloat 模型的加载和转换。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:460
msgid "需要将原始 Float 模型转换为 QFloat 模型之后再加载检查点；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:461
msgid "如果这两次训练全在同一个脚本中执行，那么训练的 traced 函数需要用不一样的， 因为此时模型的参数变化了，需要重新进行编译。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:466
msgid "Finetune - :models:`official/quantization/finetune.py`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:467
msgid "Calibration - :models:`official/quantization/calibration.py`"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:470
msgid "转换成 Q 模型"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:472
msgid "将 QFloat 模型转换为 Q 模型并导出，共包括以下几步："
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:494
msgid "定义 trace 函数，打开 ``capture_as_const`` 以进行模型导出；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:495
msgid "调用 :func:`~.quantization.quantize` 将 QAT 模型转换为 Quantized 模型；"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:496
msgid "准备数据并执行一次推理，调用 :meth:`~.trace.dump` 将模型导出。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:498
msgid "至此便得到了一个可用于部署的量化模型。"
msgstr ""

#: ../../source/user-guide/model-development/quantization/index.rst:502
msgid "Inference and dump - :models:`official/quantization/inference.py`"
msgstr ""

#~ msgid ""
#~ "量化指的是将浮点数模型（一般是 32 位浮点数）的权重或激活值用位数更少的数值类型 （比如 "
#~ "8 位整数、16 位浮点数）来近似表示的过程。 "
#~ "量化后的模型会占用更小的存储空间，还能够利用许多硬件平台上的专属算子进行提速。 比如在 MegEngine "
#~ "中使用 8 位整数来进行量化，相比默认的 32 位浮点数， 模型大小可以减少为"
#~ " 1/4，而运行在特定的设备上其计算速度也能提升为 2-4 倍。"
#~ msgstr ""

#~ msgid ""
#~ "量化的目的是为了追求极致的推理计算速度，为此舍弃了数值表示的精度，直觉上会带来较大的模型掉点， "
#~ "但是在使用一系列精细的量化处理之后，其掉点可以变得微乎其微，并能支持正常的部署使用。 "
#~ "而且近年来随着专用神经网络加速芯片的兴起，低比特非浮点的运算方式越来越普及， 因此如何把一个 GPU "
#~ "上训练的浮点数模型转化为低比特的量化模型，就成为了工业界非常关心的话题。"
#~ msgstr ""

#~ msgid "一般来说，得到量化模型的转换过程按代价从低到高可以分为以下 4 种："
#~ msgstr ""

#~ msgid ""
#~ "Type1 和 Type2 由于是在模型浮点模型训练之后介入，无需大量训练数据， "
#~ "故而转换代价更低，被称为后量化（Post Quantization）；"
#~ msgstr ""

#~ msgid ""
#~ "Type3 和 Type4 则需要在浮点模型训练时就插入一些假量化（FakeQuantize）算子，"
#~ " 模拟计算过程中数值截断后精度降低的情形，故而称为量化感知训练（Quantization Aware "
#~ "Training, QAT）。"
#~ msgstr ""

#~ msgid ""
#~ "本文主要介绍 Type2 和 Type3 在 MegEngine "
#~ "中的完整流程。 事实上，除了 Type2 无需进行假量化，两者的整体流程完全一致。"
#~ msgstr ""

#~ msgid "整体流程"
#~ msgstr ""

#~ msgid ""
#~ "以 Type3 为例，一般以一个训练完毕的浮点模型为起点，称为 Float 模型。 "
#~ "包含假量化算子的用浮点操作来模拟量化过程的新模型，我们称之为 Quantized-Float 模型，或者"
#~ " QFloat 模型。 可以直接在终端设备上运行的模型，称之为 Quantized "
#~ "模型，简称 Q 模型。"
#~ msgstr ""

#~ msgid "而三者的精度一般是 ``Float > QFloat > Q`` ，故而一般量化算法也就分为两步："
#~ msgstr ""

#~ msgid "拉近 QFloat 和 Q，这样训练阶段的精度可以作为最终 Q 精度的代理指标，这一阶段偏工程；"
#~ msgstr ""

#~ msgid "拔高 QFloat 逼近 Float，这样就可以将量化模型性能尽可能恢复到 Float 的精度，这一阶段偏算法。"
#~ msgstr ""

#~ msgid "典型的三种模型在三个阶段的精度变化如下："
#~ msgstr ""

#~ msgid "对应到具体的 MegEngine 接口中，三阶段如下："
#~ msgstr ""

#~ msgid "基于 :class:`~.module.Module` 搭建网络模型，并按照正常的浮点模型方式进行训练；"
#~ msgstr ""

#~ msgid ""
#~ "使用 :func:`~.quantization.quantize_qat` 将浮点模型转换为 "
#~ "QFloat 模型， 其中可被量化的关键 Module 会被转换为 "
#~ ":class:`~.module.qat.QATModule` ， 并基于量化配置 "
#~ ":class:`~.quantization.QConfig` 设置好假量化算子和数值统计方式；"
#~ msgstr ""

#~ msgid ""
#~ "使用 :func:`~.quantization.quantize` 将 QFloat "
#~ "模型转换为 Q 模型， 对应的 QATModule 则会被转换为 "
#~ ":class:`~.module.quantized.QuantizedModule` ， "
#~ "此时网络无法再进行训练，网络中的算子都会转换为低比特计算方式，即可用于部署了。"
#~ msgstr ""

#~ msgid ""
#~ "该流程是 Type3 对应 QAT 的步骤，Type2 对应的后量化则需使用不同"
#~ " QConfig， 且需使用 evaluation 模式运行 QFloat "
#~ "模型，而非训练模式。更多细节可以继续阅读下一节详细的接口介绍。"
#~ msgstr ""

#~ msgid "接口介绍"
#~ msgstr ""

#~ msgid ""
#~ "在 MegEngine 中，最上层的接口是配置如何量化的 "
#~ ":class:`~.quantization.QConfig` 和模型转换模块里的 "
#~ ":func:`~.quantization.quantize_qat` 与 "
#~ ":func:`~.quantization.quantize` 。"
#~ msgstr ""

#~ msgid "QConfig"
#~ msgstr ""

#~ msgid ""
#~ "QConfig 包括了 :class:`~.quantization.Observer` 和 "
#~ ":class:`~.quantization.FakeQuantize` 两部分。 "
#~ "我们知道，对模型转换为低比特量化模型一般分为两步： 一是统计待量化模型中参数和 activation "
#~ "的数值范围（scale）和零点（zero_point）， 二是根据 scale 和 "
#~ "zero_point 将模型转换成指定的数值类型。而为了统计这两个值，我们需要使用 Observer."
#~ msgstr ""

#~ msgid ""
#~ "Observer 继承自 :class:`~.module.Module` ，也会参与网络的前向传播，"
#~ " 但是其 forward 的返回值就是输入，所以不会影响网络的反向梯度传播。 "
#~ "其作用就是在前向时拿到输入的值，并统计其数值范围，并通过 "
#~ ":meth:`~.quantization.Observer.get_qparams` 来获取。 "
#~ "所以在搭建网络时把需要统计数值范围的的 Tensor 作为 Observer 的输入即可。"
#~ msgstr ""

#~ msgid ""
#~ "另外如果只观察而不模拟量化会导致模型掉点，于是我们需要有 FakeQuantize 来根据 "
#~ "Observer 观察到的数值范围模拟量化时的截断，使得参数在训练时就能提前“适应“这种操作。 "
#~ "FakeQuantize 在前向时会根据传入的 scale 和 zero_point "
#~ "对输入 Tensor 做模拟量化的操作， 即先做一遍数值转换再转换后的值还原成原类型，如下所示："
#~ msgstr ""

#~ msgid "目前 MegEngine 支持对 weight/activation 两部分的量化，如下所示："
#~ msgstr ""

#~ msgid "如果是后量化，或者说 Calibration，由于无需进行 FakeQuantize，故而其 fake_quant 属性为 None 即可："
#~ msgstr ""

#~ msgid ""
#~ "除了使用在 :class:`~.quantization.Qconfig` 里提供的预设 "
#~ "QConfig， 也可以根据需要灵活选择 Observer 和 FakeQuantize"
#~ "  实现自己的 QConfig。目前提供的 Observer 包括："
#~ msgstr ""

#~ msgid ""
#~ ":class:`~.quantization.MinMaxObserver` ， 使用最简单的算法统计 "
#~ "min/max，对见到的每批数据取 min/max 跟当前存的值比较并替换， 基于 "
#~ "min/max 得到 scale 和 zero_point；"
#~ msgstr ""

#~ msgid ""
#~ ":class:`~.quantization.ExponentialMovingAverageObserver` ， "
#~ "引入动量的概念，对每批数据的 min/max 与现有 min/max 的加权和跟现有值比较；"
#~ msgstr ""

#~ msgid ""
#~ ":class:`~.quantization.HistogramObserver` ， 更加复杂的基于直方图分布的"
#~ " min/max 统计算法，且在 forward 时持续更新该分布， "
#~ "并根据该分布计算得到 scale 和 zero_point。"
#~ msgstr ""

#~ msgid ""
#~ "对于 FakeQuantize，目前还提供了 :class:`~.quantization.TQT` "
#~ "算子， 另外还可以继承 ``_FakeQuant`` 基类实现自定义的假量化算子。"
#~ msgstr ""

#~ msgid ""
#~ "在实际使用过程中，可能需要在训练时让 Observer 统计并更新参数，但是在推理时则停止更新。 "
#~ "Observer 和 FakeQuantize 都支持 "
#~ ":meth:`~.quantization.Observer.enable` 和 "
#~ ":meth:`~.quantization.Observer.disable` 功能， 且 "
#~ "Observer 会在 :meth:`~module.Module.train` 和 "
#~ ":meth:`~module.Module.eval` 时自动分别调用 enable/disable。"
#~ msgstr ""

#~ msgid ""
#~ "所以一般在 Calibration 时，会先执行 ``net.eval()`` "
#~ "保证网络的参数不被更新， 然后再执行 :``enable_observer(net)`` 来手动开启"
#~ " Observer 的统计修改功能。"
#~ msgstr ""

#~ msgid ""
#~ "QConfig 提供了一系列如何对模型做量化的接口，而要使用这些接口， 需要网络的 Module "
#~ "能够在 forward 时给参数、activation 加上 Observer "
#~ "和进行 FakeQuantize. 转换模块的作用就是将模型中的普通 Module "
#~ "替换为支持这一系列操作的 :class:`~.module.qat.QATModule` ， "
#~ "并能支持进一步替换成无法训练、专用于部署的 "
#~ ":class:`~.module.quantized.QuantizedModule` 。"
#~ msgstr ""

#~ msgid ""
#~ "基于三种基类实现的 Module 是一一对应的关系，通过转换接口可以依次替换为不同实现的同名 "
#~ "Module。 同时考虑到量化与算子融合（Fuse）的高度关联，我们提供了一系列预先融合好的 Module， "
#~ "比如 :class:`~.module.ConvRelu2d` 、 "
#~ ":class:`~.module.ConvBn2d` 和 "
#~ ":class:`~.module.ConvBnRelu2d` 等。 除此之外还提供专用于量化的 "
#~ ":class:`~.module.QuantStub` 、 "
#~ ":class:`~.module.DequantStub` 等辅助模块。"
#~ msgstr ""

#~ msgid ""
#~ "转换的原理很简单，就是将父 Module 中可被量化（Quantable）的子 Module "
#~ "替换为对应的新 Module. 但是有一些 Quantable Module "
#~ "还包含 Quantable 子 Module，比如 ConvBn 就包含一个"
#~ " Conv2d 和一个 BatchNorm2d， 转换过程并不会对这些子 Module"
#~ " 进一步转换，原因是父 Module 被替换之后， 其 forward "
#~ "计算过程已经完全不同了，不会再依赖于这些子 Module。"
#~ msgstr ""

#~ msgid ""
#~ "如果需要使一部分 Module 及其子 Module 保留 Float "
#~ "状态，不进行转换， 可以使用 :meth:`~.module.Module.disable_quantize`"
#~ " 来处理。"
#~ msgstr ""

#~ msgid ""
#~ "另外由于转换过程修改了原网络结构，模型保存与加载无法直接适用于转换后的网络， "
#~ "读取新网络保存的参数时，需要先调用转换接口得到转换后的网络，才能用 load_state_dict 将参数进行加载。"
#~ msgstr ""

#~ msgid "实例讲解"
#~ msgstr ""

#~ msgid ""
#~ "下面我们以 ResNet18 为例来讲解量化的完整流程，完整代码见 `MegEngine/Models"
#~ " "
#~ "<https://github.com/MegEngine/Models/tree/master/official/quantization>`_"
#~ " . 主要分为以下几步："
#~ msgstr ""

#~ msgid "修改网络结构，使用已经 Fuse 好的 ConvBn2d、ConvBnRelu2d、ElementWise 代替原先的 Module；"
#~ msgstr ""

#~ msgid "在正常模式下预训练模型，并在每轮迭代保存网络检查点；"
#~ msgstr ""

#~ msgid "调用 :func:`~.quantization.quantize_qat` 转换模型，并进行 finetune；"
#~ msgstr ""

#~ msgid "调用 :func:`~.quantization.quantize` 转换为量化模型，并执行 dump 用于后续模型部署。"
#~ msgstr ""

#~ msgid ""
#~ "网络结构见 ``resnet.py`` ，相比惯常写法，我们修改了其中一些子 Module， "
#~ "将原先单独的 ``conv``, ``bn``, ``relu`` 替换为 "
#~ "Fuse 过的 Quantable Module。"
#~ msgstr ""

#~ msgid "再调用 :func:`~.quantization.quantize_qat` 来将网络转换为 QATModule："
#~ msgstr ""

#~ msgid "这里使用默认的 ``ema_fakequant_qconfig`` 来进行 ``int8`` 量化。"
#~ msgstr ""

#~ msgid ""
#~ "然后我们继续使用上面相同的代码进行 finetune 训练。 "
#~ "值得注意的是，如果这两步全在一次程序运行中执行，那么训练的 trace 函数需要用不一样的， "
#~ "因为模型的参数变化了，需要重新进行编译。 示例代码中则是采用在新的执行中读取检查点重新编译的方法。"
#~ msgstr ""

#~ msgid ""
#~ "在 QAT 模式训练完成后，我们继续保存检查点，执行 ``inference.py`` "
#~ "并设置 ``mode`` 为 ``quantized`` ， 这里需要将原始"
#~ " Float 模型转换为 QAT 模型之后再加载检查点。"
#~ msgstr ""

#~ msgid "模型转换为量化模型包括以下几步："
#~ msgstr ""

