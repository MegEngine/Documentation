msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-08-17 20:15+0800\n"
"PO-Revision-Date: 2021-11-12 00:51\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/model-development/jit/trace.po\n"
"X-Crowdin-File-ID: 8195\n"
"Language: en_US\n"

#: ../../source/user-guide/model-development/jit/trace.rst:5
msgid "将动态图转为静态图（Trace）"
msgstr "Convert dynamic graphs to static graphs (Trace)"

#: ../../source/user-guide/model-development/jit/trace.rst:9
msgid "一般的模型训练中推荐用动态图，只有在有必要的情况下才编译静态图。（ :ref:`直接跳转到用法 <trace>` ）"
msgstr "It is recommended to use dynamic graphs in general model training, and compile static graphs only when necessary. ( :ref:`Jump directly to usage <trace>`)"

#: ../../source/user-guide/model-development/jit/trace.rst:14
msgid "动态图和静态图"
msgstr "Dynamic graph and static graph"

#: ../../source/user-guide/model-development/jit/trace.rst:16
msgid "MegEngine 默认使用 **动态计算图** ，其核心特点是计算图的构建和计算同时发生（Define by run）。"
msgstr "MegEngine uses **dynamic calculation graph** by default, and its core feature is that the construction and calculation of the calculation graph occur at the same time (Define by run)."

#: ../../source/user-guide/model-development/jit/trace.rst:18
msgid "**原理：** 在计算图中定义一个 :py:class:`~megengine.Tensor` 时，其值就已经被计算且确定了。"
msgstr "**Principle：** When defining a: py:class:`~megengine.Tensor` in the calculation graph, its value has been calculated and determined."

#: ../../source/user-guide/model-development/jit/trace.rst:19
msgid "**优点：** 这种模式在调试模型时较为方便，能够实时得到中间结果的值。"
msgstr "**Advantage：** This mode is more convenient when debugging the model, and can get the value of the intermediate result in real time."

#: ../../source/user-guide/model-development/jit/trace.rst:20
msgid "**缺点：** 但是由于所有节点都需要被保存，这就导致我们难以对整个计算图进行优化。"
msgstr "**Disadvantages：** But because all nodes need to be saved, this makes it difficult for us to optimize the entire calculation graph."

#: ../../source/user-guide/model-development/jit/trace.rst:22
msgid "MegEngine 也支持 **静态计算图** 模式，将计算图的构建和实际计算分开（Define and run）。"
msgstr "MegEngine also supports the **static calculation graph** mode, which separates the construction of the calculation graph from the actual calculation (Define and run)."

#: ../../source/user-guide/model-development/jit/trace.rst:24
msgid "**原理：** 在构建阶段，MegEngine 根据完整的计算流程对原始的计算图进行优化和调整， 得到更省内存和计算量更少的计算图，这个过程称之为 “编译” 。编译之后图的结构不再改变，也就是所谓的 “静态” 。 在计算阶段，MegEngine 根据输入数据执行编译好的计算图得到计算结果。"
msgstr "**Principle：** In the construction phase, MegEngine optimizes and adjusts the original calculation graph according to the complete calculation process to obtain a calculation graph that saves more memory and has less calculation. This process is called \"compilation\". The structure of the graph does not change after compilation, which is the so-called \"static\". In the calculation phase, MegEngine executes the compiled calculation graph according to the input data to obtain the calculation result."

#: ../../source/user-guide/model-development/jit/trace.rst:27
msgid "**优点：** 静态图相比起动态图，对全局的信息掌握更丰富，可做的优化也会更多。"
msgstr "**Advantages：** Compared with dynamic images, static images have a richer grasp of global information and can do more optimizations."

#: ../../source/user-guide/model-development/jit/trace.rst:28
msgid "**缺点：** 但中间过程对于用户来说是个黑盒，无法像动态图一样随时拿到中间计算结果。"
msgstr "**Disadvantages：** However, the intermediate process is a black box for the user, and the intermediate calculation result cannot be obtained at any time like a dynamic graph."

#: ../../source/user-guide/model-development/jit/trace.rst:30
msgid "通过浏览本小节末尾的 :ref:`trace-advanced-setting` 部分，你可以了解到静态图的更多使用情景。"
msgstr ":ref:`trace-advanced-setting` section at the end of this section, you can learn more about the usage scenarios of static graphs."

#: ../../source/user-guide/model-development/jit/trace.rst:35
msgid "静态图编译优化举例"
msgstr "Examples of static graph compilation optimization"

#: ../../source/user-guide/model-development/jit/trace.rst:37
msgid "下面我们举例说明静态图编译过程中可能进行的内存和计算优化："
msgstr "Below we illustrate in FIG static memory and computing compiler optimization process may be："

#: ../../source/user-guide/model-development/jit/trace.rst:42
msgid "在上图左侧的计算图中，为了存储 ``x``, ``w``, ``p``,  ``b``, ``y`` 五个变量， 动态图需要 40 个字节（假设每个变量占用 8 字节的内存）。 在静态图中，由于我们只需要知道结果 ``y``, 可以让 ``y`` 复用中间变量 ``p`` 的内存， 实现 “原地”（Inplace）修改。这样，静态图所占用的内存就减少为 32 个字节。"
msgstr "In the calculation diagram on the left side of the above figure, in order to store the five variables of ``x``, ``w``, ``p``, ``b``, ``y``, the dynamic graph needs 40 Bytes (assuming that each variable occupies 8 bytes of memory). In the static diagram, since we only need to know the result ``y``, we can make ``y`` reuse the memory of the intermediate variable ``p`` to achieve \"inplace\" modification. In this way, the memory occupied by the static image is reduced to 32 bytes."

#: ../../source/user-guide/model-development/jit/trace.rst:50
msgid "使用 trace 装饰器"
msgstr "Use the trace decorator"

#: ../../source/user-guide/model-development/jit/trace.rst:52
msgid "MegEngine 提供了很方便的动静态图转换的方法，几乎无需代码改动即可实现转换。"
msgstr "MegEngine provides a very convenient method of dynamic and static image conversion, which can be converted almost without code changes."

#: ../../source/user-guide/model-development/jit/trace.rst:54
msgid "假设我们写好了一份动态图代码，其中训练部分代码如下："
msgstr "Suppose we have written a dynamic graph code, the training part of the code is as follows："

#: ../../source/user-guide/model-development/jit/trace.rst:75
msgid "我们可以通过以下三步将上面的动态图转换为静态图："
msgstr "We can convert the following three steps above dynamic map static map："

#: ../../source/user-guide/model-development/jit/trace.rst:77
msgid "将循环内的网络计算和优化代码提取成一个单独的训练函数，如下面例子中的 ``train_func()`` ；"
msgstr "Extract the network calculation and optimization code in the loop into a separate training function, such as ``train_func()'' in the following example;"

#: ../../source/user-guide/model-development/jit/trace.rst:78
msgid "将网络所需输入作为训练函数的参数，并返回任意你需要的结果（如计算图的结果和损失函数值）；"
msgstr "Take the input required by the network as the parameters of the training function, and return any result you need (such as the result of the calculation graph and the value of the loss function);"

#: ../../source/user-guide/model-development/jit/trace.rst:79
msgid "用 :py:mod:`~.jit` 模块中的 :py:class:`~.jit.trace` 装饰器来装饰这个函数，将其中的代码变为静态图代码。"
msgstr "Use the :py:class:`~.jit.trace` decorator in the :py:mod:`~.jit` module to decorate this function and change the code in it into static graph code."

#: ../../source/user-guide/model-development/jit/trace.rst:81
msgid "修改后的代码如下："
msgstr "The modified code is as follows："

#: ../../source/user-guide/model-development/jit/trace.rst:107
msgid "对于上述代码，我们作进一步的解释："
msgstr "For the above code, we will further explain："

#: ../../source/user-guide/model-development/jit/trace.rst:109
msgid "``jit`` ： 即时编译 （Just-in-time compilation）的缩写，这里作为整个静态图相关模块的名字。"
msgstr "``jit`` ： Abbreviation for Just-in-time compilation, here as the name of the module related to the entire static image."

#: ../../source/user-guide/model-development/jit/trace.rst:110
msgid "``trace`` ：得到静态图的一种方式，直译为 “追溯”， 含义为通过追溯输出（比如损失值、预测值）所依赖的网络结构，得到整体的计算图，再进行编译。"
msgstr "``trace`` ：is a way to get a static graph, literally translated as \"tracing\", meaning that the overall calculation graph is obtained by tracing the network structure that the output (such as loss value, predicted value) depends on, and then compiling."

#: ../../source/user-guide/model-development/jit/trace.rst:112
msgid "参数列表 ： ``trace`` 在编译静态图时会根据传入参数是位置参数还是关键字参数来采取不同的处理方式。 其中位置参数用于传入网络的输入如数据和标签，关键字参数用于传入其它变量，如网络和优化器等。"
msgstr "Parameter list ： ``trace`` will take different processing methods according to whether the input parameter is a positional parameter or a keyword parameter when compiling a static graph. The position parameter is used to pass in the input of the network, such as data and labels, and the keyword parameter is used to pass in other variables, such as the network and optimizer."

#: ../../source/user-guide/model-development/jit/trace.rst:118
msgid "trace 进阶设置"
msgstr "Advanced trace settings"

#: ../../source/user-guide/model-development/jit/trace.rst:123
msgid "指定静态图构造方式"
msgstr "Specify the static graph construction method"

#: ../../source/user-guide/model-development/jit/trace.rst:125
msgid "MegEngine 在编译静态图时有 “动态构造” 和 “静态构造” 两种模式（默认使用前者）。"
msgstr "MegEngine has two modes of \"dynamic construction\" and \"static construction\" when compiling static graphs (the former is used by default)."

#: ../../source/user-guide/model-development/jit/trace.rst:127
msgid "在绝大部分情况下，两种模式下构造出的静态图并没有区别，使用中也没有分别。"
msgstr "In most cases, there is no difference between the static images constructed in the two modes, and there is no difference in use."

#: ../../source/user-guide/model-development/jit/trace.rst:129
msgid "我们可以指定 ``symbolic`` 参数来指定构造方式，示例代码如下:"
msgstr "We can specify the ``symbolic'' parameter to specify the construction method, the sample code is as follows:"

#: ../../source/user-guide/model-development/jit/trace.rst:139
msgid "设置为 True 表示 “静态构造” 或者 “根据符号构造” ——"
msgstr "Set to True to mean \"static construction\" or \"construction based on symbols\" ——"

#: ../../source/user-guide/model-development/jit/trace.rst:141
msgid "**原理：** 此时，计算图中的所有数据节点（即张量）被视为符号（即 ``symbolic`` ）。 它们仅仅作为占位符（Placeholder），不产生实际的内存分配，也没有实际的值。 此时计算图的编译过程完全取决于计算图的结构，而不取决于张量的具体值，是真正的 “静态”。"
msgstr "**Principle：** At this time, all data nodes (ie tensors) in the calculation graph are regarded as symbols (ie ``symbolic``). They are only used as placeholders and do not generate actual memory allocations, nor do they have actual values. At this time, the compilation process of the calculation graph depends entirely on the structure of the calculation graph, and not on the specific value of the tensor, which is truly \"static\"."

#: ../../source/user-guide/model-development/jit/trace.rst:144
msgid "**优点：** 始终高效，能充分利用静态图的内存优化。"
msgstr "**Advantage：** Always efficient, can make full use of the memory optimization of static images."

#: ../../source/user-guide/model-development/jit/trace.rst:145
msgid "**缺点：** 如果网络中包含了需要运行时动态信息才能计算的条件语句，将会失败。"
msgstr "**Disadvantages：** If the network contains conditional statements that require runtime dynamic information to be calculated, it will fail."

#: ../../source/user-guide/model-development/jit/trace.rst:147
msgid "设置为 False 表示 “动态构造” 或者 “根据值构造” ——"
msgstr "Set to False means \"dynamic construction\" or \"construction based on value\" ——"

#: ../../source/user-guide/model-development/jit/trace.rst:149
msgid "**原理：** 被装饰的函数在第一次被调用时会根据输入的数据执行一次计算构建出一个动态图。 接着将这个动态图会被编译静态图。此后该函数的所有调用都会运行这个静态图，而不再依赖调用时输入的值。 此种模式可以视为 “动态构建第一次，此后静态运行”。"
msgstr "**Principle：** When the decorated function is called for the first time, it performs a calculation based on the input data to construct a dynamic graph. Then the dynamic graph will be compiled into a static graph. All subsequent calls to this function will run this static graph, instead of relying on the value entered during the call. This mode can be regarded as \"dynamic construction for the first time, then static operation\"."

#: ../../source/user-guide/model-development/jit/trace.rst:152
msgid "**优点：** 根据第一次运行时信息的不同，可以构建出不同的静态图。"
msgstr "**Advantages：** According to the different information in the first run, different static graphs can be constructed."

#: ../../source/user-guide/model-development/jit/trace.rst:153
msgid "**缺点：** 由于第一次的运行在动态图模式下，无法利用静态图的内存优化，通常会耗费更大的内存。 这可能导致本来在静态图模式下可以运行的网络，在第一次运行时由于内存不够而失败。"
msgstr "**Disadvantages：** Since the first run is in the dynamic graph mode, the memory optimization of the static graph cannot be used, and it usually consumes more memory. This may cause the network that can be run in static graph mode to fail due to insufficient memory the first time it runs."

#: ../../source/user-guide/model-development/jit/trace.rst:158
msgid "在动态构造模式（设置为 False）下，如果条件语句出现在循环语句内，在循环的第一次执行中构造出的静态图将固定不再改变 （即使在循环的后续执行中，该条件语句的结果发生了变化）"
msgstr "In the dynamic construction mode (set to False), if the conditional statement appears in the loop statement, the static graph constructed in the first execution of the loop will be fixed and no longer change (even in the subsequent execution of the loop, the conditional statement The result of has changed)"

#: ../../source/user-guide/model-development/jit/trace.rst:164
msgid "将参数固定以便导出"
msgstr "Fix parameters for export"

#: ../../source/user-guide/model-development/jit/trace.rst:166
msgid "有的时候我们希望将一些参数（比如卷积层的卷积核等）固化下来，因此需要指定 ``capture_as_const = True`` :"
msgstr "Sometimes we want to solidify some parameters (such as the convolution kernel of the convolutional layer, etc.), so we need to specify ``capture_as_const = True'':"

#: ../../source/user-guide/model-development/jit/trace.rst:178
msgid "如果想要使用 :py:meth:`~.jit.trace.dump` 导出模型序列化文件并进行后续处理， 则必须在 :py:class:`~.jit.trace` 时固定参数。"
msgstr "If you want to use: py:meth:`~.jit.trace.dump` to export the model serialization file and perform subsequent processing, you must fix the parameters when: py:class:`~.jit.trace`."

#: ../../source/user-guide/model-development/jit/trace.rst:184
msgid "亚线性内（显）存优化"
msgstr "Sub-linear internal (video) memory optimization"

#: ../../source/user-guide/model-development/jit/trace.rst:187
msgid "亚线性内存优化技术的直观好处是能节省显存，换来更大的 Batch size, 但在编译计算图和训练模型时有少量的额外时间开销（以时间换空间）。 MegEngine 提供了两种具体的亚线性内存优化的算法，分别是 `Sublinear <https://arxiv.org/abs/1604.06174>`_ 和 `DTR <https://arxiv.org/abs/2006.09616>`_ 算法。它们的基本原理都是通过事先搜索最优的计算图节点作为前向传播和反向传播检查点（checkpoints）， 省去其它中间结果存储。"
msgstr "The intuitive benefit of sub-linear memory optimization technology is that it can save video memory in exchange for a larger Batch size, but there is a small amount of extra time overhead (time for space) when compiling the calculation graph and training the model. MegEngine provides two specific linear alkylene memory optimization algorithms are Sublinear ` <https://arxiv.org/abs/1604.06174>` _ and the DTR ` <https://arxiv.org/abs/2006.09616>` _ algorithm. Their basic principle is to search for the optimal computational graph node in advance as the forward and backward propagation checkpoints (checkpoints), eliminating the need for other intermediate result storage."

#: ../../source/user-guide/model-development/jit/trace.rst:192
msgid "用户在编译静态图时使用 :class:`~.jit.SublinearMemoryConfig` 设置 :class:`~.jit.trace` 的参数 ``sublinear_memory_config`` ，就可以打开 Sublinear 优化："
msgstr "When compiling static graphs, users use :class:`~.jit.SublinearMemoryConfig` to set :class:`~.jit.trace`, then you can turn on Sublinear optimization："

#: ../../source/user-guide/model-development/jit/trace.rst:205
msgid "用户在编译静态图时使用 :class:`~.jit.DTRConfig` 设置 :class:`~.jit.trace` 的参数 ``dtr_config`` ，就可以打开 DTR 优化："
msgstr "When compiling static graphs, users can use :class:`~.jit.DTRConfig` to set :class:`~.jit.trace`, then DTR optimization："

#: ../../source/user-guide/model-development/jit/trace.rst:220
msgid "关于 ``eviction_threshold`` 的含义与设置，请参考 :ref:`动态图 Sublinear 显存优化 <dtr-guide>`"
msgstr "For the meaning and setting of ``eviction_threshold``, please refer to :ref:`Dynamic graph Sublinear video memory optimization <dtr-guide>`"

#: ../../source/user-guide/model-development/jit/trace.rst:222
msgid "经过测试，在 2080Ti GPU （显存容量为 11GB 左右）训练 ResNet50 模型， 不使用亚线性内存优化，可用的 ``batch_size`` 最大为 100 左右； 使用 Sublinear 优化，可用的 ``batch_size`` 最大为 300 左右； 使用 DTR 优化，可用的 ``batch_size`` 最大为 450 左右，效果十分明显。"
msgstr "After testing, the ResNet50 model is trained on 2080Ti GPU (the memory capacity is about 11GB), without sub-linear memory optimization, the maximum available ``batch_size'' is about 100; using Sublinear optimization, the maximum available ``batch_size'' is 300 Around; using DTR optimization, the maximum available ``batch_size'' is about 450, the effect is very obvious."

#: ../../source/user-guide/model-development/jit/trace.rst:230
msgid "减少访存操作实现加速"
msgstr "Reduce memory access operations to achieve acceleration"

#: ../../source/user-guide/model-development/jit/trace.rst:232
msgid "通常，模型中不仅含有计算受限的操作，还含有一些访存受限操作（如 Elemwsie ）. MegEngine 内嵌了 Codegen 优化机制，它可以在运行时将模型中多个操作融合起来， 并生成可以在目标机器上运行的代码，以此减少访存操作从而达到加速的目的。"
msgstr "Usually, the model contains not only computationally limited operations, but also some memory-restricted operations (such as Elemwsie). MegEngine has an embedded Codegen optimization mechanism, which can merge multiple operations in the model at runtime, and generate The code running on the target machine can reduce the memory access operation and achieve the purpose of acceleration."

#: ../../source/user-guide/model-development/jit/trace.rst:238
msgid "我们在 :class:`~.trace` 接口中传入 ``symbolic=True, opt_level=3``, 即可打开 Codegen 优化。"
msgstr "We pass in ``symbolic=True, opt_level=3'' in the :class:`~.trace` interface, and then Codegen optimization can be turned on."

#: ../../source/user-guide/model-development/jit/trace.rst:240
msgid "关于 ``symbolic`` 参数的说明，请参考 :ref:`symbolic` 。"
msgstr "For the description of the ``symbolic`` parameter, please refer to :ref:`symbolic`."

#: ../../source/user-guide/model-development/jit/trace.rst:242
msgid "MegEngine 的 Codegen 目前集成了三种后端，分别是 NVRTC, HALIDE 和 MLIR. 其中 NVRTC 和 HALIDE 仅支持在 GPU 上使用，MLIR 则同时支持 GPU 和 CPU, 不同的后端生成代码的策略有所不同，所以运行效率也各异。"
msgstr "MegEngine’s Codegen currently integrates three backends, namely NVRTC, HALIDE and MLIR. Among them, NVRTC and HALIDE only support the use on GPU, while MLIR supports both GPU and CPU. Different backends have different code generation strategies. Therefore, the operating efficiency is also different."

#: ../../source/user-guide/model-development/jit/trace.rst:246
msgid "我们可以通过设置 ``MGB_JIT_BACKEND`` 环境变量来改变 Codegen 的后端，例如："
msgstr "We can change the backend of Codegen by setting the ``MGB_JIT_BACKEND'' environment variable, such as："

#: ../../source/user-guide/model-development/jit/trace.rst:252
msgid "该环境变量在 NVIDIA GPU 环境下可取的值为 NVRTC, HALIDE 和 MLIR, 默认值为 HALIDE."
msgstr "The possible values of this environment variable in the NVIDIA GPU environment are NVRTC, HALIDE and MLIR, and the default value is HALIDE."

#: ../../source/user-guide/model-development/jit/trace.rst:254
msgid "对于 CPU, 目前暂时仅支持 MLIR 后端。"
msgstr "For CPU, only MLIR backend is currently supported."

#: ../../source/user-guide/model-development/jit/trace.rst:258
msgid "如果想要使用 MLIR 后端, 需要单独编译 MegEngine. 在使用 CMake 时换成如下命令："
msgstr "If you want to use the MLIR backend, you need to compile MegEngine separately. Change to the following command when using CMake："

#: ../../source/user-guide/model-development/jit/trace.rst:264
msgid "然后设置如下的环境变量："
msgstr "Then set the following environment variables："

#: ../../source/user-guide/model-development/jit/trace.rst:273
msgid "指定代码不被转换"
msgstr "The specified code is not converted"

#: ../../source/user-guide/model-development/jit/trace.rst:275
msgid "使用 :py:func:`~.exclude_from_trace` ，其中的代码不会被 trace, 且允许访问静态区域的 :py:class:`~megengine.Tensor` ."
msgstr "Use: py:func:`~.exclude_from_trace`, the code in it will not be traced, and allow access to the static area: py:class:`~megengine.Tensor`."

#: ../../source/user-guide/model-development/jit/trace.rst:277
msgid "示例代码如下："
msgstr "The sample code is as："

#: ../../source/user-guide/model-development/jit/trace.rst:295
msgid "输出为："
msgstr "Output is："

#~ msgid ""
#~ "亚线性内存优化技术的直观好处是能节省显存，换来更大的 Batch size, "
#~ "但在编译计算图和训练模型时有少量的额外时间开销（以时间换空间）。 其基本原理是：基于 `Gradient "
#~ "Checkpointing <https://arxiv.org/abs/1604.06174>`_ 算法， "
#~ "通过事先搜索最优的计算图节点作为前向传播和反向传播检查点（checkpoints），省去其它中间结果存储。"
#~ msgstr ""

#~ msgid ""
#~ "用户在编译静态图时使用 :class:`~.jit.SublinearMemoryConfig` 设置 "
#~ ":class:`~.jit.trace` 的参数 ``sublinear_memory_config`` "
#~ "，就可以打开亚线性内存优化："
#~ msgstr ""

#~ msgid ""
#~ "经过测试，在 2080Ti GPU （显存容量为 11GB 左右）训练 "
#~ "ResNet50 模型， 不使用亚线性内存优化，可用的 ``batch_size`` 最大为"
#~ " 100 左右； 使用亚线性内存优化，可用的 ``batch_size`` 最大为"
#~ " 200 左右，效果十分明显。"
#~ msgstr ""

