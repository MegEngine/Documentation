msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-05-12 09:37+0800\n"
"PO-Revision-Date: 2021-06-03 16:00\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/model-compression/quantization.po\n"
"X-Crowdin-File-ID: 2860\n"

#: ../../source/user-guide/model-compression/quantization.rst:5
msgid "量化（Quantization）"
msgstr "Quantization"

#: ../../source/user-guide/model-compression/quantization.rst:7
msgid "量化指的是将浮点数模型（一般是 32 位浮点数）的权重或激活值用位数更少的数值类型 （比如 8 位整数、16 位浮点数）来近似表示的过程。 量化后的模型会占用更小的存储空间，还能够利用许多硬件平台上的专属算子进行提速。 比如在 MegEngine 中使用 8 位整数来进行量化，相比默认的 32 位浮点数， 模型大小可以减少为 1/4，而运行在特定的设备上其计算速度也能提升为 2-4 倍。"
msgstr "Quantization refers to the process of approximating the weight or activation value of a floating-point number model (usually a 32-bit floating-point number) with numerical types with fewer digits (such as 8-bit integers, 16-bit floating-point numbers). The quantified model will take up less storage space, and it can also use proprietary operators on many hardware platforms to speed up. For example, using 8-bit integers for quantization in MegEngine, compared to the default 32-bit floating point number, the model size can be reduced to 1/4, and the calculation speed can be increased to 2-4 times when running on a specific device."

#: ../../source/user-guide/model-compression/quantization.rst:13
msgid "量化的目的是为了追求极致的推理计算速度，为此舍弃了数值表示的精度，直觉上会带来较大的模型掉点， 但是在使用一系列精细的量化处理之后，其掉点可以变得微乎其微，并能支持正常的部署使用。 而且近年来随着专用神经网络加速芯片的兴起，低比特非浮点的运算方式越来越普及， 因此如何把一个 GPU 上训练的浮点数模型转化为低比特的量化模型，就成为了工业界非常关心的话题。"
msgstr "The purpose of quantization is to pursue the ultimate inference calculation speed. For this reason, the precision of numerical representation is discarded. Intuitively, it will bring a larger model drop point, but after a series of fine quantization processing, its drop point can become It is minimal and can support normal deployment and use. And in recent years, with the rise of dedicated neural network acceleration chips, low-bit non-floating point arithmetic methods have become more and more popular. Therefore, how to convert a floating-point number model trained on a GPU into a low-bit quantization model has become the industry Very concerned topic."

#: ../../source/user-guide/model-compression/quantization.rst:18
msgid "一般来说，得到量化模型的转换过程按代价从低到高可以分为以下 4 种："
msgstr "In general, to obtain a quantitative model of the conversion process by the cost from low to high can be divided into the following four："

#: ../../source/user-guide/model-compression/quantization.rst:23
msgid "Type1 和 Type2 由于是在模型浮点模型训练之后介入，无需大量训练数据， 故而转换代价更低，被称为后量化（Post Quantization）；"
msgstr "Type1 and Type2 intervene after the model floating-point model training and do not require a large amount of training data, so the conversion cost is lower, which is called Post Quantization;"

#: ../../source/user-guide/model-compression/quantization.rst:25
msgid "Type3 和 Type4 则需要在浮点模型训练时就插入一些假量化（FakeQuantize）算子， 模拟计算过程中数值截断后精度降低的情形，故而称为量化感知训练（Quantization Aware Training, QAT）。"
msgstr "Type3 and Type4 need to insert some FakeQuantize operators during the floating-point model training to simulate the reduction of accuracy after numerical truncation during the calculation process, so it is called quantization awareness training (Quantization Aware Training, QAT)."

#: ../../source/user-guide/model-compression/quantization.rst:28
msgid "本文主要介绍 Type2 和 Type3 在 MegEngine 中的完整流程。 事实上，除了 Type2 无需进行假量化，两者的整体流程完全一致。"
msgstr "This article mainly introduces the complete process of Type2 and Type3 in MegEngine. In fact, except for Type2, no false quantification is required, and the overall process of the two is exactly the same."

#: ../../source/user-guide/model-compression/quantization.rst:32
msgid "整体流程"
msgstr "Overall process"

#: ../../source/user-guide/model-compression/quantization.rst:34
msgid "以 Type3 为例，一般以一个训练完毕的浮点模型为起点，称为 Float 模型。 包含假量化算子的用浮点操作来模拟量化过程的新模型，我们称之为 Quantized-Float 模型，或者 QFloat 模型。 可以直接在终端设备上运行的模型，称之为 Quantized 模型，简称 Q 模型。"
msgstr "Taking Type3 as an example, a trained floating-point model is generally used as a starting point, which is called a Float model. A new model that uses floating-point operations to simulate the quantization process that includes pseudo-quantization operators is called the Quantized-Float model, or QFloat model. The model that can be run directly on the terminal device is called the Quantized model, or Q model for short."

#: ../../source/user-guide/model-compression/quantization.rst:38
msgid "而三者的精度一般是 ``Float > QFloat > Q`` ，故而一般量化算法也就分为两步："
msgstr "The accuracy of the three generally `` Float> QFloat> Q``, quantization algorithm will therefore typically divided into two steps："

#: ../../source/user-guide/model-compression/quantization.rst:40
msgid "拉近 QFloat 和 Q，这样训练阶段的精度可以作为最终 Q 精度的代理指标，这一阶段偏工程；"
msgstr "Close QFloat and Q, so that the accuracy of the training phase can be used as a proxy indicator of the final Q accuracy. This phase is partial engineering;"

#: ../../source/user-guide/model-compression/quantization.rst:41
msgid "拔高 QFloat 逼近 Float，这样就可以将量化模型性能尽可能恢复到 Float 的精度，这一阶段偏算法。"
msgstr "Raise the QFloat to approximate the Float, so that the quantized model performance can be restored to the accuracy of the Float as much as possible. This stage is partial to the algorithm."

#: ../../source/user-guide/model-compression/quantization.rst:43
msgid "典型的三种模型在三个阶段的精度变化如下："
msgstr "The accuracy changes of the three typical models in the three stages are as follows："

#: ../../source/user-guide/model-compression/quantization.rst:48
msgid "对应到具体的 MegEngine 接口中，三阶段如下："
msgstr "Corresponding to the specific MegEngine interface, the three stages are as follows："

#: ../../source/user-guide/model-compression/quantization.rst:50
msgid "基于 :class:`~.module.Module` 搭建网络模型，并按照正常的浮点模型方式进行训练；"
msgstr "Build a network model based on :class:`~.module.Module`, and train according to the normal floating-point model;"

#: ../../source/user-guide/model-compression/quantization.rst:51
msgid "使用 :func:`~.quantization.quantize_qat` 将浮点模型转换为 QFloat 模型， 其中可被量化的关键 Module 会被转换为 :class:`~.module.qat.QATModule` ， 并基于量化配置 :class:`~.quantization.QConfig` 设置好假量化算子和数值统计方式；"
msgstr "Use :func:`~.quantization.quantize_qat` to convert a floating-point model to a QFloat model, where the key module that can be quantized will be converted to :class:`~.module.qat.QATModule` and based on the quantization configuration :class:`~.quantization. QConfig` set up false quantization operators and numerical statistics methods;"

#: ../../source/user-guide/model-compression/quantization.rst:54
msgid "使用 :func:`~.quantization.quantize` 将 QFloat 模型转换为 Q 模型， 对应的 QATModule 则会被转换为 :class:`~.module.quantized.QuantizedModule` ， 此时网络无法再进行训练，网络中的算子都会转换为低比特计算方式，即可用于部署了。"
msgstr "Use :func:`~.quantization.quantize` to convert the QFloat model to the Q model, and the corresponding QATModule will be converted to :class:`~.module.quantized.QuantizedModule`, at this time the network can no longer be trained, and the operators in the network will be Converted to a low-bit calculation method, it can be used for deployment."

#: ../../source/user-guide/model-compression/quantization.rst:58
msgid "该流程是 Type3 对应 QAT 的步骤，Type2 对应的后量化则需使用不同 QConfig， 且需使用 evaluation 模式运行 QFloat 模型，而非训练模式。更多细节可以继续阅读下一节详细的接口介绍。"
msgstr "This process is the steps of Type3 corresponding to QAT, and Type2 corresponding post-quantization needs to use different QConfig, and the QFloat model needs to be run in evaluation mode instead of training mode. For more details, you can continue reading the detailed interface introduction in the next section."

#: ../../source/user-guide/model-compression/quantization.rst:62
msgid "接口介绍"
msgstr "Interface introduction"

#: ../../source/user-guide/model-compression/quantization.rst:64
msgid "在 MegEngine 中，最上层的接口是配置如何量化的 :class:`~.quantization.QConfig` 和模型转换模块里的 :func:`~.quantization.quantize_qat` 与 :func:`~.quantization.quantize` 。"
msgstr "In MegEngine, the top-level interface is :class: :func:`~.quantization.quantize_qat` and :func:`~.quantization.quantize` in the model conversion module."

#: ../../source/user-guide/model-compression/quantization.rst:68
msgid "QConfig"
msgstr "QConfig"

#: ../../source/user-guide/model-compression/quantization.rst:70
msgid "QConfig 包括了 :class:`~.quantization.Observer` 和 :class:`~.quantization.FakeQuantize` 两部分。 我们知道，对模型转换为低比特量化模型一般分为两步： 一是统计待量化模型中参数和 activation 的数值范围（scale）和零点（zero_point）， 二是根据 scale 和 zero_point 将模型转换成指定的数值类型。而为了统计这两个值，我们需要使用 Observer."
msgstr "QConfig includes :class:`~.quantization.Observer` and :class:`~.quantization.FakeQuantize`. We know that the model is converted to a low-bit quantization-step model is generally divided into： one statistical model parameter to be quantized values and the activation range (scale) and zero (zero_point), according to the scale and the second is converted into a specified model zero_point The numeric type. In order to count these two values, we need to use Observer."

#: ../../source/user-guide/model-compression/quantization.rst:75
msgid "Observer 继承自 :class:`~.module.Module` ，也会参与网络的前向传播， 但是其 forward 的返回值就是输入，所以不会影响网络的反向梯度传播。 其作用就是在前向时拿到输入的值，并统计其数值范围，并通过 :meth:`~.quantization.Observer.get_qparams` 来获取。 所以在搭建网络时把需要统计数值范围的的 Tensor 作为 Observer 的输入即可。"
msgstr "Observer inherits from :class:`~.module.Module` and will also participate in the forward propagation of the network, but the return value of its forward is the input, so it will not affect the backward gradient propagation of the network. Its function is to get the input value in the forward direction, and count its numerical range, and get it through :meth:`~.quantization.Observer.get_qparams`. Therefore, when building the network, the Tensor that needs the statistical value range can be used as the input of the Observer."

#: ../../source/user-guide/model-compression/quantization.rst:92
msgid "另外如果只观察而不模拟量化会导致模型掉点，于是我们需要有 FakeQuantize 来根据 Observer 观察到的数值范围模拟量化时的截断，使得参数在训练时就能提前“适应“这种操作。 FakeQuantize 在前向时会根据传入的 scale 和 zero_point 对输入 Tensor 做模拟量化的操作， 即先做一遍数值转换再转换后的值还原成原类型，如下所示："
msgstr "In addition, if only observing and not simulating quantization will cause the model to drop points, so we need FakeQuantize to simulate the truncation of quantization according to the value range observed by Observer, so that the parameters can be \"adapted\" to this operation in advance during training. When the front FakeQuantize based on incoming analog scale and zero_point make the Tensor quantization operation on the input, i.e. the value reduction do it first and then converted values into the original type, as shown below："

#: ../../source/user-guide/model-compression/quantization.rst:112
msgid "目前 MegEngine 支持对 weight/activation 两部分的量化，如下所示："
msgstr "MegEngine currently supported weight / activation quantization two parts, as shown below："

#: ../../source/user-guide/model-compression/quantization.rst:123
msgid "这里使用了两种 Observer 来统计信息，而 FakeQuantize 使用了默认的算子。"
msgstr "Two Observers are used here for statistics, and FakeQuantize uses the default operator."

#: ../../source/user-guide/model-compression/quantization.rst:125
msgid "如果是后量化，或者说 Calibration，由于无需进行 FakeQuantize，故而其 fake_quant 属性为 None 即可："
msgstr "If it is post-quantization, or Calibration, since FakeQuantize is not required, the fake_quant attribute is None, which is："

#: ../../source/user-guide/model-compression/quantization.rst:136
msgid "除了使用在 :class:`~.quantization.Qconfig` 里提供的预设 QConfig， 也可以根据需要灵活选择 Observer 和 FakeQuantize  实现自己的 QConfig。目前提供的 Observer 包括："
msgstr "In addition to using :class:`~.quantization.Qconfig`, you can also flexibly choose Observer and FakeQuantize to implement your own QConfig according to your needs. The Observer currently provided includes："

#: ../../source/user-guide/model-compression/quantization.rst:139
msgid ":class:`~.quantization.MinMaxObserver` ， 使用最简单的算法统计 min/max，对见到的每批数据取 min/max 跟当前存的值比较并替换， 基于 min/max 得到 scale 和 zero_point；"
msgstr ":class:`~.quantization.MinMaxObserver`, use the simplest algorithm to count min/max, take min/max for each batch of data seen, compare and replace with the current stored value, and get scale and zero_point based on min/max;"

#: ../../source/user-guide/model-compression/quantization.rst:142
msgid ":class:`~.quantization.ExponentialMovingAverageObserver` ， 引入动量的概念，对每批数据的 min/max 与现有 min/max 的加权和跟现有值比较；"
msgstr ":class:`~.quantization.ExponentialMovingAverageObserver`, introduces the concept of momentum, and compares the weighted sum of min/max and the existing min/max of each batch of data with the existing value;"

#: ../../source/user-guide/model-compression/quantization.rst:144
msgid ":class:`~.quantization.HistogramObserver` ， 更加复杂的基于直方图分布的 min/max 统计算法，且在 forward 时持续更新该分布， 并根据该分布计算得到 scale 和 zero_point。"
msgstr ":class:`~.quantization.HistogramObserver`, a more complex min/max statistical algorithm based on histogram distribution, and continuously update the distribution during forward, and calculate the scale and zero_point according to the distribution."

#: ../../source/user-guide/model-compression/quantization.rst:148
msgid "对于 FakeQuantize，目前还提供了 :class:`~.quantization.TQT` 算子， 另外还可以继承 ``_FakeQuant`` 基类实现自定义的假量化算子。"
msgstr "For FakeQuantize, the :class:`~.quantization.TQT` operator is currently provided, and the ``_FakeQuant`` base class can also be inherited to implement custom false quantization operators."

#: ../../source/user-guide/model-compression/quantization.rst:151
msgid "在实际使用过程中，可能需要在训练时让 Observer 统计并更新参数，但是在推理时则停止更新。 Observer 和 FakeQuantize 都支持 :meth:`~.quantization.Observer.enable` 和 :meth:`~.quantization.Observer.disable` 功能， 且 Observer 会在 :meth:`~module.Module.train` 和 :meth:`~module.Module.eval` 时自动分别调用 enable/disable。"
msgstr "In actual use, it may be necessary to let Observer count and update parameters during training, but stop updating during inference. Observer and FakeQuantize both support :meth:`~.quantization.Observer.enable` and :meth:`~.quantization.Observer.disable` functions, and Observer will be in :meth:`~module.Module.train` and :meth:`~module.Module.eval `Automatically call enable/disable respectively."

#: ../../source/user-guide/model-compression/quantization.rst:157
msgid "所以一般在 Calibration 时，会先执行 ``net.eval()`` 保证网络的参数不被更新， 然后再执行 :``enable_observer(net)`` 来手动开启 Observer 的统计修改功能。"
msgstr "Therefore, during calibration, ``net.eval()'' will be executed first to ensure that the network parameters are not updated, and then: ``enable_observer(net)'' will be executed to manually enable the statistical modification function of the Observer."

#: ../../source/user-guide/model-compression/quantization.rst:161
msgid "模型转换模块与相关基类"
msgstr "Model conversion module and related base classes"

#: ../../source/user-guide/model-compression/quantization.rst:163
msgid "QConfig 提供了一系列如何对模型做量化的接口，而要使用这些接口， 需要网络的 Module 能够在 forward 时给参数、activation 加上 Observer 和进行 FakeQuantize. 转换模块的作用就是将模型中的普通 Module 替换为支持这一系列操作的 :class:`~.module.qat.QATModule` ， 并能支持进一步替换成无法训练、专用于部署的 :class:`~.module.quantized.QuantizedModule` 。"
msgstr "QConfig provides a series of interfaces on how to quantify the model, and to use these interfaces, the module of the network needs to be able to add Observer and FakeQuantize to the parameters and activation during forwarding. The function of the conversion module is to replace the ordinary Module in the model. In order to support this series of operations, :class:`~.module.qat.QATModule` can be further replaced with :class:`~.module.quantized.QuantizedModule` that cannot be trained and dedicated to deployment."

#: ../../source/user-guide/model-compression/quantization.rst:168
msgid "基于三种基类实现的 Module 是一一对应的关系，通过转换接口可以依次替换为不同实现的同名 Module。 同时考虑到量化与算子融合（Fuse）的高度关联，我们提供了一系列预先融合好的 Module， 比如 :class:`~.module.ConvRelu2d` 、 :class:`~.module.ConvBn2d` 和 :class:`~.module.ConvBnRelu2d` 等。 除此之外还提供专用于量化的 :class:`~.module.QuantStub` 、 :class:`~.module.DequantStub` 等辅助模块。"
msgstr "Modules implemented based on the three base classes have a one-to-one correspondence, and can be replaced by modules with the same name in different implementations through the conversion interface. At the same time, considering the high correlation between quantization and operator fusion (Fuse), we provide a series of pre-integrated modules, such as :class:`~.module.ConvRelu2d`, :class:`~.module.ConvBn2d` and :class:`~.module .ConvBnRelu2d` etc. :class:`~.module.QuantStub`, :class:`~.module.DequantStub` and other auxiliary modules dedicated to quantification are also provided."

#: ../../source/user-guide/model-compression/quantization.rst:173
msgid "转换的原理很简单，就是将父 Module 中可被量化（Quantable）的子 Module 替换为对应的新 Module. 但是有一些 Quantable Module 还包含 Quantable 子 Module，比如 ConvBn 就包含一个 Conv2d 和一个 BatchNorm2d， 转换过程并不会对这些子 Module 进一步转换，原因是父 Module 被替换之后， 其 forward 计算过程已经完全不同了，不会再依赖于这些子 Module。"
msgstr "The principle of conversion is very simple. It is to replace Quantable sub-Modules in the parent Module with the corresponding new Modules. However, some Quantable Modules also contain Quantable sub-Modules. For example, ConvBn contains a Conv2d and a BatchNorm2d. These child modules will not be further converted, because after the parent module is replaced, its forward calculation process is completely different and will no longer depend on these child modules."

#: ../../source/user-guide/model-compression/quantization.rst:180
msgid "如果需要使一部分 Module 及其子 Module 保留 Float 状态，不进行转换， 可以使用 :meth:`~.module.Module.disable_quantize` 来处理。"
msgstr "If you need to keep a part of Module and its sub-modules in Float state without conversion, you can use :meth:`~.module.Module.disable_quantize` to process."

#: ../../source/user-guide/model-compression/quantization.rst:183
msgid "如果网络结构中涉及一些二元及以上的 ElementWise 操作符，比如加法乘法等， 由于多个输入各自的 scale 并不一致，必须使用量化专用的算子，并指定好输出的 scale. 实际使用中只需要把这些操作替换为 :class:`~.module.Elemwise` 即可， 比如 ``self.add_relu = Elemwise(\"FUSE_ADD_RELU\")``"
msgstr "If some binary and above ElementWise operators are involved in the network structure, such as addition and multiplication, etc., since the scales of multiple inputs are not consistent, you must use a special quantization operator and specify the output scale. In actual use, only need Just replace these operations with :class:`~.module.Elemwise`, such as ``self.add_relu = Elemwise(\"FUSE_ADD_RELU\")''"

#: ../../source/user-guide/model-compression/quantization.rst:188
msgid "另外由于转换过程修改了原网络结构，模型保存与加载无法直接适用于转换后的网络， 读取新网络保存的参数时，需要先调用转换接口得到转换后的网络，才能用 load_state_dict 将参数进行加载。"
msgstr "In addition, because the conversion process has modified the original network structure, model saving and loading cannot be directly applied to the converted network. When reading the parameters saved in the new network, you need to call the conversion interface to get the converted network before loading the parameters with load_state_dict ."

#: ../../source/user-guide/model-compression/quantization.rst:192
msgid "实例讲解"
msgstr "Example explanation"

#: ../../source/user-guide/model-compression/quantization.rst:194
msgid "下面我们以 ResNet18 为例来讲解量化的完整流程，完整代码见 ``MegEngine/Models`` . 主要分为以下几步："
msgstr "Here we have an example to explain ResNet18 quantify the complete process, complete code, see `` MegEngine / Models``. Consists of the following steps："

#: ../../source/user-guide/model-compression/quantization.rst:196
msgid "修改网络结构，使用已经 Fuse 好的 ConvBn2d、ConvBnRelu2d、ElementWise 代替原先的 Module；"
msgstr "Modify the network structure and use Fuse-ready ConvBn2d, ConvBnRelu2d, ElementWise to replace the original Module;"

#: ../../source/user-guide/model-compression/quantization.rst:197
msgid "在正常模式下预训练模型，并在每轮迭代保存网络检查点；"
msgstr "Pre-train the model in normal mode and save network checkpoints in each iteration;"

#: ../../source/user-guide/model-compression/quantization.rst:198
msgid "调用 :func:`~.quantization.quantize_qat` 转换模型，并进行 finetune；"
msgstr "Call :func:`~.quantization.quantize_qat` to convert the model and perform finetune;"

#: ../../source/user-guide/model-compression/quantization.rst:199
msgid "调用 :func:`~.quantization.quantize` 转换为量化模型，并执行 dump 用于后续模型部署。"
msgstr "Call :func:`~.quantization.quantize` to convert to a quantized model, and execute dump for subsequent model deployment."

#: ../../source/user-guide/model-compression/quantization.rst:201
msgid "网络结构见 ``resnet.py`` ，相比惯常写法，我们修改了其中一些子 Module， 将原先单独的 ``conv``, ``bn``, ``relu`` 替换为 Fuse 过的 Quantable Module。"
msgstr "See ``resnet.py`` for the network structure. Compared with the usual writing, we have modified some of the sub-modules, and replaced the original ``conv``, ``bn``, ``relu`` with Fuse Quantable Module."

#: ../../source/user-guide/model-compression/quantization.rst:229
msgid "然后对该模型进行若干轮迭代训练，并保存检查点，这里省略细节："
msgstr "The model is then trained a number of iterations, and saving the checkpoint, details will be omitted here："

#: ../../source/user-guide/model-compression/quantization.rst:248
msgid "再调用 :func:`~.quantization.quantize_qat` 来将网络转换为 QATModule："
msgstr "Then call :func:`~.quantization.quantize_qat` to convert the network to QATModule："

#: ../../source/user-guide/model-compression/quantization.rst:259
msgid "这里使用默认的 ``ema_fakequant_qconfig`` 来进行 ``int8`` 量化。"
msgstr "Here, the default ``ema_fakequant_qconfig`` is used to perform ``int8`` quantization."

#: ../../source/user-guide/model-compression/quantization.rst:261
msgid "然后我们继续使用上面相同的代码进行 finetune 训练。 值得注意的是，如果这两步全在一次程序运行中执行，那么训练的 trace 函数需要用不一样的， 因为模型的参数变化了，需要重新进行编译。 示例代码中则是采用在新的执行中读取检查点重新编译的方法。"
msgstr "Then we continue to use the same code above for finetune training. It is worth noting that if these two steps are all executed in one program run, then the trace function for training needs to be different, because the model parameters have changed and need to be recompiled. The sample code adopts the method of reading checkpoints and recompiling in the new execution."

#: ../../source/user-guide/model-compression/quantization.rst:266
msgid "在 QAT 模式训练完成后，我们继续保存检查点，执行 ``inference.py`` 并设置 ``mode`` 为 ``quantized`` ， 这里需要将原始 Float 模型转换为 QAT 模型之后再加载检查点。"
msgstr "After the QAT mode training is completed, we continue to save the checkpoint, execute ``inference.py`` and set the ``mode'' to ``quantized``, here we need to convert the original Float model to a QAT model before loading the checkpoint ."

#: ../../source/user-guide/model-compression/quantization.rst:281
msgid "模型转换为量化模型包括以下几步："
msgstr "Model to model comprises the steps of quantizing："

#: ../../source/user-guide/model-compression/quantization.rst:312
msgid "至此便得到了一个可用于部署的量化模型。"
msgstr "At this point, a quantitative model that can be used for deployment is obtained."

#~ msgid "量化"
#~ msgstr "Quantify"

