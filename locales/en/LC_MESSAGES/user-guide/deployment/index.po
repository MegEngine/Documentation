msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-15 19:44+0800\n"
"PO-Revision-Date: 2021-06-03 10:21\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/deployment/index.po\n"
"X-Crowdin-File-ID: 2862\n"
"Language: en_US\n"

#: ../../source/user-guide/deployment/index.rst:5
msgid "将模型部署到 C++ 环境"
msgstr "Deploy the model to the C++ environment"

#: ../../source/user-guide/deployment/index.rst:7
msgid "MegEngine 的一大核心优势是 “训练推理一体化”，其中 “训练” 是在 Python 环境中进行的， 而 “推理” 则特指在 C++ 环境下使用训练完成的模型进行推理。 而将模型迁移到无需依赖 Python 的环境中，使其能正常进行推理计算，被称为 **部署** 。 部署的目的是简化除了模型推理所必需的一切其它依赖，使推理计算的耗时变得尽可能少， 比如手机人脸识别场景下会需求毫秒级的优化，而这必须依赖于 C++ 环境才能实现。"
msgstr "One of the core advantages of MegEngine is \"integration of training and inference\", where \"training\" is carried out in a Python environment, and \"inference\" specifically refers to using the trained model for inference in a C++ environment. Migrating the model to an environment that does not need to rely on Python so that it can perform inference calculations normally is called **deployment**. The purpose of deployment is to simplify all other dependencies except model reasoning, so that the time-consuming calculation of reasoning becomes as little as possible. For example, in the mobile phone face recognition scenario, millisecond-level optimization is required, which must rely on the C++ environment ."

#: ../../source/user-guide/deployment/index.rst:13
msgid "本文将从一个训练好的异或（XOR）网络模型出发，讲解如何将其部署到 CPU（X86）环境下运行。"
msgstr "This article will start from a trained exclusive OR (XOR) network model and explain how to deploy it to run in a CPU (X86) environment."

#: ../../source/user-guide/deployment/index.rst:16
msgid "将模型序列化并导出"
msgstr "Serialize and export the model"

#: ../../source/user-guide/deployment/index.rst:18
msgid "首先我们需要 :ref:`dump` , 用到的模型定义与序列化代码在 :src:`sdk/xor-deploy/xornet.py` ."
msgstr "First we need :ref:`dump`, the model definition and serialization code used are in :src:`sdk/xor-deploy/xornet.py`."

#: ../../source/user-guide/deployment/index.rst:21
msgid "编写 C++ 程序读取模型"
msgstr "Write a C++ program to read the model"

#: ../../source/user-guide/deployment/index.rst:23
msgid "接下来我们需要编写一个 C++ 程序，来实现我们期望在部署平台上完成的功能。"
msgstr "Next we need to write a C++ program to implement the functions we expect to be completed on the deployment platform."

#: ../../source/user-guide/deployment/index.rst:25
msgid "继续以上面导出的异或网络模型为例子，我们实现一个最简单的功能—— 给定两个浮点数，输出对其做异或操作后结果为 0 的概率（以及为 1 的概率）。"
msgstr "Continuing to take the XOR network model derived above as an example, we implement the simplest function-given two floating-point numbers, output the probability that the result will be 0 (and the probability of being 1) after XORing them."

#: ../../source/user-guide/deployment/index.rst:28
msgid "在此之前，为了能够正常使用 MegEngine 底层 C++ 接口， 需要先按照 MegeEngine 中提供的编译脚本( :src:`scripts/` ) 从源码编译得到 MegEngine 的相关库, 通过这些脚本可以交叉编译安卓（ARMv7，ARMv8，ARMv8.2）版本、Linux 版本（ARMv7，ARMv8，ARMv8.2）以及 iOS 相关库， 也可以本机编译 Windows/Linux/MacOS 相关库文件。"
msgstr "Prior to this, in order to be able to use the underlying C++ interface of MegEngine normally, you need to :src:`scripts/` ). Through these scripts, you can cross compile Android (ARMv7, ARMv8, ARMv8.2) version, Linux version (ARMv7, ARMv8, ARMv8.2) and iOS related libraries, you can also compile Windows/Linux/MacOS related library files natively."

#: ../../source/user-guide/deployment/index.rst:33
msgid "实现上述异或计算的示例 C++ 代码如下（引自 ``xor-deploy.cpp`` ）："
msgstr "The sample C++ code for realizing the above XOR calculation is as follows (quoted from ``xor-deploy.cpp``)："

#: ../../source/user-guide/deployment/index.rst:78
msgid "简单解释一下代码的意思："
msgstr "Briefly explain the meaning of the code："

#: ../../source/user-guide/deployment/index.rst:80
msgid "我们首先通过 ``GraphLoader`` 将模型加载进来，"
msgstr "We first load the model through ``GraphLoader``,"

#: ../../source/user-guide/deployment/index.rst:81
msgid "接着通过 ``tensor_map`` 和上节指定的输入名称 ``data`` ，找到模型的输入指针，"
msgstr "Then find the input pointer of the model through ``tensor_map`` and the input name ``data`` specified in the previous section,"

#: ../../source/user-guide/deployment/index.rst:82
msgid "再将运行时提供的输入 ``x`` 和 ``y`` 赋值给输入指针，"
msgstr "Then assign the input ``x`` and ``y`` provided at runtime to the input pointer,"

#: ../../source/user-guide/deployment/index.rst:83
msgid "然后我们使用 ``network.graph->compile`` 将模型编译成一个函数接口，并调用执行，"
msgstr "Then we use ``network.graph->compile`` to compile the model into a functional interface, and call execution,"

#: ../../source/user-guide/deployment/index.rst:84
msgid "最后将得到的结果 ``predict`` 进行输出，该输出的两个值即为异或结果为 0 的概率以及为 1 的概率。"
msgstr "Finally, output the result ``predict``, and the two values of the output are the probability that the XOR result is 0 and the probability that it is 1."

#: ../../source/user-guide/deployment/index.rst:86
msgid "另外可以配置上面加载模型时候的 ``config`` 来优化 Inference 计算效率， 为了加速一般在 ARM 上面配置 ``enable_nchw44_layout()`` , 在x86 CPU 上面配置 ``enable_nchw88_layout()`` ，具体的配置方法参考 :ref:`load-and-run` ."
msgstr "In addition, you can configure the ``config'' when loading the model above to optimize the calculation efficiency of Inference. In order to speed up the general configuration of ``enable_nchw44_layout()'' on the ARM, and ``enable_nchw88_layout()'' on the x86 CPU, the specific configuration Method reference :ref:`load-and-run`."

#: ../../source/user-guide/deployment/index.rst:91
msgid "编译与执行"
msgstr "Compile and execute"

#: ../../source/user-guide/deployment/index.rst:93
msgid "为了更完整地实现 “训练推理一体化”，我们还需要支持同一个 C++ 程序能够交叉编译到不同平台上执行，而不需要修改代码。 之所以能够实现不同平台一套代码，是由于底层依赖的算子库（内部称作 MegDNN）实现了对不同平台接口的封装， 在编译时会自动根据指定的目标平台选择兼容的接口。"
msgstr "In order to realize the \"integration of training and reasoning\" more completely, we also need to support the same C++ program to be cross-compiled for execution on different platforms without modifying the code. The reason why a set of codes for different platforms can be realized is because the underlying operator library (called MegDNN internally) realizes the encapsulation of the interfaces of different platforms, and the compatible interface is automatically selected according to the specified target platform during compilation."

#: ../../source/user-guide/deployment/index.rst:99
msgid "目前发布的版本我们开放了对 CPU（X86、X64、ARMv7、ARMv8、ARMv8.2） 和 GPU（CUDA）平台的 float 和量化 int8 的支持。"
msgstr "In the currently released version, we have opened up support for float and quantized int8 on CPU (X86, X64, ARMv7, ARMv8, ARMv8.2) and GPU (CUDA) platforms."

#: ../../source/user-guide/deployment/index.rst:102
msgid "我们在这里以 CPU（X86）平台为例，首先直接使用 gcc 或者 clang （用 ``$CXX`` 指代）进行编译即可："
msgstr "Here we take the CPU (X86) platform as an example. First, directly use gcc or clang ($CXX``) to compile："

#: ../../source/user-guide/deployment/index.rst:108
msgid "上面的 ``$MGE_INSTALL_PATH`` 指代了编译安装时通过 ``CMAKE_INSTALL_PREFIX`` 指定的安装路径。 编译完成之后，通过以下命令执行即可："
msgstr "The ``$MGE_INSTALL_PATH`` above refers to the installation path specified by ``CMAKE_INSTALL_PREFIX`` during compilation and installation. After the compilation is complete, execute the following command to："

#: ../../source/user-guide/deployment/index.rst:115
msgid "这里将 ``$MGE_INSTALL_PATH/lib`` 加进 ``LD_LIBRARY_PATH`` 环境变量，确保 MegEngine 库可以被编译器找到。 上面命令对应的输出如下："
msgstr "Here, add ``$MGE_INSTALL_PATH/lib`` to the ``LD_LIBRARY_PATH`` environment variable to ensure that the MegEngine library can be found by the compiler. The output corresponding to the above command is as follows："

#: ../../source/user-guide/deployment/index.rst:122
msgid "至此我们便完成了从 Python 模型到 C++ 可执行文件的部署流程， 如果需要快速的运行模型以及测试模型性能，请参考 :ref:`load-and-run` ."
msgstr "So far we have completed the deployment process from the Python model to the C++ executable file. If you need to run the model quickly and test the performance of the model, please refer to :ref:`load-and-run`."

