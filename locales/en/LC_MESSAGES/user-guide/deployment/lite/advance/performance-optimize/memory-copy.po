# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2021, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.7\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-28 09:14+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:5
msgid "输入输出内存拷贝优化"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:7
msgid ""
"MegEngine Lite 中的内存拷贝优化主要指输入输出 Tensor "
"内存拷贝的优化，模型内部固有的内存拷贝优化不能够被优化，主要有下面几种情况："
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:9
msgid ""
"device IO 优化：输入数据本来就不是 CPU 端的内存，如：是一段 CUDA 的内存或者一段 OpenCL "
"的内存，希望模型推理直接使用这段内存作为输入，避免将其拷贝到 CPU 端，然后再在模型内部从 CPU 拷贝到设备上，节省两次内存拷贝。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:11
msgid ""
"输入输出零拷贝：希望模型的推理结果保存在用户提供的内存中，避免将数据保存在 MegEngine "
"自己申请的内存中，然后再将内存拷贝到用户指定的内存中。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:14
msgid "Device IO 优化"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:16
msgid ""
"MegEngine Lite 支持模型的输入输出配置，用户可以根据实际情况灵活配置。主要方式是在创建 Network 时候配置其 IO 属性， "
"下面的 example 是指定模型名字为 \"data\" 的 Tensor 的内存为 CUDA 设备上内存，输出名字为 \"TRUE_DIV\""
" 的 Tensor 数据 保存在 CUDA 设备上。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:94
msgid "上面分别是 C++ 和 Python 使用 MegEngine Lite 配置 IO 为 device 上输入输出的示例，C++ 主要的配置为："
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:119
msgid ""
"Network 的 IO 中 input 名字为 \"data\" 和 output 名字为 \"TRUE_DIV\" 的 IO 的 "
"is_host 属性为 false，host 默认指 CPU 端， 为 flase 则表述输入或者输出的内存为设备端。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:123
msgid "输入输出零拷贝"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:125
msgid ""
"输入输出零拷贝，指用户的输入数据可以不用拷贝到 MegEngine Lite 中，模型推理完成的输出数据可以直接写到用户指定的内存中， "
"减少将输出数据拷贝到用户的内存中的过程，用户的内存 MegEngine Lite 不会进行管理，用户需要确保 "
"**内存的生命周期大于模型推理的生命周期**。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:130
msgid "force_output_use_user_specified_memory 参数目前只在 CPU 测试通过使用，其他设备上没有充分的进行测试。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:157
msgid "实现这个功能主要为两步："
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:159
msgid "设置 force_output_use_user_specified_memory 为 True。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:160
msgid ""
"模型运行之前通过 LiteTensor 的 reset 接口设置设置自己管理的内存到输入输出 Tensor 中，在 python 中可以调用 "
"set_data_by_share 达到相同的功能。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:164
msgid ""
"使用 force_output_use_user_specified_memory 这个参数时，只能获取模型计算的输出 Tensor "
"的结果，获取中间 Tensor 的计算结果是不被允许的。"
msgstr ""

