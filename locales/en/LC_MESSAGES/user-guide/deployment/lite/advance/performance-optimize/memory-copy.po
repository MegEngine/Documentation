# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2021, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.7\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-14 16:12+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:5
msgid "输入输出内存拷贝优化"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:7
msgid ""
"MegEngine Lite 中的内存拷贝优化主要指输入输出 Tensor "
"内存拷贝的优化，模型内部固有的内存拷贝优化不能够被优化，主要有下面几种情况："
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:9
msgid ""
"device IO 优化：输入数据本来就不是 CPU 端的内存，如：是一段 CUDA 的内存或者一段 OpenCL "
"的内存，希望模型推理直接使用这段内存作为输入，避免将其拷贝到 CPU 端，然后再在模型内部从 CPU 拷贝到设备上，节省两次内存拷贝。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:11
msgid ""
"输入输出零拷贝：希望模型的推理结果保存在用户提供的内存中，避免将数据保存在 MegEngine "
"自己申请的内存中，然后再将内存拷贝到用户指定的内存中。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:16
msgid "Device IO 优化"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:18
msgid ""
"MegEngine Lite 支持模型的输入输出配置，用户可以根据实际情况灵活配置。主要方式是在创建 Network 时候配置其 IO 属性， "
"下面的 example 是指定模型名字为 \"data\" 的 Tensor 的内存为 CUDA 设备上内存，输出名字为 \"TRUE_DIV\""
" 的 Tensor 数据 保存在 CUDA 设备上。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:96
msgid "上面分别是 C++ 和 Python 使用 MegEngine Lite 配置 IO 为 device 上输入输出的示例，C++ 主要的配置为："
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:121
msgid ""
"Network 的 IO 中 input 名字为 \"data\" 和 output 名字为 \"TRUE_DIV\" 的 IO 的 "
"is_host 属性为 false，host 默认指 CPU 端， 为 flase 则表述输入或者输出的内存为设备端。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:125
msgid "输入输出拷贝优化"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:130
msgid "Device 上输入输出优化"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:132
msgid ""
"Device 上进行模型推理除了 :ref:`device-io-optimize` 的情况外，都需要将输入从 CPU 拷贝到 Device "
"上，然后执行模型推理，执行完成之后，将 输出数据拷贝到 CPU 上，这是在 Device 上执行推理不可缺少的情况，(除了 :ref"
":`device-io-optimize` )。但是我们可以优化输入从真实数据拷贝 到模型的 CPU 输入数据和输出从 CPU "
"再拷贝到用户指定的内存中这些内存拷贝操作。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:195
msgid ""
"该优化主要是使用 LiteTensor 的 reset 或者 memory share 的接口，将用户的内存共享到 Network 中的输入输出 "
"LiteTensor 中。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:198
msgid "CPU 上输入输出零拷贝"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:200
msgid ""
"输入输出零拷贝，指用户的输入数据可以不用拷贝到 MegEngine Lite 中，模型推理完成的输出数据可以直接写到用户指定的内存中， "
"减少将输出数据拷贝到用户的内存中的过程，用户的内存 MegEngine Lite 不会进行管理，用户需要确保 "
"**内存的生命周期大于模型推理的生命周期**。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:203
msgid ""
"实现这个功能主要将上面 :ref:`device-io-memcopy-optimize` 优化中配置 network 时，使能 "
"force_output_use_user_specified_memory 选项："
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:205
msgid "设置 force_output_use_user_specified_memory 为 True。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:206
msgid ""
"模型运行之前通过 LiteTensor 的 reset 接口设置设置自己管理的内存到输入输出 Tensor 中，在 python 中可以调用 "
"set_data_by_share 达到相同的功能。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:210
msgid ""
"使用 force_output_use_user_specified_memory 这个参数时，只能获取模型计算的输出 Tensor "
"的结果，获取中间 Tensor 的计算结果是不被允许的。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:211
msgid "模型必须是静态模型，输出 LiteTensor 的 layout 需要在模型载入之后就能够被推导出来。"
msgstr ""

#: ../../source/user-guide/deployment/lite/advance/performance-optimize/memory-copy.rst:212
msgid "force_output_use_user_specified_memory 参数目前只在 CPU 使用，其他 Device 上不能使用。"
msgstr ""

#~ msgid "输入输出零拷贝"
#~ msgstr ""

#~ msgid ""
#~ "force_output_use_user_specified_memory 参数目前只在 CPU "
#~ "测试通过使用，其他设备上没有充分的进行测试。"
#~ msgstr ""

#~ msgid "实现这个功能主要为两步："
#~ msgstr ""

