msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-11-08 21:51+0800\n"
"PO-Revision-Date: 2023-04-21 09:37\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /dev/locales/en/LC_MESSAGES/user-guide/deployment/lite/cpp-basic.po\n"
"X-Crowdin-File-ID: 10003\n"
"Language: en_US\n"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:5
msgid "MegEngine Lite C++ 基础功能介绍"
msgstr "MegEngine Lite C++ basic function introduction"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:9
msgid "在CPU上做推理的一个例子"
msgstr "An example of inference on the CPU"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:10
msgid "接下来，我们将逐步讲解一个使用MegEngine Lite在CPU上做推理的简单例子，即：:src:`basic.cpp <lite/example/mge/basic.cpp>` 中的 ``basic_load_from_path()`` 函数。"
msgstr "Next, we will gradually explain a simple example MegEngine Lite do reasoning on CPU usage, ie：:src:`basic.cpp <lite/example/mge/basic.cpp>` in the `` basic_load_from_path () `` function."

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:14
msgid "1. 一些配置"
msgstr "1. Some configurations"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:16
msgid "首先需要配置Log的级别，**LiteLogLevel** 一共有 **DEBUG**、**INFO**、**WARN** 和 **ERROR** 四个级别，其中 **DEBUG** 级别会把最细致的信息作为log显示在屏幕。此外，模型文件和输入数据的路径也要设定。对应代码如下："
msgstr "First, you need to configure the level of Log. **LiteLogLevel** has four levels: **DEBUG**, **INFO**, **WARN** and **ERROR**, of which the **DEBUG** level will change The most detailed information is displayed on the screen as a log. In addition, the path of the model file and input data should also be set. The corresponding code is as follows："

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:26
msgid "2. 加载模型"
msgstr "2. Load the model"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:28
msgid "模型文件将被作为参数，传给 **Network** 类的 ``load_model`` 方法。**Network** 类包含了加载、初始化、推理模型和显示模型信息的功能。详情请参考 :src:`network.h <lite/include/lite/network.h>`"
msgstr "The model file will be passed as a parameter to the ``load_model`` method of the **Network** class. The **Network** class contains the functions of loading, initializing, inferring models and displaying model information. For details, please refer to :src:`network.h <lite/include/lite/network.h>`"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:37
msgid "3. 加载输入数据"
msgstr "3. Load input data"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:39
msgid "加载前，先要构建 **Tensor** 实体，需要注意与之相关的 **LiteBackend** 、 **LiteDeviceType** Enum和 **Layout** 类。 **Layout** 类的实体是 **Tensor** 类的成员之一，用于描述 **Tensor** 的维度和数据类型，而 **LiteBackend** 、 **LiteDeviceType** 的实现如下："
msgstr "Before loading, you must first build the **Tensor** entity, and pay attention to the **LiteBackend**, **LiteDeviceType** Enum and **Layout** classes related to it. The entity of the **Layout** class is one of the members of the **Tensor** class, used to describe the dimension and data type of **Tensor**, and the implementation of **LiteBackend** and **LiteDeviceType** is as follows："

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:61
msgid "本例中，输入数据是从某npy文件加载进来的，具体的加载方式详见 :src:`main.cpp <lite/example/main.cpp>` 中关于 ``parse_npy()`` 函数的实现。本例通过 ``parse_npy()`` 加载数据并构建src_tensor。然后把src_tensor的数据拷贝到network的输入tensor中。"
msgstr "In this example, the input data is loaded from a npy file. For the specific loading method, see the implementation of the ``parse_npy()`` function in :src:`main.cpp <lite/example/main.cpp>This example uses ``parse_npy()'' to load data and build src_tensor. Then copy the data of src_tensor to the input tensor of the network."

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:86
msgid "4. 推理"
msgstr "4. Reasoning"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:88
msgid "网络的推理是通过调用 **Network** 的 ``forward()`` 方法和 ``wait()`` 方法完成的。如果想记录运行时间，可以使用 **lite\\:\\:Timer** 。本例中相关代码如下："
msgstr "The reasoning of the network is done by calling the ``forward()`` method and the ``wait()`` method of **Network**. If you want to record the running time, you can use **lite\\:\\:Timer**. The relevant code in this example is as follows："

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:108
msgid "5. 获取输出数据"
msgstr "5. Get output data"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:110
msgid "推理完成后，网络的输出数据可通过 **Network** 的 ``get_output_tensor()`` 函数获取。具体用法可参看 :src:`basic.cpp <lite/example/mge/basic.cpp>` 中的 ``output_info()`` 函数代码。"
msgstr "After the inference is completed, the output data of the network can be obtained through the ``get_output_tensor()'' function of **Network**. For specific usage, please refer to the ``output_info()`` function code in :src:`basic.cpp <lite/example/mge/basic.cpp>"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:115
msgid "对于在N卡设备上的推理"
msgstr "For reasoning on N card devices"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:117
msgid "如果用N卡设备做推理，需要在上面例子的基础上稍作修改：把输入Tensor需要构造为 **LiteDeviceType\\:\\:LITE_CUDA** 类型。即 ``load_from_path_run_cuda()`` 函数中的如下部分（完整代码在 :src:`basic.cpp <lite/example/mge/basic.cpp>` 里）："
msgstr "If you use an N card device for inference, you need to make a slight modification on the basis of the above example.：The input Tensor needs to be constructed as a **LiteDeviceType\\:\\:LITE_CUDA** type. I.e. `` load_from_path_run_cuda () `` following section (in the function of the complete code :src:`basic.cpp <lite/example/mge/basic.cpp>` li)："

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:130
msgid "对于支持OpenCL的后端设备上的推理"
msgstr "For inference on back-end devices that support OpenCL"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:132
msgid "在以OpenCL为后端的设备上，有两种加载并推理模型的方式："
msgstr "OpenCL on to the back-end equipment, there are two ways to load and reasoning model："

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:134
msgid "首次推理的同时搜索最优算法并将搜索结果存为文件（ ``load_from_path_use_opencl_tuning()`` 函数）"
msgstr "Search for the optimal algorithm during the first inference and save the search result as a file (``load_from_path_use_opencl_tuning()'' function)"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:136
msgid "以算法搜索结果文件中的算法推理模型（ ``load_from_path_run_opencl_cache_and_policy()`` 函数）。"
msgstr "Search the algorithm inference model in the result file by algorithm (the ``load_from_path_run_opencl_cache_and_policy()'' function)."

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:138
msgid "前者首次推理的速度较慢，可以看做是为后者做的准备。后者的运行效率才是更贴近工程应用水平的。两者的详细实现都在文件 :src:`basic.cpp <lite/example/mge/basic.cpp>` 中。"
msgstr "The first reasoning speed of the former is slower, which can be seen as a preparation for the latter. The operating efficiency of the latter is closer to the engineering application level. The detailed implementation of the two documents are :src:`basic.cpp <lite/example/mge/basic.cpp>` in."

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:143
msgid "用异步执行模式进行推理"
msgstr "Use asynchronous execution mode for reasoning"

#: ../../source/user-guide/deployment/lite/cpp-basic.rst:145
msgid "实现在 :src:`basic.cpp <lite/example/mge/basic.cpp>` 中的 ``async_forward()`` 函数 。用户通过接口注册异步回调函数将设置 Network 的 Forward 模式为异步执行模式，目前异步执行模式只有在 CPU 和 CUDA 10.0 以上才支持，在inference时异步模式，主线程可以在工作线程正在执行计算的同时做一些其他的运算，避免长时间等待，但是在一些单核处理器上没有收益。"
msgstr "Implemented :src:`basic.cpp <lite/example/mge/basic.cpp>` `async_forward ()` `` function of. The user registers the asynchronous callback function through the interface to set the Forward mode of the Network to the asynchronous execution mode. Currently, the asynchronous execution mode is only supported by CPU and CUDA 10.0 and above. In the asynchronous mode during inference, the main thread can do the calculation while the worker thread is performing calculations. Some other operations avoid long waits, but there is no benefit on some single-core processors."

