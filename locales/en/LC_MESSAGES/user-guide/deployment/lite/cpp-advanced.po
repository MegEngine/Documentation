msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-11-08 21:51+0800\n"
"PO-Revision-Date: 2023-04-21 09:37\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /dev/locales/en/LC_MESSAGES/user-guide/deployment/lite/cpp-advanced.po\n"
"X-Crowdin-File-ID: 10001\n"
"Language: en_US\n"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:5
msgid "MegEngine Lite C++ 进阶功能介绍"
msgstr "MegEngine Lite C++ advanced function introduction"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:7
msgid "在 example 目录中，除了 :src:`basic.cpp <lite/example/mge/basic.cpp>` 介绍了一些基本用法之外，还有其他一些例子演示了用 lite 的接口做推理相关更进阶的功能。比如："
msgstr "In the example directory, in addition to :src:`basic.cpp <lite/example/mge/basic.cpp>` introduces some basic usage, there are a few other examples demonstrate the reasoning do more advanced features associated with lite interface. For example："

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:11
msgid "CPU上的模型加载和推理"
msgstr "Model loading and inference on the CPU"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:13
msgid "主要介绍了用 lite 来完成基本的 inference 功能，load 模型使用默认的配置，进行 forward 之前将输入数据 copy 到输入 tensor 中，完成 forward 之后，再将数据从输出 tensor 中 copy 到用户的内存中，输入 tensor 和输出 tensor 都是从Network 中通过 name 来获取的，输入输出 tensor 的 layout 也可以从对应的 tensor中直接获取获取，"
msgstr "It mainly introduces the use of lite to complete the basic inference function. The load model uses the default configuration. Before forwarding, the input data is copied to the input tensor. After the forward is completed, the data is copied from the output tensor to the user's memory, and input Both tensor and output tensor are obtained by name from Network, and the layout of input and output tensor can also be obtained directly from the corresponding tensor."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:17
msgid "输出 tensor 的 layout 必须在 forward 完成之后获取才是正确的。"
msgstr "The layout of the output tensor must be obtained after the forward is completed to be correct."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:19
msgid "着重强调两种加载模型的方式： *通过模型文件加载模型* （ ``basic_load_from_path()`` ）和 *通过内存加载模型* （ ``basic_load_from_memory()`` ），请着重对比两个函数调用 ``network->load_model()`` 时参数的不同。详细实现在文件 :src:`basic.cpp <lite/example/mge/basic.cpp>` 中。"
msgstr "Emphasizes two ways to load the model： *Load the model through the model file* (``basic_load_from_path()``) and *Load the model through the memory* (``basic_load_from_memory()``), please focus on comparing the two function calls` The parameters of `network->load_model()`` are different. Details implemented in the file :src:`basic.cpp <lite/example/mge/basic.cpp>` in."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:23
msgid "OpenCL后端设备上的模型加载和推理"
msgstr "Model loading and inference on OpenCL back-end devices"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:25
msgid "在以OpenCL为后端的设备上，有两种加载并推理模型的方式： *首次推理的同时搜索最优算法并将搜索结果存为文件* （ ``load_from_path_use_opencl_tuning()`` ）和 *以算法搜索结果文件中的算法推理模型* （ ``load_from_path_run_opencl_cache_and_policy()`` ）。前者首次推理的速度较慢，可以看做是为后者做的准备。后者的运行效率才是更贴近工程应用水平的。详细实现在文件 :src:`basic.cpp <lite/example/mge/basic.cpp>` 中。"
msgstr "On devices with OpenCL as the backend, there are two ways to load and infer the model.： *Search for the optimal algorithm during the first inference and save the search result as a file* (``load_from_path_use_opencl_tuning()``) and *Search by algorithm Algorithmic inference model in the result file* (``load_from_path_run_opencl_cache_and_policy()``). The first reasoning speed of the former is slower, which can be seen as a preparation for the latter. The operating efficiency of the latter is closer to the engineering application level. Details implemented in the file :src:`basic.cpp <lite/example/mge/basic.cpp>` in."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:29
msgid "指定输入输出的内存"
msgstr "Specify input and output memory"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:32
msgid "在以CPU为后端的设备上"
msgstr "On a device with a CPU as the backend"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:34
msgid "实现在 :src:`reset_io.cpp <lite/example/mge/reset_io.cpp>` 中，包括 ``reset_input()`` 和 ``reset_input_output()`` 两个函数。"
msgstr "Implemented :src:`reset_io.cpp <lite/example/mge/reset_io.cpp>`, including `` reset_input () `` and `` reset_input_output () `` two functions."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:36
msgid "两个函数演示了输入 tensor 的内存为用户指定的内存（该内存中已经保存好输入数据），输出 tensor 也可以是用户指定的内存，这样 Network 完成 Forward 之后就会将数据保存在指定的输出内存中。如此减少不必要的 memory copy 的操作。"
msgstr "The two functions demonstrate that the memory of the input tensor is the memory specified by the user (the input data has been stored in the memory), and the output tensor can also be the memory specified by the user, so that the data will be saved in the specified output memory after the Network completes the Forward. middle. This reduces unnecessary memory copy operations."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:38
msgid "tensor 中的 reset 接口可以重新指定 tensor 的内存和对应的layout，如果 layout 没有指定，默认为 tensor 中原来的 layout。"
msgstr "The reset interface in tensor can re-specify the memory of the tensor and the corresponding layout. If the layout is not specified, it will default to the original layout in the tensor."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:42
msgid "该方法中由于内存是用户申请，需要用户提前知道输入，输出 tensor 对应的 layout，然后根据 layout 来申请内存。另外，通过 reset 设置到 tensor 中的内存的生命周期不由 tensor管理，而是由外部用户来管理。"
msgstr "In this method, because the memory is requested by the user, the user needs to know the input in advance, output the layout corresponding to the tensor, and then apply for the memory according to the layout. In addition, the life cycle of the memory set to tensor through reset is not managed by tensor, but by external users."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:46
msgid "在N卡设备上"
msgstr "On N card device"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:48
msgid "**指定输入输出的显存**"
msgstr "**Specify input and output video memory**"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:50
msgid "实现在 :src:`device_io.cpp <lite/example/mge/device_io.cpp>` 中的 ``device_input()`` 和 ``device_input_output()`` 两个函数。示例中，模型运行在 device(CUDA) 上，并且使用用户提前申请的 device 上的内存作为模型运行的输入和输出。这需要在 Network 构建的时候指定输入输出的在 device 上（如不设置，则默认在 CPU 上），其他地方和**输入输出为用户指定的内存**的使用相同。可以通过 tensor 的 ``is_host()`` 接口来判断该 tensor 在 device 端还是 host 端。"
msgstr "Implemented :src:`device_io.cpp <lite/example/mge/device_io.cpp>` the `` device_input () `` and `` device_input_output () `` two functions. In the example, the model runs on the device (CUDA), and uses the memory on the device that the user applies for in advance as the input and output of the model. This needs to specify the input and output on the device when the network is built (if not set, the default is on the CPU), and the other places are the same as the use of **input and output for user-specified memory**. The tensor's ``is_host()'' interface can be used to determine whether the tensor is on the device side or the host side."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:53
msgid "**申请 pinned host 内存作为输入**"
msgstr "**Apply for pinned host memory as input**"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:55
msgid "实现在 :src:`device_io.cpp <lite/example/mge/device_io.cpp>` 中的 ``pinned_host_input()`` 。示例中的模型运行在 device(CUDA) 上，但是输入输出在 CPU 上，为了加速 host2device 的copy，将 CPU 上的 input tensor 的内存指定提前申请为 cuda pinned 内存。目前如果输出output tensor 不是 device 上的时候，默认就是 pinned host 的。申请 pinned host 内存的方法是：构建 tensor 的时候指定 device，layout，以及 is_host_pinned参数，这样申请的内存就是 pinned host 的内存。"
msgstr "Implemented :src:`device_io.cpp <lite/example/mge/device_io.cpp>` `` pinned_host_input in () ``. The model in the example runs on the device (CUDA), but the input and output are on the CPU. In order to speed up the copy of the host2device, the memory of the input tensor on the CPU is designated as cuda pinned memory in advance. At present, if the output tensor is not on the device, the default is pinned host. The method of application pinned host memory is：building tensor when the designated device, layout, and is_host_pinned parameters, such application memory is pinned host memory."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:65
msgid "用户指定内存分配器"
msgstr "User-specified memory allocator"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:67
msgid "实现在 :src:`user_allocator.cpp <lite/example/mge/user_allocator.cpp>` 中的 ``config_user_allocator()`` 。"
msgstr "Implemented in ``config_user_allocator()`` :src:`user_allocator.cpp <lite/example/mge/user_allocator.cpp>"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:69
msgid "这个例子中使用用户自定义的 CPU 内存分配器演示了用户设置自定义的 Allocator 的方法，用户自定义内存分配器需要继承自 lite 中的 Allocator 基类，并实现 allocate 和 free 两个接口。目前在 CPU上验证是正确的，其他设备上有待测试。"
msgstr "In this example, a user-defined CPU memory allocator is used to demonstrate the method of setting a user-defined Allocator. The user-defined memory allocator needs to inherit from the Allocator base class in lite and implement the allocate and free interfaces. At present, the verification is correct on the CPU, and it needs to be tested on other devices."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:71
msgid "设置自定定义内存分配器的接口为 Network 中如下接口："
msgstr "Set the interface of the custom memory allocator to the following interface in Network："

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:79
msgid "多个 Network 共享同一份模型 weights"
msgstr "Multiple Networks share the same model weights"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:81
msgid "实现在 :src:`network_share_weights.cpp <lite/example/mge/network_share_weights.cpp>` 中的 ``network_share_same_weights()`` 。"
msgstr "Implemented in ``network_share_same_weights()`` :src:`network_share_weights.cpp <lite/example/mge/network_share_weights.cpp>"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:83
msgid "很多情况用户希望多个 Network 共享同一份 weights，因为模型中 weights 是只读的，这样可以节省模型的运行时内存使用量。这个例子主要演示了 lite 中如何实现这个功能，首先创建一个新的 Network，用户可以指定新的 Config 和 NetworkIO 以及其他一些配置，使得新创建出来的 Network 完成不同的功能。"
msgstr "In many cases, users want multiple networks to share the same weights, because the weights in the model are read-only, which can save the memory usage of the model at runtime. This example mainly demonstrates how to implement this function in lite. First, create a new Network. The user can specify the new Config and NetworkIO and some other configurations, so that the newly created Network can perform different functions."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:85
msgid "通过已有的 NetWork load 一个新的 Network 的接口为 Network 中如下接口："
msgstr "The interface of a new Network through the existing NetWork load is the following interface in the Network："

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:94
msgid "**dst_network** 指新 load 出来的Network。**src_network** 指已经 load 的旧的 Network。"
msgstr "**dst_network** refers to the newly loaded Network. **src_network** refers to the old network that has been loaded."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:98
msgid "CPU 绑核"
msgstr "CPU core"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:100
msgid "实现在 :src:`cpu_affinity.cpp <lite/example/mge/cpu_affinity.cpp>` 中的 ``cpu_affinity()`` 。"
msgstr "Implemented :src:`cpu_affinity.cpp <lite/example/mge/cpu_affinity.cpp>` `` cpu_affinity in () ``."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:102
msgid "该 example 之中指定模型运行在 CPU 多线程上，然后使用 Network 中的 ``set_runtime_thread_affinity()`` 来设置绑核回调函数。该回调函数中会传递当前线程的 id 进来，用户可以根据该 id 决定具体绑核行为，在多线程中，如果线程总数为 n，则 id 为 n-1 的线程为主线程。"
msgstr "In this example, the model is specified to run on the CPU multi-thread, and then the ``set_runtime_thread_affinity()'' in the Network is used to set the core binding callback function. The callback function will pass in the id of the current thread, and the user can determine the specific core binding behavior according to the id. In multithreading, if the total number of threads is n, the thread with id n-1 is the main thread."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:106
msgid "用户注册自定义解密算法和 key"
msgstr "User registration custom decryption algorithm and key"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:108
msgid "实现在 :src:`user_cryption.cpp <lite/example/mge/user_cryption.cpp>` 中的 ``register_cryption_method()`` 和 ``update_aes_key()`` 。"
msgstr "Implemented :src:`user_cryption.cpp <lite/example/mge/user_cryption.cpp>` `` register_cryption_method in () and `` `` update_aes_key () ``."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:110
msgid "这两个示例主要使用 lite 自定义解密算法和更新解密算法的接口，实现了使用用户自定的解密算法实现模型的 load 操作。在这个 example 中，自定义了一个解密方法，(其实没有做任何事情，将模型两次异或上 key 之后返回，等于将原始模型直接返回)，然后将其注册到 lite 中，后面创建 Network 时候在其config中的bare_model_cryption_name指定具体的解密算法名字。在第二个 example 展示了对其key 的更新操作。目前 lite 里面定义好了几种解密算法："
msgstr "These two examples mainly use the lite custom decryption algorithm and the interface to update the decryption algorithm to realize the load operation of the model using the user-defined decryption algorithm. In this example, a decryption method is customized (actually, nothing is done, and the model is XORed twice and then returned, which is equivalent to returning the original model directly), and then registered in lite, and then when creating a network The bare_model_cryption_name in its config specifies the name of the specific decryption algorithm. The second example shows the update operation of its key. Currently lite which defined the several decryption algorithm："

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:112
msgid "**AES_default** : 其 key 是由 32 个 unsighed char 组成，默认为0到31"
msgstr "**AES_default**: its key is composed of 32 unsighed chars, the default is 0 to 31"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:113
msgid "**RC4_default** : 其 key 由 hash key 和 enc_key 组成的8个 unsigned char，hash key 在前，enc_key 在后。"
msgstr "**RC4_default**: The key consists of 8 unsigned chars consisting of hash key and enc_key, with hash key in the front and enc_key in the back."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:115
msgid "**SIMPLE_FAST_RC4_default**: 其key组成同RC4_default。大概命名规则为：前面大写是具体算法的名字，'_'后面的小写，代表解密 key。具体的接口为："
msgstr "**SIMPLE_FAST_RC4_default**: Its key composition is the same as RC4_default. The approximate naming rule is that the：is the name of the specific algorithm, and the lowercase after the'_' represents the decryption key. The specific interface is："

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:126
msgid "register 接口中必须要求三个参数都是正确的值，update中 decrypt_nam 必须为已有的解密算法， 将使用 func 和 key 中不为空的部分对 decrypt_nam 解密算法进行更新"
msgstr "The register interface must require all three parameters to be correct values. The decrypt_nam in update must be an existing decryption algorithm, and the non-empty part of func and key will be used to update the decrypt_nam decryption algorithm"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:131
msgid "异步执行模式"
msgstr "Asynchronous execution mode"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:133
msgid "实现在 :src:`basic.cpp <lite/example/mge/basic.cpp>` 中的 ``async_forward()`` 。"
msgstr "Implemented in :src:`basic.cpp <lite/example/mge/basic.cpp>```async_forward()``."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:135
msgid "用户通过接口注册异步回调函数将设置 Network 的 Forward 模式为异步执行模式，目前异步执行模式只有在 CPU 和 CUDA 10.0 以上才支持，在inference时异步模式，主线程可以在工作线程正在执行计算的同时做一些其他的运算，避免长时间等待，但是在一些单核处理器上没有收益。"
msgstr "The user registers the asynchronous callback function through the interface to set the Forward mode of the Network to the asynchronous execution mode. Currently, the asynchronous execution mode is only supported by CPU and CUDA 10.0 and above. In the asynchronous mode during inference, the main thread can do the calculation while the worker thread is performing calculations. Some other operations avoid long waits, but there is no benefit on some single-core processors."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:139
msgid "纯 C example"
msgstr "Pure C example"

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:141
msgid "实现在 :src:`lite_c_interface.cpp <lite/example/mge/lite_c_interface.cpp>` 中的 ``basic_c_interface()``， ``device_io_c_interface()`` 和 ``async_c_interface()`` 。"
msgstr "Implemented :src:`lite_c_interface.cpp <lite/example/mge/lite_c_interface.cpp>` the `` basic_c_interface () ``, `` device_io_c_interface () `` and `` async_c_interface () ``."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:143
msgid "Lite 完成对 C++ 接口的封装，对外暴露了纯 C 的接口，用户如果不是源码依赖 Lite 的情况下，应该使用纯 C 接口来完成集成。"
msgstr "Lite completes the encapsulation of the C++ interface and exposes the pure C interface to the outside. If the user does not rely on Lite for the source code, the pure C interface should be used to complete the integration."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:144
msgid "纯 C 的所有接口都是返回一个 int，如果这个 int 的数值不为 0，则又错误产生，需要调用 ``LITE_get_last_error`` 来获取错误信息。"
msgstr "All interfaces of pure C return an int. If the value of this int is not 0, an error occurs. You need to call ``LITE_get_last_error`` to get the error information."

#: ../../source/user-guide/deployment/lite/cpp-advanced.rst:145
msgid "纯 C 的所有 get 函数都需要先定义一个对应的对象，然后将该对象的指针传递进接口，Lite 会将结果写入到 对应指针的地址里面。"
msgstr "All get functions in pure C need to define a corresponding object first, and then pass the pointer of the object into the interface, and Lite will write the result to the address of the corresponding pointer."

