msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-28 09:14+0000\n"
"PO-Revision-Date: 2023-04-21 09:37\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /dev/locales/en/LC_MESSAGES/user-guide/deployment/lite/index.po\n"
"X-Crowdin-File-ID: 10007\n"

#: ../../source/user-guide/deployment/lite/index.rst:5
msgid "使用 MegEngine Lite 部署模型"
msgstr "Deploy the model with MegEngine Lite"

#: ../../source/user-guide/deployment/lite/index.rst:8
msgid "简介"
msgstr "Introduction"

#: ../../source/user-guide/deployment/lite/index.rst:10
msgid "MegEngine Lite 是 MegEngine 的一层接口封装，主要目的是为用户提供更加简洁、易用、高效的推理接口，充分发挥 MegEngine 的多平台的推理能力，其结构如下:"
msgstr "MegEngine Lite is a layer of interface encapsulation of MegEngine. The main purpose is to provide users with a more concise, easy-to-use and efficient reasoning interface, and give full play to the multi-platform reasoning capabilities of MegEngine. Its structure is as follows:"

#: ../../source/user-guide/deployment/lite/index.rst:15
msgid "MegEngine Lite 主要是对训推一体的 MegEngine 框架进行一层很薄的封装，并对用户提供多种模型推理接口，包括： C++ / C / Python 接口，同时 MegEngine Lite 底层也可以接入其它的推理框架，以及其他的 NPU 支持。 相比较直接调用 MegEngine 的接口进行推理，使用 MegEngine Lite 的接口有使用方便、接口简单、功能齐全等优点， 其底层实现依然是 MegEngine, 因此继承了 MegEngine 的所有优点，MegEngine 在推理层面具有以下特点："
msgstr "MegEngine Lite mainly encapsulates the MegEngine framework integrated with training and pushing in a thin layer, and provides users with a variety of model inference interfaces, including： C++ / C / Python interfaces, and the bottom layer of MegEngine Lite can also access other inference frameworks , and other NPU support. Compared with directly calling the interface of MegEngine for reasoning, the interface using MegEngine Lite has the advantages of convenient use, simple interface and complete functions. Its underlying implementation is still MegEngine, so it inherits all the advantages of MegEngine. MegEngine has the following characteristics at the inference level："

#: ../../source/user-guide/deployment/lite/index.rst:21
msgid "高性能"
msgstr "high performance"

#: ../../source/user-guide/deployment/lite/index.rst:23
msgid "MegEngine 首先在计算图中对 Inference 做了很多高效的优化，例如： 将 BN 融合到 Convolution 中，将 Activation 融合到 Convolution 中等... 这些优化能有效地减少访存，提高计算访存比。 另外 MegEngine 还对底层的 Kernel 做了细粒度的优化，从算法到指令都进行深入优化， 卷积算法层面 Convolution 就有直接卷积，Im2col, Winograd 等优化， 在 Kernel 层面有粗粒度的 Intrinsic 级别的优化，在一些关键的算子会进行汇编，深入指令集优化。"
msgstr "MegEngine first made a lot of efficient optimizations on Inference in the calculation graph, such as： fused BN into Convolution, Activation into Convolution, etc. These optimizations can effectively reduce memory access and improve the computing memory access ratio. In addition, MegEngine has also made fine-grained optimization to the underlying Kernel, in-depth optimization from algorithms to instructions. Convolution at the convolution algorithm level includes direct convolution, Im2col, Winograd and other optimizations. At the Kernel level, there are coarse-grained Intrinsic level Optimization, in some key operators will be assembled, in-depth instruction set optimization."

#: ../../source/user-guide/deployment/lite/index.rst:30
msgid "多平台支持"
msgstr "Multi-platform support"

#: ../../source/user-guide/deployment/lite/index.rst:32
msgid "MegEngine 支持多种主流深度学习推理平台，包括 Arm，X86，Cuda，Rocm，Atlas，Cambricom 等平台，另外 MegEngine Lite 还支持以 RuntimeOpr/Loader 的形式接入第三方推理框架以及NPU。"
msgstr "MegEngine supports a variety of mainstream deep learning inference platforms, including Arm, X86, Cuda, Rocm, Atlas, Cambricom and other platforms. In addition, MegEngine Lite also supports access to third-party inference frameworks and NPUs in the form of RuntimeOpr/Loader."

#: ../../source/user-guide/deployment/lite/index.rst:35
msgid "高精度"
msgstr "High precision"

#: ../../source/user-guide/deployment/lite/index.rst:37
msgid "使用 MegEngine 训练的模型可以不需要进行任何模型转换，就可以直接进行推理， 有效地避免由于模型转换以及量化等带来的模型精度的损失，降低了模型部署的难度。"
msgstr "Models trained with MegEngine can be directly inferred without any model conversion, which effectively avoids the loss of model accuracy due to model conversion and quantization, and reduces the difficulty of model deployment."

#: ../../source/user-guide/deployment/lite/index.rst:41
msgid "接下来：快速上手"
msgstr "Next：get started quickly"

#~ msgid "MegEngine Lite 是对 MegEngine 的推理功能的封装，具有配置统一、接口完备、化繁为简、支持第三方硬件等优点。"
#~ msgstr ""
#~ "MegEngine Lite is an encapsulation of"
#~ " the reasoning function of MegEngine. "
#~ "It has the advantages of unified "
#~ "configuration, complete interface, simplified "
#~ "complexity, and third-party hardware "
#~ "support."

#~ msgid "**配置统一** ：配置选项可以存为json文件，在模型制作阶段和模型调用阶段均可加载，获得同一份配置选项。"
#~ msgstr ""
#~ "**Configuration unified** ：configuration options "
#~ "can be saved as a json file, "
#~ "which can be loaded during the "
#~ "model making stage and the model "
#~ "calling stage, and the same "
#~ "configuration options can be obtained."

#~ msgid "**接口完备** ：从业务需要出发，涵盖了推理和部署所涉及的网络和张量的接口。"
#~ msgstr ""
#~ "**Complete interface** ：Starting from business"
#~ " needs, it covers the network and "
#~ "tensor interfaces involved in reasoning "
#~ "and deployment."

#~ msgid ""
#~ "**化繁为简** "
#~ "：lite所封装的接口，是MegEngine接口中只和推理部署相关的部分。对于只对推理部署、而非训练测试感兴趣的开发者，学习和上手的成本降到最低。"
#~ msgstr ""
#~ "** ：The interface encapsulated by lite"
#~ " is the part of the MegEngine "
#~ "interface that is only related to "
#~ "inference deployment. For developers who "
#~ "are only interested in inference "
#~ "deployment, not training and testing, "
#~ "the cost of learning and getting "
#~ "started is minimized."

#~ msgid ""
#~ "**支持第三方硬件** "
#~ "：lite里提供了新硬件接入的方案：新硬件接口被封装后，以“插件”的方式注册在lite上，而弱化对lite或MegEngine版本号的依赖。"
#~ msgstr ""
#~ "**Support third-party hardware** ：A new"
#~ " hardware access solution is provided "
#~ "in lite.：new hardware interface is "
#~ "encapsulated, it is registered on the"
#~ " lite as a \"plug-in\", and the"
#~ " dependence on the version number of"
#~ " lite or MegEngine is weakened."

#~ msgid "编译"
#~ msgstr "Compile"

#~ msgid "将 Lite 结合到整个生产流程，步骤如下："
#~ msgstr "Integrate Lite into the entire production process, the steps are as："

#~ msgid "模型准备：模型开发完成后，你需要通过 :ref:`dump <dump>` 得到 Lite 支持的模型格式；"
#~ msgstr ""
#~ "Model preparation：after model development is"
#~ " complete, you need to pass "
#~ ":ref:`dump <dump>model format` get Lite "
#~ "support;"

#~ msgid ""
#~ "环境准备：按照使用环境，:ref:`从源码编译 MegEngine <build-from-"
#~ "source>` . （ ``MGE_WITH_LITE`` 变量默认为 "
#~ "``ON`` ）；"
#~ msgstr ""
#~ "Environment preparation：According to the use"
#~ " environment,:ref: <build-from-source>`from "
#~ "source code. (``MGE_WITH_LITE`` variable "
#~ "defaults to ``ON``);"

#~ msgid "工程接入：调用 MegEngine Lite 提供的接口，实现接入；"
#~ msgstr ""
#~ "Engineering access：calls the interface "
#~ "provided by MegEngine Lite to achieve"
#~ " access;"

#~ msgid "集成编译：将之前编译得到的 Lite 库文件和对应调用代码一同编译成 SDK."
#~ msgstr ""
#~ "Integrated compilation：compiles the previously "
#~ "compiled Lite library files and "
#~ "corresponding calling codes into SDK."

#~ msgid "文档目录"
#~ msgstr "Document directory"

