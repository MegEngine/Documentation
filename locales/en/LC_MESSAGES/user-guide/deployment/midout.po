msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-15 19:44+0800\n"
"PO-Revision-Date: 2021-06-03 10:21\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/deployment/midout.po\n"
"X-Crowdin-File-ID: 2864\n"
"Language: en_US\n"

#: ../../source/user-guide/deployment/midout.rst:5
msgid "减少二进制文件体积"
msgstr "Reduce the size of binary files"

#: ../../source/user-guide/deployment/midout.rst:7
msgid "midout 是 MegEngine 中用来减小生成的二进制文件体积的工具，有助于在空间受限的设备上部署应用。 midout 通过记录模型推理时用到的 opr 和执行流，使用 ``if(0)`` 关闭未被记录的代码段后重新编译， 利用 ``-flto`` 链接参数，可以大幅度减少静态链接的可执行文件的大小。 现在基于 MegEngine 提供模型验证工具 :ref:`Load and Run <load-and-run>` , 展示怎样在某 AArch64 架构的 Android 端上裁剪 MegEngine 库。"
msgstr "Midout is a tool used in MegEngine to reduce the size of the generated binary files, which helps to deploy applications on space-constrained devices. Midout records the opr and execution flow used during model inference, uses ``if(0)'' to close the unrecorded code segment and recompiles, and uses ``-flto`` link parameter to greatly reduce static linking The size of the executable file. Now based MegEngine provide model validation tools :ref:`the Load and the Run <load-and-run>`, to show how a AArch64 Android architecture end cut MegEngine library."

#: ../../source/user-guide/deployment/midout.rst:14
msgid "编译静态链接的 load_and_run"
msgstr "Compile statically linked load_and_run"

#: ../../source/user-guide/deployment/midout.rst:16
msgid "端上裁剪 MegEngine 库需要一个静态连接 MegEngine 的可执行程序，编译方法详见 load-and-run 的编译部分。 稍有不同的是编译时需要先设置 load_and_run 静态链接 MegEngine."
msgstr "Tailoring the MegEngine library on the end requires an executable program that is statically connected to the MegEngine. For the compilation method, see the compilation part of load-and-run. The slight difference is that you need to set load_and_run to statically link MegEngine when compiling."

#: ../../source/user-guide/deployment/midout.rst:23
msgid "否则，MegEngine 会自动编译成动态库。然后执行："
msgstr "Otherwise, MegEngine will be automatically compiled into a dynamic library. Then execute："

#: ../../source/user-guide/deployment/midout.rst:29
msgid "查看一下 load_and_run 的大小："
msgstr "Check the size of load_and_run："

#: ../../source/user-guide/deployment/midout.rst:36
msgid "此时 load_and_run 大小超过 20MB. load_and_run 的执行，请参考下文“代码执行”部分。"
msgstr "At this time, the size of load_and_run exceeds 20MB. For the execution of load_and_run, please refer to the \"Code Execution\" section below."

#: ../../source/user-guide/deployment/midout.rst:39
msgid "裁剪 load_and_run"
msgstr "Crop load_and_run"

#: ../../source/user-guide/deployment/midout.rst:41
msgid "MegEngine 的裁剪可以从两方面进行："
msgstr "MegEngine cut two ways can be："

#: ../../source/user-guide/deployment/midout.rst:43
msgid "通过opr 裁剪。在 dump 模型时，可以同时将模型用到的 opr 信息以 json 文件的形式输出， midout 在编译期裁掉没有被模型使用到的所有 opr."
msgstr "Cut through opr. When dumping the model, the opr information used by the model can be output in the form of a json file at the same time, and midout will cut off all opr not used by the model during the compile time."

#: ../../source/user-guide/deployment/midout.rst:45
msgid "通过 trace 流裁剪。运行一次模型推理，根据代码的执行流生成 trace 文件， 通过trace文件，在二次编译时将没有执行的代码段裁剪掉。"
msgstr "Tailor through the trace stream. Run the model inference once, and generate a trace file according to the execution flow of the code. Through the trace file, the code segments that are not executed during the second compilation are cut out."

#: ../../source/user-guide/deployment/midout.rst:48
msgid "整个裁剪过程分为两个步骤："
msgstr "The entire two-step cutting process："

#: ../../source/user-guide/deployment/midout.rst:50
msgid "第一步，dump 模型，获得模型 opr 信息；通过一次推理，获得 trace 文件。"
msgstr "The first step is to dump the model to obtain model opr information; to obtain the trace file through an inference."

#: ../../source/user-guide/deployment/midout.rst:51
msgid "第二步，使用MegEngine的头文件生成工具 ``tools/gen_header_for_bin_reduce.py`` 将 opr 信息和 trace 文件作为输入， 生成 ``bin_reduce.h`` 并将该头文件加入编译 Release 版的应用程序。 当然也可以单独使用模型 opr 信息或是 trace 文件来生成 ``bin_reduce.h`` ， 单独使用 opr 信息时，默认保留所有 kernel，单独使用 trace 文件时，默认保留所有 opr."
msgstr "The second step is to use MegEngine's header file generation tool ``tools/gen_header_for_bin_reduce.py'' to generate ``bin_reduce.h'' with opr information and trace files as input, and add the header file to the compiled Release version of the application. Of course, you can also use the model opr information or the trace file alone to generate ``bin_reduce.h``. When the opr information is used alone, all kernels are reserved by default, and when the trace file is used alone, all oprs are reserved by default."

#: ../../source/user-guide/deployment/midout.rst:57
msgid "dump 模型获得 opr 类型名称"
msgstr "dump model to get the opr type name"

#: ../../source/user-guide/deployment/midout.rst:59
msgid "一个模型通常不会用到所有的opr，根据模型使用的opr，可以裁掉那些模型没有使用的 opr. 在转换模型时，我们可以通过如下方式获得模型的 opr 信息。 使用 ``load_and_run/dump_with_testcase_mge.py`` 准备模型时，加上 ``--output-strip-info`` 参数。"
msgstr "A model usually does not use all opr. According to the opr used by the model, those that are not used by the model can be cut off. When converting the model, we can obtain the opr information of the model in the following way. When using ``load_and_run/dump_with_testcase_mge.py`` to prepare the model, add the ``--output-strip-info`` parameter."

#: ../../source/user-guide/deployment/midout.rst:67
msgid "执行完毕后，会生成 ``resnet50.mge`` 和 ``resnet50.mge.json`` . 查看这个 JSON 文件，它记录了模型用到的 opr 名称。"
msgstr "After the execution is completed, ``resnet50.mge`` and ``resnet50.mge.json`` will be generated. Check this JSON file, it records the opr name used by the model."

#: ../../source/user-guide/deployment/midout.rst:75
msgid "执行模型获得 trace 文件"
msgstr "Execute the model to obtain the trace file"

#: ../../source/user-guide/deployment/midout.rst:77
msgid "基于 trace 的裁剪需要通过一次推理获得模型的执行 trace 文件。具体步骤如下："
msgstr "Tailoring based on trace requires one inference to obtain the execution trace file of the model. The specific steps are as："

#: ../../source/user-guide/deployment/midout.rst:79
msgid "CMake 构建时，打开 ``MGE_WITH_MIDOUT_PROFILE`` 开关，编译 load_and_run："
msgstr "When CMake is building, turn on the ``MGE_WITH_MIDOUT_PROFILE`` switch and compile load_and_run："

#: ../../source/user-guide/deployment/midout.rst:86
msgid "编译完成后，将 ``build_dir/android/arm64-v8a/Release/install/bin`` 下的 ``load_and_run`` 推至设备并执行："
msgstr "After the compilation is complete, push the ``load_and_run'' under ``build_dir/android/arm64-v8a/Release/install/bin'' to the device and execute："

#: ../../source/user-guide/deployment/midout.rst:92
msgid "得到如下输出："
msgstr "Get the following output："

#: ../../source/user-guide/deployment/midout.rst:117
msgid "注意到执行模型后，生成了 ``midout_trace.20717`` 文件，该文件记录了模型在底层执行了哪些 kernel."
msgstr "After noticing that the model is executed, the ``midout_trace.20717'' file is generated, which records which kernels the model has executed at the bottom."

#: ../../source/user-guide/deployment/midout.rst:119
msgid "生成 ``bin_recude.h`` 并再次编译 load_and_run："
msgstr "Generate ``bin_recude.h'' and compile load_and_run："

#: ../../source/user-guide/deployment/midout.rst:121
msgid "将生成的 ``midout_trace.20717`` 拷贝至本地， 使用上文提到的头文件生成工具 ``gen_header_for_bin_reduce.py`` 生成 ``bin_reduce.h`` ."
msgstr "Copy the generated ``midout_trace.20717'' to the local, and use the header file generation tool ``gen_header_for_bin_reduce.py'' mentioned above to generate ``bin_reduce.h``."

#: ../../source/user-guide/deployment/midout.rst:128
msgid "再次编译 load_and_run，注意要将 ``bin_reduce.h`` 加入并编译 Release 版本。设置 CMake 编译选项："
msgstr "Compile load_and_run again, pay attention to adding ``bin_reduce.h`` and compile the Release version. Setting CMake compiler option："

#: ../../source/user-guide/deployment/midout.rst:138
msgid "编译完成后，检查 load_and_run 的大小："
msgstr "After compiling, check the size of load_and_run："

#: ../../source/user-guide/deployment/midout.rst:145
msgid "此时 load_and_run 的大小减小到 2MB 多。推到设备上运行，得到如下输出："
msgstr "At this time, the size of load_and_run is reduced to more than 2MB. Push to the device to run, get the following output："

#: ../../source/user-guide/deployment/midout.rst:170
msgid "可以看到模型依然正常运行，并且运行速度正常。"
msgstr "You can see that the model is still running normally, and the running speed is normal."

#: ../../source/user-guide/deployment/midout.rst:173
msgid "使用裁剪后的 load_and_run"
msgstr "Use cropped load_and_run"

#: ../../source/user-guide/deployment/midout.rst:175
msgid "想要裁剪前后的应用能够正常运行，需要保证裁剪前后两次推理使用同样的命令行参数。 如果使用上文裁剪的 load_and_fun 的 fast-run功能（详见 :ref:`load-and-run` ）。"
msgstr "If you want the application before and after cutting to run normally, you need to ensure that the same command line parameters are used for the two inferences before and after cutting. If you use the fast-run function of load_and_fun cut above (see :ref:`load-and-run` for details)."

#: ../../source/user-guide/deployment/midout.rst:182
msgid "可能得到如下输出："
msgstr "You may get the following output："

#: ../../source/user-guide/deployment/midout.rst:193
msgid "这是因为程序运行到了已经被裁剪掉的函数中，未被记录在 trace 文件中的函数的实现已经被替换成 ``trap()`` . 如果想要裁剪与 fast-run 配合使用，需要按如下流程获得 trace 文件："
msgstr "This is because the program runs into a function that has been trimmed, and the implementation of the function that is not recorded in the trace file has been replaced with ``trap()``. If you want to trim and use fast-run, you need to press the following process to obtain trace files："

#: ../../source/user-guide/deployment/midout.rst:196
msgid "开启 fast-run 模式，执行未裁剪的 load_and_run 获得 ``.cache`` 文件，注意本次执行生成的 trace 应该被丢弃："
msgstr "Turn on fast-run mode, execute uncropped load_and_run to obtain the ``.cache`` file, note that the trace generated by this execution should be discarded："

#: ../../source/user-guide/deployment/midout.rst:202
msgid "使用 ``.cache`` 文件，执行 load_and_run 获得 trace 文件："
msgstr "Use the ``.cache`` file, execute load_and_run to get the trace file："

#: ../../source/user-guide/deployment/midout.rst:208
msgid "如上节，将 trace 文件拷贝回本机，生成 ``bin_reduce.h`` ，再次编译 load_and_run 并推至设备。"
msgstr "As in the previous section, copy the trace file back to the machine, generate ``bin_reduce.h``, compile load_and_run again and push it to the device."

#: ../../source/user-guide/deployment/midout.rst:210
msgid "使用裁剪后的 load_and_run 的 fast-run 功能，执行同 2 的命令，得到如下输出："
msgstr "Use the fast-run function of the cropped load_and_run and execute the same command as 2 to get the following output："

#: ../../source/user-guide/deployment/midout.rst:236
msgid "使用其他 load_and_run 提供的功能也是如此，想要裁剪前后的应用能够正常运行， 需要保证裁剪前后两次推理使用同样的命令行参数。"
msgstr "The same is true for other functions provided by load_and_run. If the application before and after cutting can run normally, it is necessary to ensure that the same command line parameters are used for the two inferences before and after cutting."

#: ../../source/user-guide/deployment/midout.rst:240
msgid "多个模型合并裁剪"
msgstr "Merge and crop multiple models"

#: ../../source/user-guide/deployment/midout.rst:241
msgid "多个模型的合并裁剪与单个模型流程相同。 ``gen_header_for_bin_reduce.py`` 接受多个输入。 假设有模型 A 与模型 B, 已经获得 ``A.mge.json`` , ``B.mge.json`` 以及 ``A.trace`` , ``B.trace`` . 执行："
msgstr "The process of merging and cutting multiple models is the same as that of a single model. ``gen_header_for_bin_reduce.py`` accepts multiple inputs. Suppose there are model A and model B, and have obtained ``A.mge.json``, ``B.mge.json`` and ``A.trace``, ``B.trace``. Go to："

#: ../../source/user-guide/deployment/midout.rst:249
msgid "编译选项"
msgstr "Compilation options"

#: ../../source/user-guide/deployment/midout.rst:251
msgid "MegEngine 的 CMake 中有一些开关是默认打开的，它们提供了 RTTI、异常抛出等特性， 可以在第二次构建时关闭它们，以获得体积更小的 load_and_run. 它们是："
msgstr "There are some switches in MegEngine's CMake that are turned on by default. They provide features such as RTTI and exception throwing. You can turn them off during the second build to get a smaller load_and_run. They are："

#: ../../source/user-guide/deployment/midout.rst:254
msgid "``MGB_WITH_FLATBUFFERS`` : FLABUFFERS格式支持"
msgstr "``MGB_WITH_FLATBUFFERS``: FLABUFFERS format support"

#: ../../source/user-guide/deployment/midout.rst:255
msgid "``MGE_ENABLE_RTTI`` : C++ RTTI特性"
msgstr "``MGE_ENABLE_RTTI``: C++ RTTI features"

#: ../../source/user-guide/deployment/midout.rst:256
msgid "``MGE_ENABLE_LOGGING`` : 日志功能"
msgstr "``MGE_ENABLE_LOGGING``: log function"

#: ../../source/user-guide/deployment/midout.rst:257
msgid "``MGE_ENABLE_EXCEPTIONS`` : 异常功能"
msgstr "``MGE_ENABLE_EXCEPTIONS``: abnormal function"

#: ../../source/user-guide/deployment/midout.rst:259
msgid "MegEngine 提供一个总开关 ``MGE_WITH_MINIMUM_SIZE`` 来关闭上述特性。 需要注意的是，只有在 ``MGE_BIN_REDUCE`` 被设置时，此开关才会被检查并生效。"
msgstr "MegEngine provides a master switch ``MGE_WITH_MINIMUM_SIZE'' to turn off the above features. It should be noted that this switch will be checked and effective only when ``MGE_BIN_REDUCE`` is set."

#: ../../source/user-guide/deployment/midout.rst:263
msgid "裁剪基于 MegEngine 的应用"
msgstr "Tailor MegEngine-based applications"

#: ../../source/user-guide/deployment/midout.rst:265
msgid "可以通过如下几种方式集成 MegEngine，对应的裁剪方法相差无几："
msgstr "MegEngine may be integrated by several ways, corresponding to almost the same method of clipping："

#: ../../source/user-guide/deployment/midout.rst:267
msgid "参照 ``CMakeLists.txt`` ，将应用集成到整个 MegEngine 的工程。 假设已经将 ``app.cpp`` 集成到 MegEngine ，那么会编译出静态链接 MegEngine 的可执行程序 ``app`` . 只需要按照上文中裁剪 load_and_run 的流程裁剪 ``app`` 即可。"
msgstr "Refer to ``CMakeLists.txt`` to integrate the application into the entire MegEngine project. Assuming that ``app.cpp`` has been integrated into MegEngine, then the executable program ``app`` of statically linked MegEngine will be compiled. Just follow the process of cutting load_and_run above to cut ``app``."

#: ../../source/user-guide/deployment/midout.rst:270
msgid "可能一个应用想要通过静态库集成 MegEngine。此时需要获得一个裁剪过的 ``libmegengine.a`` . 可以依然使用 load_and_run 运行模型获得 trace 文件， 生成 ``bin_reduce.h`` ，并二次编译获得裁剪过的 ``libmegengine.a`` . 此时，用户使用自己编写的构建脚本构建应用程序，并静态链接 ``libmegengine.a`` ， 加上链接参数 ``-flto=full`` . 即可得到裁剪过的基于 MegEngine 的应用。"
msgstr "Maybe an application wants to integrate MegEngine through a static library. At this time, you need to obtain a trimmed ``libmegengine.a``. You can still use load_and_run to run the model to obtain the trace file, generate ``bin_reduce.h``, and compile it again to obtain the trimmed ``libmegengine.a`` At this point, the user uses the build script written by himself to build the application, statically link ``libmegengine.a``, and add the link parameter ``-flto=full``. You can get the tailored MegEngine-based application."

#: ../../source/user-guide/deployment/midout.rst:275
msgid "上述流程亦可以用于 ``libmegengine.so`` 的裁剪，但是动态库的裁剪效果远不及静态库。 原因在于动态库并不知道某段代码是否会被调用，因此链接器不会进行激进的优化。"
msgstr "The above process can also be used for the tailoring of ``libmegengine.so``, but the tailoring effect of the dynamic library is far less than that of the static library. The reason is that the dynamic library does not know whether a certain piece of code will be called, so the linker will not perform radical optimizations."

