msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-06-21 15:57+0800\n"
"PO-Revision-Date: 2021-06-21 14:59\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/getting-started/beginner/model-construction.po\n"
"X-Crowdin-File-ID: 7062\n"
"Language: en_US\n"

#: ../../source/getting-started/beginner/model-construction.ipynb:9
msgid "使用 Module 构造模型结构"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:12
msgid "|image0| `在 MegStudio 运行 <https://studio.brainpp.com/project/#>`__"
msgstr "|image0| `Run <https://studio.brainpp.com/project/#>in MegStudio `__"

#: ../../source/getting-started/beginner/model-construction.ipynb:27
msgid "image0"
msgstr "image0"

#: ../../source/getting-started/beginner/model-construction.ipynb:12
msgid "|image1| `查看源文件 <https://github.com/MegEngine/Documentation/blob/main/source/getting-started/beginner/model-construction.ipynb>`__"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:28
msgid "image1"
msgstr "image1"

#: ../../source/getting-started/beginner/model-construction.ipynb:16
msgid "在本次教程中，你将会学习到在实际开发中我们更加常用的 ``module`` 建模方式与工程化技巧，通过实现几个比 LeNet 更复杂的经典 CNN 模型结构，感受一下其优点。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:18
msgid "我们将尝试使用 ``module`` 模块中的实现来构建 LeNet 模型，并灵活地取得内部结构信息，同时完成对 ``Parameters`` 的相关处理；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:19
msgid "我们将接触到 AlexNet, VGGNet 和 GoogLeNet 三种经典的模型结构，并学会一些实用的编码技巧；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:20
msgid "我们还将了解到如何保存和加载我们的模型，有利于在不同的设备上进行训练和迁移；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:21
msgid "与此同时，我们的代码将逐渐变得工程化，这有利于将来我们写出可读性、可维护性更好的代码，也方便我们阅读其它开源的模型代码文件。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:23
msgid "与之前的教程不同，这次的教程需要你花费更多的时间去理解不同的代码，将抽象的概念变为具体的代码实践。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:25
msgid "请先运行下面的代码，检验你的环境中是否已经安装好 MegEngine（\\ `访问官网安装教程 <https://megengine.org.cn/install>`__\\ ）："
msgstr "Please run the following code first to verify whether MegEngine has been installed in your environment (\\ `Visit the official website installation tutorial <https://megengine.org.cn/install>`__\\)："

#: ../../source/getting-started/beginner/model-construction.ipynb:79
msgid "神经网络中的模块（Module）"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:81
msgid "在神经网络模型中，经常存在着许多非常相似的结构，比如全连接的前馈神经网络中，每个层都是线性层。而在卷积神经网络中，卷积层、池化层也非常常见。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:83
msgid "与 ``add()`` 等通常作用于 ``Tensor`` 的 ``functional`` 算子不同，网络层结构中通常存在着需要被管理的参数 ``Parameter``, 包括初始化和更新等操作；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:84
msgid "我们此前一直关注于“单个”的概念，即计算图中的“单个”计算节点/数据节点，神经网络中“单个”神经元；现在我们需要想一想复杂规模下的情景；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:85
msgid "当模型结构变得越来越复杂，如果依旧通过人工的方式来管理参数，依靠 ``funtional`` API 写出完整的结构，代码非常不优雅，且和工程化的理念不符；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:87
msgid "在 MegEngine 中提供了 ``module`` 模块，实现了包括 ``Conv2d`` 等在内的基本结构，可以为层（Layer）的概念提供足够的抽象，方便用户进行模型定义和参数管理。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:89
msgid "我们现在就来改写 LeNet 模型："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:91
msgid "每一个神经网络模型都需要继承自 ``Module`` 基类并且调用\\ ``super().__init__()``;"
msgstr "Every neural network model needs to inherit from the ``Module'' base class and call \\ ``super().__init__()'';"

#: ../../source/getting-started/beginner/model-construction.ipynb:92
msgid "我们需要在 ``__init__`` 方法中定义模块结构，也可以做一些其它处理如初始化权重；"
msgstr "We need to define the module structure in the ``__init__`` method, and we can also do some other processing such as initializing weights;"

#: ../../source/getting-started/beginner/model-construction.ipynb:93
msgid "我们需要定义 ``forward()`` 方法，即前向传播的计算过程；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:94
msgid "显然，由于 MegEngine 中实现了自动求导机制，我们不需要对 ``backward()`` 方法进行实现。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:177
msgid "使用 Sequential 顺序容器"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:179
msgid "``module`` 模块中也提供了 ``Module`` 的有序容器 ``Sequential``, 具体用法如下："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:296
msgid "``Sequential`` 除了本身可以用来定义模型之外，它还可以包装层，把几个层包装起来像一个块一样，我们在 VGG 等模型中会见到更加高级的用法。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:308
msgid "Module 内置方法"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:310
msgid "可以发现，我们能够通过 ``print`` 打印输出模型内部的结构信息，实际上它调用了 ``Module`` 类中的一些内部实现（感兴趣的话可阅读源代码）。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:312
msgid "以下是 ``Module`` 类中常见的方法（但不是全部）："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:314
msgid "``modules()``: 返回一个可迭代对象，可以遍历当前模块中的所有模块，包括其本身;"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:315
msgid "``named_modules()``: 返回一个迭代对象，可以遍历当前模块包括自身在内的所有其内部模块所组成的 ``key`` 与 ``Parameter`` 键值对，其中 ``key`` 是从当前模块到各子模块的点路径（dotted path）."
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:316
msgid "``parameters()``: 返回一个可迭代对象，遍历当前模块中的所有 ``Parameter``;"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:317
msgid "``named_parameters()``: 返回一个可迭代对象，可以遍历当前模块中 ``key`` 与 ``Parameter`` 组成的键值对。其中 ``key`` 是从模块到 ``Parameter`` 的点路径（dotted path）."
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:318
msgid "``state_dict()``: 返回一个有序字典，其键值对是每个网络层和其对应的参数。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:320
msgid "除此以外还提供了 ``buffers()``, ``children()`` 等方法，通过在文档中搜索和阅读 Module API, 可以了解更多细节。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:323
msgid "modules() 与 named\\_modules()"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:325
msgid "我们首先看一下使用 ``module`` 方式实现的 LeNet 结构中存在着哪些模块："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:388
msgid "可以发现，一个 Model 本质上也是一个 ``Module``, 彼此之间有明显的父子关系，这种设计也有利于在 ``Module`` 之间实现嵌套，设计出复用程度高的复杂结构。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:400
msgid "parameters() 与 named\\_parameters（）"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:402
msgid "接下来我们需要关注一下模型中的参数（具体来说是权重 ``weight`` 和偏置 ``bias``\\ ）："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:461
msgid "这意味着，我们既可以借助 ``parameters()`` 完成整体性的操作，也可以根据 ``name`` 进行精细化的处理。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:463
msgid "比如，我们可以通过添加一些 ``if`` 逻辑来对满足规则的参数进行操作："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:511
msgid "更常见的使用情景是，我们可以在定义求导器和优化器时，非常方便地对所有参数进行绑定："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:534
msgid "使用 state\\_dict() 获取信息"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:545
msgid "已知 ``state_dict()`` 是一个有序字典，因此我们可以先看看内部的键有什么："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:588
msgid "很自然地，我们可以通过 ``state_dict()['name']`` 的形式来获取某一层的参数信息："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:631
msgid "我们也可以通过使用 ``named_parameters()`` 来取得同样的效果，但显然这样做比较麻烦，效率也比较低："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:681
msgid "另外，在 ``Optimizer`` 中也提供了 ``state_dict()`` 方法，可以用来获取学习率等超参数信息。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:683
msgid "注意：我们在本次教程中，没有介绍过 ``buffers()``, ``children()`` 的用法，感兴趣的话你可以亲自试一试。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:686
msgid "如何实现共享参数"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:697
msgid "有时候我们需要对模型的相同结构部分参数进行共享，实现起来也很简单："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:699
msgid "由于 ``Parameters`` 其实就是一个 ``Tensor`` 的子类，如果要共享某一层的参数，只需要在 ``Module`` 类的 ``forward()`` 函数里多次调用这个层。因为调用同一个层，计算的时候就是使用的这个层代表的张量，所以多次调用同一个层相当于共享了模型参数。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:816
msgid "可以发现，对于 ``shared_net`` 和 ``sequential_shared_net``, 虽然实际进行了四次线性层运算，但显示出的模型结构却只有一层，表明参数进行了共享。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:818
msgid "现在请你思考一下，对于共享的参数，反向传播的更新策略是什么样的呢？（你可以通过编码验证自己的想法）"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:821
msgid "更多实用的编码技巧"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:823
msgid "对于一些更加细节的操作（比如训练过程中对学习率的更改、固定部分参数不优化等等），可以参考文档用户指南中的《参数优化进阶配置》部分。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:835
msgid "Module 参数初始化"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:837
msgid "敏锐的你或许已经注意到了：我们在上面进行了 ``Module`` 参数的初始化操作，接下来我们将介绍为此而设计的 ``init`` 子模块。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:839
msgid "在 ``module`` 模块中，实现了 ``init`` 子模块，即初始化（Initialization）模块，里面包含着常见的初始化方式："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:841
msgid "``init.zeros_()``: 0 值初始化"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:842
msgid "``init.ones_()``: 1 值初始化"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:843
msgid "``init.fill_()``: 根据给定值进行初始化"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:896
msgid "除此以外还实现了一些统计学意义上的随机初始化策略，如 ``init.uniform_()``, ``init.normal_`` 等。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:898
msgid "我们在这里不进行过多的说明，你可以通过查看 ``Module.init`` API 文档以了解更多细节。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:910
msgid "默认初始化策略"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:912
msgid "在默认情况下，一些 ``Module`` 的基类实现中会有一个默认的参数初始化策略："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:914
msgid "比如 ``Conv1d`` 和 ``Conv2d`` 其实都继承自 ``_ConvNd``, 而在 ``_ConvNd`` 的 ``__init__()`` 方法中进行了参数初始化；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:915
msgid "阅读下面的部分源代码，你会发现 ``Linear`` 也采用了与 ``_ConvNd`` 类似的默认初始化策略；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:916
msgid "我们曾经提到，神经网络模型的参数使用全零初始化不是一个好策略，只是目前我们还没有解释原因；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:917
msgid "目前需要明白的是，MegEngine 中实现的神经网络模块中的默认初始化方式通常能够满足基本需求。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:971
msgid "阅读源代码是我们了解程序本质的好方法，源码面前，了无秘密，欢迎在 MegEngine 的源代码中进行探索～"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:983
msgid "自定义初始化策略"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:985
msgid "在实际的模型设计中，默认的初始化策略可能无法满足需求（或者由于没有阅读源代码，不清楚默认初始化策略），因此总会有需要自己定义初始化策略的时候。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:987
msgid "我们重新定义一个 ``CustomInitLeNet`` 类，并且在里面完成自定义初始化策略的设置："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1039
msgid "我们可以直观地感受到，上面的代码根据 ``module`` 的类型对 ``Conv2d`` 和 ``Linear`` 模块进行了区分，采取了不同的初始化策略。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1041
msgid "针对参数是 ``weight`` 还是 ``bias``, 所执行的初始化策略也有所不同。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1043
msgid "究竟什么样的初始化策略是最优的呢？目前不用搞清楚背后的原因，我们只需要明白通过 ``module`` 和方式来定义模型，比 ``funtional`` 更加灵活高效。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1055
msgid "经典 CNN 结构实现"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1057
msgid "研究和工程人员的一个重要能力是将通过实验想法转变为现实进行验证，有了 ``Module`` 这样强有力的工具，我们要能够做到将想到的模型结构进行高效的实现。但所谓巧妇难为无米之炊，想要一下子蹦出一个有趣的结构是比较困难的。目前我们需要积累更多的神经网络模型设计的代码经验，一种有效的方式就是根据已有的经典科研文献中的网络模型结构进行实现，这样在将来才能够胸有成竹地选择放飞自我的想象力。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1060
msgid "深层卷积神经网络 AlexNet"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1062
msgid "LeNet 通过引入卷积运算成功证明了 CNN 模型的有效性，但是其网络结构比较浅，简单的模型在面对复杂任务的时候容易力不从心。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1064
msgid "接下来，我们利用 ``module`` 来实现一个比 LeNet 更加深的卷积神经网络模型结构，常被称为 `AlexNet <https://paperswithcode.com/method/alexnet>`__:"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1066
msgid "原始的 AlexNet 论文将模型使用于 ImageNet 数据集，而我们用于 CIFAR10 数据集；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1067
msgid "由于不同数据集输入的数据形状不一致，我们如果想要使用 AlexNet 在 CIFAR10 上进行训练，则需要做一些适应性的变化；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1068
msgid "一种思路是对输入图片进行 ``Resize`` 预处理，比如将原本 :math:`32 \\times 32` 的图片缩放成 :math:`224 \\times 224` 的图片；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1069
msgid "另外一种思路是，我们按照输入为 CIFAR10 数据的形状去修改 AlexNet 各层的参数（感兴趣的话可以自己试一下）；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1070
msgid "对应地，分类的数量也要从原本 ImageNet 数据集的 :math:`1000` 修改为 CIFAR10 数据集的 :math:`10` 类；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1071
msgid "我们使用到了 ``module.Sequential`` 来作为顺序容器，可以作为对顺序 ``Module`` 结构的封装。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1126
msgid "AlexNet 模型中引入了 Dropout 的概念，我们会在下个教程中描述它的作用。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1138
msgid "使用重复层级结构的 VGGNet"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1140
msgid "MegEngine 通过 ``module`` 实现了层（Layer）层面的抽象，也提供了顺序容器 ``Sequential``, 有利于开发者写出更加简洁优雅的代码。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1142
msgid "上面的 AlexNet 中，我们将模型结构显式地分为了特征提取部分 ``feature`` 和线性分类 ``classifier`` 部分。但有的时候我们会遇到这样一种情况："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1144
msgid "除了超参数的多次调整与尝试之外，我还想要微调特征提取的逻辑，比如仅改变 ``Conv2d->ReLU->MaxPool2d`` 子结构重复的次数；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1145
msgid "问题在于，这些模块的 ``channel`` 参数在不同的层之间会发生变化，因此不能直接复制粘贴；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1146
msgid "我们不希望每次都要重新定义整个模型的代码，最好能够有一种技巧对相似的子模块进行重复的利用。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1148
msgid "在 VGGNet 中，通过使用循环和子程序，实现了对结构的高效重复，值得我们学习和参考："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1210
msgid "在上面的代码中实现了最基本的 ``VGG`` 类，注意：我们的特征提取序列 ``features`` 需要被指定。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1212
msgid "请欣赏下面一段代码，尝试看出其作用："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1214
msgid "注意：下面这段代码中的 VGGNet 模型中使用了 BatchNorm 层，我们会在下个教程中解释它的用途；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1215
msgid "可以发现在 ``make_layers`` 中产生的序列会是 ``Conv2d(->BatchNorm2d)->ReLU->MaxPool2d`` 的重复；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1216
msgid "它通过变量 ``v`` 的在 ``cfg`` 中的循环来将当前的 ``out_channels`` 更新为下一步的 ``in_channels``."
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1260
msgid "这样我们能够很方便地得到层数不同但结构设计相似的 VGG 模型："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1383
msgid "使用重复块状结构的 GoogLeNet"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1385
msgid "AlexNet 和 VGGNet 都是在基于 LeNet 的背景下，选择对模型层数进行加深，试图取得更好的效果。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1387
msgid "现在我们将要介绍由 Google 提出的 `GoogLeNet(Inception) <https://paperswithcode.com/method/googlenet>`__ 模型结构，看其如何处理块状结构。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1389
msgid "注意：我们不会在这里实现完整的模型，仅仅关注其设计的 Inception Block."
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1391
msgid "|inception|"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1404
msgid "inception"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1393
msgid "如上图所示（图片来自 `原论文 <https://research.google.com/pubs/archive/43022.pdf>`__\\ ），Inception 块由 :math:`4` 条分支（Branch）组成："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1395
msgid "前三条分支使用大小为 :math:`1 \\times 1` 、\\ :math:`3 \\times 3` 和 :math:`5 \\times 5` 的卷积层，负责从不同空间大小中提取信息；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1396
msgid "中间的两条分支在输入上执行 :math:`1 \\times 1` 卷积，以减少通道数，从而降低模型的复杂性；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1397
msgid "第四条分支使用 $3 :nbsphinx-math:`\\times`3 $ 最大池化层，然后使用 :math:`1 \\times 1` 卷积层来改变通道数。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1398
msgid "最后我们将每条分支的输出在 :math:`N, C, H, W` 的 :math:`C` 维度上拼接，构成 Inception 块的输出。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1400
msgid "与 VGG 的区别在于，在 Inception 块中，通常调整的超参数是每层输出通道的数量。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1402
msgid "我们的任务不在于理解当前模型设计的优点，而是要 “看图说话”，实现出 Inception 块结构："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1488
msgid "在完整的 GoogLeNet 中多次使用了 Inception 块来作为整体模型结构的一部分，这种将采用多分支以及重复结构抽象为块（Block）的思路值得我们借鉴。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1500
msgid "深度学习代码的组织形式"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1502
msgid "在训练模型的过程中，我们需要频繁地改变超参数的值以尝试得到最优的配置，甚至会用表格来记录不同的配置组合所取得的效果，一种比较高级的技术是 AutoML, 能够自动地找到最优的超参数配置，但这不是我们所关注的重点。问题在于：而到目前为止，为了便于学习，我们都是通过 Jupyter Notebook 的形式（\\ ``.ipynb`` 格式文件）来交互式地修改每个单元格中的代码，重新运行它们以使得新的配置生效——这并不是实际的工程项目中我们会采取的做法。通常在一份工程化的代码中，我们会以带参数的命令行的形式来运行 ``.py`` 脚本。例如你在阅读 MegEngine 官方提供的 `Models <https://github.com/MegEngine/Models>`__ 模型库代码时，可能会看到这样的目录结构："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1516
msgid "顾名思义，不同的 ``.py`` 脚本文件负责机器学习流水线中不同的环节："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1518
msgid "README: 说明文件，告诉用户如何使用里面的代码；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1519
msgid "data: 数据集的获取和预处理，最终满足传入模型的格式；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1520
msgid "model: 定义你的模型结构，我们即将看到 VGG 模型会具有多种不同的结构；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1521
msgid "train: 模型的训练代码，用于训练数据；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1522
msgid "test: 模型的测试代码，用于测试数据；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1523
msgid "inference: 模型的推理代码，用于实际数据，比如你自己的一张图片。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1525
msgid "比如 `ResNet <https://github.com/MegEngine/Models/tree/master/official/vision/classification/resnet>`__ 的 ``train.py`` 提供了灵活的命令行选项，包括："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1527
msgid "``--data``, ImageNet 数据集的根目录，默认 /data/datasets/imagenet;"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1528
msgid "``--arch``, 需要训练的网络结构，默认 resnet50；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1529
msgid "``--batch-size``\\ ，训练时每张卡采用的 batch size, 默认 64；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1530
msgid "``--ngpus``, 训练时每个节点采用的 GPU 数量，默认 None，即使用全部 GPU；当使用多张 GPU 时，将自动切换为分布式训练模式；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1531
msgid "``--save``, 模型以及 log 存储的目录，默认 output;"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1532
msgid "``--learning-rate``, 训练时的初始学习率，默认 0.025, 在分布式训练下，实际学习率等于初始学习率乘以总 GPU 数；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1533
msgid "``--epochs``, 训练多少个 epoch, 默认90；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1535
msgid "这些参数的传递通过 Python 的命令行选项、参数和子命令解析器 ``argparse`` 模块完成，可以让人轻松编写用户友好的命令行接口。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1538
msgid "args 配置项"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1540
msgid "在本次教程中，我们可以假设通过命令行传递的参数跑到了 ``args`` 对象中（\\ ``argparse`` 模块的真正用法比这复杂一些），看其如何发挥作用。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1572
msgid "用一个简单的例子进行说明，我们在获取数据集的时候，就可以利用到 ``args.datapath``, ``args.train_bs``, ``args.test_bs`` 作为参数："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1623
msgid "对一些常见功能进行适当的封装，通过参数进行控制，将更有利于进行代码复用。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1625
msgid "随着我们阅读过的代码越来越多，我们也会见到更多有利于工程化的代码设计，这些经验将极大提升我们的开发效率。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1628
msgid "模型训练和测试"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1630
msgid "注意：由于使用深层的（计算量更大的）模型结构，\\ **训练会占用掉比较多的时间，**\\ 请自行选择是否要进行模型训练。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1632
msgid "我们以 AlexNet 为例来测试一下最终的效果（你可以自行将下面的代码替换成 VGG 模型）。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1634
msgid "需要注意的是，使用 Module 建模得到的模型 ``model`` 可以分为训练和测试两种模式："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1636
msgid "这是因为一些神经网络算子的行为在不同的模式下也需要不同（尽管我们目前可能还没有用到这些算子）；"
msgstr "This is because the behavior of some neural network operators needs to be different in different modes (although we may not use these operators yet);"

#: ../../source/getting-started/beginner/model-construction.ipynb:1637
msgid "默认情况下启用训练模式 ``model.train()``, 但在进行模型测试时，需要执行 ``model.eval()`` 切换到测试/评估模式。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1749
msgid "虽然我们在 10 个 ``epochs`` 内损失就收敛到了不错的范围，但每个 ``epoch`` 的计算量/用时也变多了。这之间的权衡取舍，则是见仁见智了。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1751
msgid "接下来我们对训练好的模型进行测试："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1793
msgid "模型保存与加载"
msgstr "Model saving and loading"

#: ../../source/getting-started/beginner/model-construction.ipynb:1795
msgid "到目前为止，我们一直在关注如何实现并且训练好一个模型。但我们有时需要把训练好的（模型）参数以文件的形式存储在硬盘上，以供后续使用，比如："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1797
msgid "你预计需要训练超过 1000 个迭代周期的模型，但时间不足，你希望能够将途中模型参数临时保存起来，下次有时间再继续基于保存的参数进行训练；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1798
msgid "你有充足的时间，但是训练太久容易导致过拟合，因此需要每训练一段时间就保存好当前的模型，根据最终的测试效果选出最优的模型；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1799
msgid "你需要将训练的模型部署到其它的设备上进行使用..."
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1801
msgid "最简单的办法是，使用 MegEngine 中提供的 ``save`` 和 ``load`` 接口，我们以最简单的 ``Tensor`` 进行举例："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1846
msgid "在上面的代码执行过程中 ``megengine.save()`` 负责把对象保存成磁盘文件，而 ``megengine.load()`` 负责从文件中加载用 ``save()`` 所保存的对象。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1849
msgid "state\\_dict() 回顾"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1851
msgid "我们回顾一下教程开始提到的一个知识点，在每个 ``Module``/``Optimizer`` 对象中，都提供了 ``state_dict()`` 方法来获取内部的状态信息："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1853
msgid "``Module`` 的 ``state_dict()`` 获取到的是一个有序字典，其键值对是每个网络层和其对应的参数；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1854
msgid "``Optimizer`` 的 ``state_dict()`` 获取到的是无序字典，内部有包含优化器参数的 ``param_groups`` 和 ``state`` 信息。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1900
msgid "因此我们有几种不同的思路来保存和加载我们的模型，现在分别进行介绍。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1903
msgid "保存/加载模型中的参数"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1905
msgid "这是比较推荐的做法。但是注意："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1907
msgid "**在进行预测之前，必须调用 ``.eval()`` 将一些算子设置成验证状态，** 否则会生成前后不一致的预测结果；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1908
msgid "``load_state_dict()`` 方法必须传入一个字典对象，而不是对象的保存路径，也就是说必须先反序列化字典对象，然后再调用该方法。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1939
msgid "加载后的模型可以继续用于训练或者直接用于预测，这里我们测试一下这个刚加载进来的模型，结果应该与之前保存的模型精度完全一致。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1980
msgid "保存/加载整个模型"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1982
msgid "你也可以选择将整个模型采用 Python 的 ``pickle`` 模块来进行保存，缺点在于："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1984
msgid "由于没有 ``state_dict()`` 信息，序列化后的数据只属于特定模型的字典结构；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:1985
msgid "如果对模型的原有实现进行了修改和重构，极有可能导致错误。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2028
msgid "使用检查点（Checkpoint）"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2030
msgid "我们知道了 ``model`` 和 ``optimizer`` 都能够提供 ``state_dict`` 信息，因此一种推荐的做法是将它们和更多的信息组合起来存储："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2042
msgid "这样在加载使用的时候，我们就能够利用更多已经保存下来的信息："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2055
msgid "你会在一些工程化的代码中看到这样的用法。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2067
msgid "总结回顾"
msgstr "Summary review"

#: ../../source/getting-started/beginner/model-construction.ipynb:2069
msgid "MegEngine 中的 ``module`` 为开发者提供了足够程度的抽象，使得我们能够灵活高效地实现各种神经网络的模型结构："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2071
msgid "我们能够借助 ``Module.parameters()`` 方法，快速地获取所有参数的迭代，并且作为 Optimizer 和 GradManager 的输入参数统一管理；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2072
msgid "通过 ``Module.init`` 子模块，模型参数的初始化变得十分简单，我们也能够用自己的逻辑对各层的参数、权重进行更加精细的初始化策略控制；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2073
msgid "通过 ``megengine.save()`` 和 ``megengine.load()`` 可以方便地完成状态信息的保存和加载，\\ ``state_dict`` 信息可以灵活组合使用。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2075
msgid "与此同时，我们也接触到了一些超出深度学习框架服务范围的编程知识（“授人以鱼，不如授人以渔”），我们见识到了更多的经典 CNN 结构设计："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2077
msgid "AlexNet: 将卷积神经网络的层数加深，在 CIFAR10 分类任务上取得了更好的效果；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2078
msgid "VGGNet: 利用循环和子程序，可以高效地“配置”出不同类型的 VGG11, VGG13, VGG16, VGG19 模型结构；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2079
msgid "GoogLeNet: 除了不断地加深神经网络的深度，我们也可以像搭积木一样将整个模型看作是由不同的 “块” 组成的大网络。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2081
msgid "尽管这些模型主要被用在计算机视觉分类相关的应用上，但其中发展出的设计思想，希望能够给你带来一些启发。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2083
msgid "在工程化方面，我们见识到了标准的深度学习工程模型代码长什么样子，这有利于我们将来去阅读其他人复现好的模型代码，或者是将自己的代码组织得更加优雅。另外我们在举例的过程中提到了 ResNet, 这是再经典不过的 CNN 模型了！当你能够自己实现完整的 ResNet 中 ``model.py`` 内部的各种结构时，即意味着你可以从初学者教程“毕业”啦，遇到奇怪的模型结构也不再慌张～ 不过心急吃不了热豆腐，在抵达这个小目标之前，我们还有一段路要走。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2095
msgid "问题思考"
msgstr "Problem thinking"

#: ../../source/getting-started/beginner/model-construction.ipynb:2097
msgid "我们在这个教程中引入了许多新的概念没有解释，比如什么是 ``Dropout`` 算子，什么又是 ``BatchNorm2d`` 算子，它和预处理中的 ``Normalize`` 又有着什么样的关系呢？以及我们一直都没有解释为什么神经网络模型不应该使用零值初始化策略？如果你始终留有疑惑，这是件好事，耐心完成后面的教程，终究会守得云开见月明。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2099
msgid "CIFAR10 图片预测的准确率还有很大的提升空间，是不是意味着只要不断加大模型的深度/宽度，让它变得更复杂，加上精细的超参数调整，就总是能够取得更好的效果呢？"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2101
msgid "你是否思考过这样一个明显的问题：VGG 论文中设计出了 VGG11, VGG13, VGG16, VGG19... 为什么不搞出一个 VGG100 模型呢？直接 VGG6666 岂不是无敌？"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2103
msgid "一个比较直接的原因是，大模型的计算量更大，训练起来的时间成本和经济成本也更高，耗电也多（我们需要有环保意识）；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2104
msgid "我们在处理机器学习任务时，不但要考虑精度，还要考虑速度——比如与目标检测/追踪有关的应用就很在意推理时的速度；"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2105
msgid "实际上，不使用 VGG6666 模型的背后还有着其他的原因，我们在下个教程中将进行解释。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2107
msgid "这个教程中也给你留下了一个挑战，我们没有完整地写出 GoogLeNet 模型，请你试试能否独立地实现出 GoogLeNet, 甚至是整个 Inception Net 系列的网络结构。"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2109
msgid "如果你尝试自己训练过上面的几个模型，修改过不同的超参数方案组合，还可能会遇到如下一些情况："
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2111
msgid "我的 Loss 在训练几个 epoch 之后就不下降了，有时甚至从来没有下降过！"
msgstr "My Loss didn't drop after a few epochs, and sometimes it never dropped!"

#: ../../source/getting-started/beginner/model-construction.ipynb:2112
msgid "我的 Loss 从第一个 epoch 开始就显示为 NaN, 发生什么事了？"
msgstr "My Loss has been displayed as NaN since the first epoch, what happened?"

#: ../../source/getting-started/beginner/model-construction.ipynb:2113
msgid "训练一个 epoch 的成本比之前高了很多，寻找合适的超参数会花掉很多时间..."
msgstr "The cost of training an epoch is much higher than before, and finding the right hyperparameters will take a lot of time..."

#: ../../source/getting-started/beginner/model-construction.ipynb:2115
msgid "除此以外，在不改变模型结构的情况下，我们还在代码中进行了一些修改，比如我们改变了超参数，对输入数据进行了预处理，模型参数使用了除零值之外其它的初始化策略，我还在训练卷积神经网络时偷偷地换用了 Adam 优化器，因为似乎在这个任务上用 Adam 的效果会比 SGD 要好一些。这些修改背后的直觉是什么，当我们需要用神经网络解决不同的任务时，有没有什么经验可循？"
msgstr "In addition, without changing the structure of the model, we also made some modifications in the code. For example, we changed the hyperparameters, preprocessed the input data, and used initializations other than zero for the model parameters. Strategy, I also secretly switched to the Adam optimizer when training the convolutional neural network, because it seems that the effect of using Adam on this task will be better than SGD. What is the intuition behind these modifications, and is there any experience to follow when we need to use neural networks to solve different tasks?"

#: ../../source/getting-started/beginner/model-construction.ipynb:2117
msgid "恭喜你 🎉 ，我们已经开始接触到深度学习炼丹师再熟悉不过的一些 “玄学” 现象，如果能分析出问题背后可能的原因，一定会很有成就感。"
msgstr "Congratulations 🎉, we have already begun to come into contact with some \"metaphysics\" phenomena that deep learning alchemists are not familiar with. If we can analyze the possible reasons behind the problem, we will definitely feel a sense of accomplishment."

#: ../../source/getting-started/beginner/model-construction.ipynb:2119
msgid "对于深度学习初学者来说，经典论文的复现是一个比较有趣的练习方式，我们在介绍 AlexNet 和 VGGNet 时给出了 `Paper with Code <https://paperswithcode.com/>`__ 的网站链接，里面是一个巨大的宝库。你可以尝试参照着一些其他人复现过的模型代码，实现自己的 MegEngine 版本，看看是否能写得更加优雅，鼓励你以开源的方式将它们分享出来。多看，多写，多想。我们在接下来的教程中，将会一起探索一些炼丹玄学现象背后的原因，总结出一些神经网络模型训练中的常见技巧。你也许会觉得奇怪，似乎这些内容没有必要在一个“深度学习框架的文档”中进行提供，可以在网络上找到许多类似的学习材料，但我们总是在不断探索自己的认知边界，无限进步！"
msgstr ""

#: ../../source/getting-started/beginner/model-construction.ipynb:2122
msgid "深度学习，简单开发。我们鼓励你在实践中不断思考，并启发自己去探索直觉性或理论性的解释。"
msgstr "Deep learning, simple development. We encourage you to keep thinking in practice and inspire yourself to explore intuitive or theoretical explanations."

