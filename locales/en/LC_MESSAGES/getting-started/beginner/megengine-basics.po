msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-07-26 13:50+0800\n"
"PO-Revision-Date: 2022-07-27 05:46\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/getting-started/beginner/megengine-basics.po\n"
"X-Crowdin-File-ID: 8377\n"

#: ../../source/getting-started/beginner/megengine-basics.rst:5
msgid "MegEngine 基础概念"
msgstr "MegEngine basic concepts"

#: ../../source/getting-started/beginner/megengine-basics.rst:7
msgid "本教程涉及的内容"
msgstr "What this tutorial covers"

#: ../../source/getting-started/beginner/megengine-basics.rst:10
msgid "介绍 MegEngine 的基本数据结构 :class:`~.Tensor` 以及 :mod:`~.functional` 模块中的基础运算操作；"
msgstr "Introduce the basic data structures of MegEngine :class:`~.Tensor` and :mod:`~.functional` basic operations in modules;"

#: ../../source/getting-started/beginner/megengine-basics.rst:11
msgid "介绍计算图的有关概念，实践深度学习中前向传播、反向传播和参数更新的基本流程；"
msgstr "Introduce the relevant concepts of computational graphs, and practice the basic processes of forward propagation, back propagation and parameter update in deep learning;"

#: ../../source/getting-started/beginner/megengine-basics.rst:12
msgid "根据前面的介绍，分别使用 NumPy 和 MegEngine 完成一个简单的直线拟合任务。"
msgstr "According to the previous introduction, use NumPy and MegEngine respectively to complete a simple line fitting task."

#: ../../source/getting-started/beginner/megengine-basics.rst:14
msgid "学习的过程中应避开完美主义"
msgstr "Avoid perfectionism while learning"

#: ../../source/getting-started/beginner/megengine-basics.rst:17
msgid "请以完成教程目标为首要任务进行学习，MegEngine 教程中会充斥着许多的拓展解释和链接，这些内容往往不是必需品。 通常它们是为学有余力的同学，亦或者基础过于薄弱的同学而准备的，如果你遇到一些不是很清楚的地方， 不妨试着先将整个教程看完，代码跑完，再回头补充那些需要的知识。"
msgstr "Please study with the goal of completing the tutorial as the first task. The MegEngine tutorial will be filled with many extended explanations and links, which are often not necessary. Usually they are prepared for students who have spare capacity for learning, or students whose foundation is too weak. If you encounter something that is not very clear, you may try to read the entire tutorial first, run the code, and then go back and add those required knowledge."

#: ../../source/getting-started/beginner/megengine-basics.rst:22
msgid "基础数据结构：张量"
msgstr "underlying data structure：tensors"

#: ../../source/getting-started/beginner/megengine-basics.rst:28
msgid "MegEngine 中提供了一种名为 “张量” （ :py:class:`~.Tensor` ）的数据结构， 区别于数学中的定义，其概念与 NumPy_  :footcite:p:`harris2020array` 中的 :py:class:`~numpy.ndarray` 更加相似，即多维数组。 真实世界中的很多非结构化的数据，如文字、图片、音频、视频等，都可以抽象成 Tensor 的形式进行表达。 我们所提到的 Tensor 的概念往往是其它更具体概念的概括（或者说推广）："
msgstr "MegEngine provides a data structure called \"tensor\" ( :py:class:`~.Tensor` ), which is different from the definition in mathematics, and its concept is the same as that in NumPy_ :footcite:p:`harris2020array`: py:class:`~numpy.ndarray` is more similar, i.e. multidimensional arrays. Many unstructured data in the real world, such as text, pictures, audio, video, etc., can be abstracted into the form of Tensor for expression. The concept of Tensor we mentioned is often a generalization (or generalization) of other more specific concepts："

#: ../../source/getting-started/beginner/megengine-basics.rst:34
msgid "数学"
msgstr "math"

#: ../../source/getting-started/beginner/megengine-basics.rst:34
msgid "计算机科学"
msgstr "computer science"

#: ../../source/getting-started/beginner/megengine-basics.rst:34
msgid "几何概念"
msgstr "geometric concept"

#: ../../source/getting-started/beginner/megengine-basics.rst:34
msgid "具象化例子"
msgstr "Concrete example"

#: ../../source/getting-started/beginner/megengine-basics.rst:36
msgid "标量（scalar）"
msgstr "Scalar"

#: ../../source/getting-started/beginner/megengine-basics.rst:36
msgid "数字（number）"
msgstr "Number"

#: ../../source/getting-started/beginner/megengine-basics.rst:36
msgid "点"
msgstr "point"

#: ../../source/getting-started/beginner/megengine-basics.rst:36
msgid "得分、概率"
msgstr "Score, probability"

#: ../../source/getting-started/beginner/megengine-basics.rst:37
msgid "向量（vector）"
msgstr "Vector"

#: ../../source/getting-started/beginner/megengine-basics.rst:37
msgid "数组（array）"
msgstr "Array"

#: ../../source/getting-started/beginner/megengine-basics.rst:37
msgid "线"
msgstr "String"

#: ../../source/getting-started/beginner/megengine-basics.rst:37
msgid "列表"
msgstr "List"

#: ../../source/getting-started/beginner/megengine-basics.rst:38
msgid "矩阵（matrix）"
msgstr "Matrix"

#: ../../source/getting-started/beginner/megengine-basics.rst:38
msgid "2 维数组（2d-array）"
msgstr "2 dimensional array (2d-array)"

#: ../../source/getting-started/beginner/megengine-basics.rst:38
msgid "面"
msgstr "noodle"

#: ../../source/getting-started/beginner/megengine-basics.rst:38
msgid "Excel 表格"
msgstr "Excel spreadsheet"

#: ../../source/getting-started/beginner/megengine-basics.rst:39
msgid "3 维张量"
msgstr "3-dimensional tensor"

#: ../../source/getting-started/beginner/megengine-basics.rst:39
msgid "3 维数组（3d-array）"
msgstr "3-dimensional array (3d-array)"

#: ../../source/getting-started/beginner/megengine-basics.rst:39
msgid "体"
msgstr "body"

#: ../../source/getting-started/beginner/megengine-basics.rst:39
msgid "RGB 图片"
msgstr "RGB pictures"

#: ../../source/getting-started/beginner/megengine-basics.rst:40
msgid "..."
msgstr "..."

#: ../../source/getting-started/beginner/megengine-basics.rst:41
msgid "n 维张量"
msgstr "n-dimensional tensor"

#: ../../source/getting-started/beginner/megengine-basics.rst:41
msgid "n 维数组（nd-array）"
msgstr "n-dimensional array (nd-array)"

#: ../../source/getting-started/beginner/megengine-basics.rst:41
msgid "高维空间"
msgstr "high dimensional space"

#: ../../source/getting-started/beginner/megengine-basics.rst:44
msgid "以一个 2x3 的矩阵（2 维张量）为例，在 MegEngine 中用嵌套的 Python 列表初始化 Tensor:"
msgstr "Taking a 2x3 matrix (a 2-dimensional tensor) as an example, initialize the Tensor with a nested Python list in MegEngine:"

#: ../../source/getting-started/beginner/megengine-basics.rst:50
msgid "它的基本属性有 :attr:`.Tensor.ndim`, :attr:`.Tensor.shape`, :attr:`.Tensor.dtype`, :attr:`.Tensor.device` 等。"
msgstr "Its basic properties are :attr:`.Tensor.ndim`, :attr:`.Tensor.shape`, :attr:`.Tensor.dtype`, :attr:`.Tensor.device` and so on."

#: ../../source/getting-started/beginner/megengine-basics.rst:52
msgid "我们可以基于 Tensor 数据结构，进行各式各样的科学计算；"
msgstr "We can perform various scientific calculations based on the Tensor data structure;"

#: ../../source/getting-started/beginner/megengine-basics.rst:53
msgid "Tensor 也是神经网络编程时所用的主要数据结构，网络的输入、输出和转换都使用 Tensor 表示。"
msgstr "Tensor is also the main data structure used in neural network programming. The input, output and conversion of the network are all represented by Tensor."

#: ../../source/getting-started/beginner/megengine-basics.rst:59
msgid "与 NumPy 的区别之处在于，MegEngine 还支持利用 GPU 设备进行更加高效的计算。 当 GPU 和 CPU 设备都可用时，MegEngine 将优先使用 GPU 作为默认计算设备，无需用户进行手动设定。 另外 MegEngine 还支持自动微分（Autodiff）等特性，我们将在后续教程适当的环节进行介绍。"
msgstr "The difference from NumPy is that MegEngine also supports the use of GPU devices for more efficient computations. When both GPU and CPU devices are available, MegEngine will preferentially use the GPU as the default computing device without requiring manual settings by the user. In addition, MegEngine also supports features such as automatic differentiation (Autodiff), which we will introduce in the appropriate sections of the subsequent tutorials."

#: ../../source/getting-started/beginner/megengine-basics.rst:63
msgid "如果你完全没有 NumPy 使用经验"
msgstr "If you have absolutely no experience with NumPy"

#: ../../source/getting-started/beginner/megengine-basics.rst:66
msgid "可以参考 :ref:`tensor-guide` 中的介绍, 或者先查看 NumPy_ 官网文档和教程；"
msgstr "You can refer to the introduction in :ref:`tensor-guide`, or check the NumPy_ official website documentation and tutorials first;"

#: ../../source/getting-started/beginner/megengine-basics.rst:67
msgid "其它比较不错的补充材料还有 CS231n 的 《 `Python Numpy Tutorial (with Jupyter and Colab) <https://cs231n.github.io/python-numpy-tutorial/>`_ 》。"
msgstr "Other good supplementary material is \" `Python Numpy Tutorial (with Jupyter and Colab) <https://cs231n.github.io/python-numpy-tutorial/>`_ \" by CS231n."

#: ../../source/getting-started/beginner/megengine-basics.rst:72
msgid "Tensor 操作与计算"
msgstr "Tensor operations and computations"

#: ../../source/getting-started/beginner/megengine-basics.rst:74
msgid "与 NumPy 的多维数组一样，Tensor 可以用标准算数运算符进行逐元素（Element-wise）的加减乘除等运算："
msgstr "Like NumPy's multidimensional arrays, Tensor can perform element-wise addition, subtraction, multiplication, and division operations using standard arithmetic：."

#: ../../source/getting-started/beginner/megengine-basics.rst:91
msgid ":class:`~.Tensor` 类中提供了一些比较常见的方法，比如 :meth:`.Tensor.reshape` 方法， 可以用来改变 Tensor 的形状（该操作不会改变 Tensor 元素总数目以及各个元素的值）："
msgstr ":class:`~.Tensor` class provides some common methods, such as :meth:`.Tensor.reshape` method, which can be used to change the shape of Tensor (this operation will not change the total number of Tensor elements and the value of each element)："

#: ../../source/getting-started/beginner/megengine-basics.rst:99
msgid "但通常我们会 :ref:`functional-guide`, 例如使用 :func:`.functional.reshape` 来改变形状："
msgstr "But usually we would :ref:`functional-guide`, for example use :func:`.functional.reshape` to change shape："

#: ../../source/getting-started/beginner/megengine-basics.rst:108
msgid "一个常见误区是，初学者会认为调用 ``a.reshape()`` 后 ``a`` 自身的形状会发生改变。 事实上并非如此，在 MegEngine 中绝大部分操作都不是原地（In-place）操作， 这意味着通常调用这些接口将会返回一个新的 Tensor, 而不会对原本的 Tensor 进行更改。"
msgstr "A common misconception is that beginners think that the shape of ``a`` itself will change after calling ``a.reshape()``. In fact, this is not the case. Most operations in MegEngine are not in-place operations, which means that usually calling these interfaces will return a new Tensor without changing the original Tensor."

#: ../../source/getting-started/beginner/megengine-basics.rst:114
msgid "在 :mod:`~.functional` 模块中提供了更多的算子（Operator），并按照使用情景对命名空间进行了划分， 目前我们只需要接触这些最基本的算子即可，将来会接触到专门用于神经网络编程的算子。"
msgstr ":mod:`~.functional` module provides more operators, and divides the namespace according to the usage scenarios. At present, we only need to touch these most basic operators, and in the future, we will touch the specialized ones. Operators for neural network programming."

#: ../../source/getting-started/beginner/megengine-basics.rst:118
msgid "理解计算图"
msgstr "Understanding Computational Graphs"

#: ../../source/getting-started/beginner/megengine-basics.rst:122
msgid "MegEngine 是基于计算图（Computing Graph）的深度神经网络学习框架；"
msgstr "MegEngine is a deep neural network learning framework based on Computing Graph;"

#: ../../source/getting-started/beginner/megengine-basics.rst:123
msgid "在深度学习领域，任何复杂的深度神经网络模型本质上都可以用一个计算图表示出来。"
msgstr "In the field of deep learning, any complex deep neural network model can essentially be represented by a computational graph."

#: ../../source/getting-started/beginner/megengine-basics.rst:125
msgid "我们先通过一个简单的数学表达式 :math:`y=w*x+b` 作为例子，来介绍计算图的基本概念："
msgstr "Let's first introduce the basic concept of computational graph：by taking a simple mathematical expression :math:`y=w*x+b` as an example"

#: ../../source/getting-started/beginner/megengine-basics.rst:129
msgid "MegEngine 中 Tensor 为数据节点, Operator 为计算节点"
msgstr "In MegEngine, Tensor is the data node, and Operator is the computing node"

#: ../../source/getting-started/beginner/megengine-basics.rst:131
msgid "从输入数据到输出数据之间的节点依赖关系可以构成一张有向无环图（DAG），其中有："
msgstr "The node dependencies from input data to output data can form a directed acyclic graph (DAG), which has："

#: ../../source/getting-started/beginner/megengine-basics.rst:133
msgid "数据节点：如输入数据 :math:`x`, 参数 :math:`w` 和 :math:`b`, 中间结果 :math:`p`, 以及最终输出 :math:`y`;"
msgstr "Data node：as input data :math:`x`, parameters :math:`w` and :math:`b`, intermediate result :math:`p`, and final output :math:`y`;"

#: ../../source/getting-started/beginner/megengine-basics.rst:134
msgid "计算节点：如图中的 :math:`*` 和 :math:`+` 分别代表乘法和加法两种算子，根据给定的输入计算输出；"
msgstr "In the calculation node：, :math:`*` and :math:`+` in the figure represent two operators of multiplication and addition, respectively, and calculate the output according to the given input;"

#: ../../source/getting-started/beginner/megengine-basics.rst:135
msgid "有向边：表示了数据的流向，体现了数据节点和计算节点之间的前后依赖关系。"
msgstr "Directed edge：represents the flow of data, and reflects the front and back dependencies between data nodes and computing nodes."

#: ../../source/getting-started/beginner/megengine-basics.rst:137
msgid "有了计算图这一表示形式，我们可以对前向传播和反向传播的过程有更加直观的理解。"
msgstr "With the representation of the computational graph, we can have a more intuitive understanding of the process of forward propagation and back propagation."

#: ../../source/getting-started/beginner/megengine-basics.rst:141
msgid "根据模型的定义进行前向计算得到输出，在上面的例子中即是 ——"
msgstr "The forward calculation is performed according to the definition of the model to obtain the output, which in the above example is -"

#: ../../source/getting-started/beginner/megengine-basics.rst:143
msgid "输入数据 :math:`x` 和参数 :math:`w` 经过乘法运算得到中间结果 :math:`p`;"
msgstr "Input data :math:`x` and parameter :math:`w` are multiplied to get intermediate result :math:`p`;"

#: ../../source/getting-started/beginner/megengine-basics.rst:144
msgid "中间结果 :math:`p` 和参数 :math:`b` 经过加法运算得到输出结果 :math:`y`;"
msgstr "The intermediate result :math:`p` and the parameter :math:`b` are added to obtain the output result :math:`y`;"

#: ../../source/getting-started/beginner/megengine-basics.rst:145
msgid "对于更加复杂的计算图结构，其前向计算的依赖关系本质上就是一个拓扑排序。"
msgstr "For more complex computational graph structures, the forward computation dependency is essentially a topological sort."

#: ../../source/getting-started/beginner/megengine-basics.rst:149
msgid "根据需要优化的目标（这里我们简单假定为 :math:`y`），通过链式求导法则， 求出模型中所有参数所对应的梯度，在上面的例子中即计算 :math:`\\nabla y(w, b)`, 由偏导 :math:`\\frac{\\partial y}{\\partial w}` 和 :math:`\\frac{\\partial y}{\\partial b}` 组成。"
msgstr "According to the target to be optimized (here we simply assume :math:`y`), the gradient corresponding to all parameters in the model is obtained through the chain derivation rule. In the above example, :math:`\\nabla y(w, b)`, consisting of partial derivatives :math:`\\frac{\\partial y}{\\partial w}` and :math:`\\frac{\\partial y}{\\partial b}`."

#: ../../source/getting-started/beginner/megengine-basics.rst:153
msgid "这一小节会使用到微积分知识，可以借助互联网上的一些资料进行快速学习/复习："
msgstr "This section will use calculus knowledge, which can be quickly learned/reviewed with the help of some materials on the："

#: ../../source/getting-started/beginner/megengine-basics.rst:155
msgid "3Blue1Brown - `微积分的本质 [Bilibili] <https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528931>`_ / `Essence of calculus [YouTube] <https://youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr>`_"
msgstr "3Blue1Brown - `Essence of calculus [Bilibili] <https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528931>`_ / `Essence of calculus [YouTube] <https://youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr>`_"

#: ../../source/getting-started/beginner/megengine-basics.rst:159
msgid "例如，为了得到上图中 :math:`y` 关于参数 :math:`w` 的偏导，反向传播的过程如下图所示："
msgstr "For example, in order to get the partial derivative of :math:`y` with respect to parameter :math:`w` in the above figure, the process of backpropagation is shown in the following figure："

#: ../../source/getting-started/beginner/megengine-basics.rst:163
msgid "首先有 :math:`y=p+b`, 因此有 :math:`\\frac{\\partial y}{\\partial p}=1`;"
msgstr "First there is :math:`y=p+b`, so :math:`\\frac{\\partial y}{\\partial p}=1`;"

#: ../../source/getting-started/beginner/megengine-basics.rst:164
msgid "继续反向传播，有 :math:`p=w*x`, 因此有 :math:`\\frac{\\partial p}{\\partial w}=x`;"
msgstr "Continue backpropagation, there are :math:`p=w*x`, so there is :math:`\\frac{\\partial p}{\\partial w}=x`;"

#: ../../source/getting-started/beginner/megengine-basics.rst:165
msgid "根据链式法则有 :math:`\\frac{\\partial y}{\\partial w}=\\frac{\\partial y}{\\partial p} \\cdot \\frac{\\partial p}{\\partial w}=1 \\cdot x`, 因此最终求出 :math:`y` 关于参数 :math:`w` 的偏导为 :math:`x`."
msgstr "According to the chain rule :math:`\\frac{\\partial y}{\\partial w}=\\frac{\\partial y}{\\partial p} \\cdot \\frac{\\partial p}{\\partial w}=1 \\cdot x`, so the final partial derivative of :math:`y` with respect to parameter :math:`w` is :math:`x`."

#: ../../source/getting-started/beginner/megengine-basics.rst:168
msgid "求得的梯度也会是一个 Tensor, 将在下一步参数优化中被使用。"
msgstr "The obtained gradient will also be a Tensor, which will be used in the next step of parameter optimization."

#: ../../source/getting-started/beginner/megengine-basics.rst:172
msgid "常见的做法是使用梯度下降法对参数进行更新，在上面的例子中即对 :math:`w` 和 :math:`b` 的值做更新。 我们用一个例子帮助你理解梯度下降的核心思路：假设你现在迷失于一个山谷中，需要寻找有人烟的村庄，我们的目标是最低的平原点 （那儿有人烟的概率是最大的）。采取梯度下降的策略，则要求我们每次都环顾四周，看哪个方向是最陡峭的； 然后沿着梯度的负方向向下迈出一步，循环执行上面的步骤，我们认为这样能更快地下山。"
msgstr "A common practice is to use gradient descent to update the parameters, in the above example to the values of :math:`w` and :math:`b`. We use an example to help you understand the core idea of gradient descent：Suppose you are now lost in a valley and need to find a populated village, our goal is the lowest plain point (where the probability of being populated is the greatest). Adopting a gradient descent strategy requires us to look around each time to see which direction is the steepest; then take a step down in the negative direction of the gradient and cycle through the above steps, which we think can go downhill faster ."

#: ../../source/getting-started/beginner/megengine-basics.rst:177
msgid "我们每完成一次参数的更新，便说明对参数进行了一次迭代（Iteration），训练模型时往往会有多次迭代。"
msgstr "Every time we complete a parameter update, it means that the parameters have been iterated once, and there are often multiple iterations when training the model."

#: ../../source/getting-started/beginner/megengine-basics.rst:179
msgid "如果你还不清楚梯度下降能取得什么样的效果，没有关系，本教程末尾会有更加直观的任务实践。 你也可以在互联网上查阅更多解释梯度下降算法的资料。"
msgstr "If you don't know what gradient descent can achieve, that's okay, there are more intuitive tasks practiced at the end of this tutorial. You can also find more material explaining the gradient descent algorithm on the Internet."

#: ../../source/getting-started/beginner/megengine-basics.rst:197
msgid "自动微分与参数优化"
msgstr "Automatic differentiation and parameter optimization"

#: ../../source/getting-started/beginner/megengine-basics.rst:199
msgid "不难发现，有了链式法则，要做到计算梯度并不困难。但我们上述演示的计算图仅仅是一个非常简单的运算， 当我们使用复杂的模型时，抽象出的计算图结构也会变得更加复杂，如果此时再去手动地根据链式法则计算梯度， 整个过程将变得异常枯燥无聊，而且这对粗心的朋友来说极其不友好，谁也不希望因为某一步算错导致进入漫长的 Debug 阶段。 MegEngine 作为深度学习框架的另一特性是支持了自动微分，即自动地完成反传过程中根据链式法则去推导参数梯度的过程。 与此同时，也提供了方便进行参数优化的相应接口。"
msgstr "It is not difficult to find that with the chain rule, it is not difficult to calculate the gradient. But the computational graph we demonstrated above is only a very simple operation. When we use complex models, the abstracted computational graph structure will also become more complicated. If we manually calculate the gradient according to the chain rule at this time, The whole process will become extremely boring, and it is extremely unfriendly to careless friends, and no one wants to enter the long Debug stage because of a wrong calculation. Another feature of MegEngine as a deep learning framework is that it supports automatic differentiation, that is, it automatically completes the process of deriving parameter gradients according to the chain rule in the backpropagation process. At the same time, it also provides a corresponding interface to facilitate parameter optimization."

#: ../../source/getting-started/beginner/megengine-basics.rst:206
msgid "Tensor 梯度与梯度管理器"
msgstr "Tensor Gradients and Gradient Manager"

#: ../../source/getting-started/beginner/megengine-basics.rst:208
msgid "在 MegEngine 中，每个 :class:`~.Tensor` 都具备 :attr:`.Tensor.grad` 这一属性，即梯度（Gradient）的缩写。"
msgstr "In MegEngine, each :class:`~.Tensor` has a property of :attr:`.Tensor.grad`, which is short for Gradient."

#: ../../source/getting-started/beginner/megengine-basics.rst:213
msgid "然而上面的用法并不正确，默认情况下 Tensor 计算时不会计算和记录梯度信息。"
msgstr "However, the above usage is incorrect. By default, Tensor does not calculate and record gradient information when calculating."

#: ../../source/getting-started/beginner/megengine-basics.rst:215
msgid "我们需要用到梯度管理器 :class:`~.GradManager` 来完成相关操作："
msgstr "We need to use gradient manager :class:`~.GradManager` to complete related operations："

#: ../../source/getting-started/beginner/megengine-basics.rst:217
msgid "使用 :meth:`.GradManager.attach` 来绑定需要计算梯度的参数；"
msgstr "Use :meth:`.GradManager.attach` to bind the parameters that need to calculate the gradient;"

#: ../../source/getting-started/beginner/megengine-basics.rst:218
msgid "使用 ``with`` 关键字，配合记录整个前向计算的过程，形成计算图；"
msgstr "Use the ``with`` keyword to record the entire forward calculation process to form a calculation graph;"

#: ../../source/getting-started/beginner/megengine-basics.rst:219
msgid "调用 :meth:`.GradManager.backward` 即可自动进行反向传播（过程中进行了自动微分）。"
msgstr "Call :meth:`.GradManager.backward` for automatic backpropagation (automatic differentiation is done in the process)."

#: ../../source/getting-started/beginner/megengine-basics.rst:231
msgid "这时可以看到参数 :math:`w` 和 :math:`b` 对应的梯度（前面计算过了 :math:`\\frac{\\partial y}{\\partial w} = x = 2.0` ）："
msgstr "At this time, you can see the gradient corresponding to the parameters :math:`w` and :math:`b` ( :math:`\\frac{\\partial y}{\\partial w} = x = 2.0` was calculated earlier)："

#: ../../source/getting-started/beginner/megengine-basics.rst:238
msgid "值得注意的是， :meth:`.GradManager.backward` 计算梯度时的做法是累加而不是替换，如果接着执行："
msgstr "It is worth noting that :meth:`.GradManager.backward` calculates gradients by accumulating instead of replacing, if：is then executed"

#: ../../source/getting-started/beginner/megengine-basics.rst:247
msgid "可以发现此时参数 :math:`w` 的梯度是 4 而不是 2, 这是因为新的梯度和旧的梯度进行了累加。"
msgstr "It can be found that the gradient of parameter :math:`w` is 4 instead of 2 at this time, this is because the new gradient and the old gradient are accumulated."

#: ../../source/getting-started/beginner/megengine-basics.rst:251
msgid "想要了解更多细节，可以参考 :ref:`autodiff-guide` 。"
msgstr "For more details, see :ref:`autodiff-guide` ."

#: ../../source/getting-started/beginner/megengine-basics.rst:254
msgid "参数（Parameter）"
msgstr "Parameter"

#: ../../source/getting-started/beginner/megengine-basics.rst:256
msgid "你可能注意到了这样一个细节：我们在前面的介绍中，使用参数（Parameter）来称呼 :math:`w` 和 :math:`b`. 因为与输入数据 :math:`x` 不同，它们是需要在模型训练过程中被优化更新的变量。 在 MegEngine 中有 :class:`~.Parameter` 类专门和 :class:`~.Tensor` 进行区分，但它本质上是一种特殊的张量。 因此梯度管理器也支持维护计算过程中 :class:`~.Parameter` 的梯度信息。"
msgstr "You may have noticed such a detail. In the previous introduction, we used parameters to call :math:`w` and：`b`. Because unlike the input data :math:` :math:, they are required in the model training process. The variable to be updated by optimization. There are :class:`~.Parameter` classes in MegEngine specifically to differentiate from :class:`~.Tensor`, but it is essentially a special kind of tensor. Therefore, the gradient manager also supports maintaining the gradient information of :class:`~.Parameter` in the calculation process."

#: ../../source/getting-started/beginner/megengine-basics.rst:281
msgid ":class:`~.Parameter` 和 :class:`~.Tensor` 的区别主要体现在参数优化这一步，在下一小节会进行介绍。"
msgstr "The difference between :class:`~.Parameter` and :class:`~.Tensor` is mainly reflected in the parameter optimization step, which will be introduced in the next section."

#: ../../source/getting-started/beginner/megengine-basics.rst:283
msgid "在前面我们已知了参数 :math:`w` 和它对应的梯度 :math:`w.grad`, 执行一次梯度下降的逻辑非常简单："
msgstr "In the front we know the parameter :math:`w` and its corresponding gradient :math:`w.grad`, the logic of performing a gradient descent is very simple："

#: ../../source/getting-started/beginner/megengine-basics.rst:285
msgid "w = w - lr * w.grad"
msgstr "w = w - lr * w.grad"

#: ../../source/getting-started/beginner/megengine-basics.rst:289
msgid "对每个参数都执行一样的操作。这里引入了一个超参数：学习率（Learning rate），控制每次参数更新的幅度。 不同的参数在更新时可以使用不同的学习率，甚至同样的参数在下一次更新时也可以改变学习率， 但是为了便于初期的学习和理解，我们在教程中将使用一致的学习率。"
msgstr "Do the same for each parameter. A hyperparameter：learning rate is introduced here to control the magnitude of each parameter update. Different parameters can be updated with different learning rates, and even the same parameter can change the learning rate on the next update, but for ease of initial learning and understanding, we will use a consistent learning rate throughout the tutorial."

#: ../../source/getting-started/beginner/megengine-basics.rst:294
msgid "优化器（Optimizer）"
msgstr "Optimizer"

#: ../../source/getting-started/beginner/megengine-basics.rst:296
msgid "MegEngine 的 :mod:`~.optimizer` 模块提供了基于各种常见优化策略的优化器，如 :class:`~.SGD` 和 :class:`~.Adam` 等。 它们的基类是 :class:`~.Optimizer`，其中 :class:`~.SGD` 对应随机梯度下降算法，也是本教程中将会用到的优化器。"
msgstr ":mod:`~.optimizer` module of MegEngine provides optimizers based on various common optimization strategies, such as :class:`~.SGD` and :class:`~.Adam`, etc. Their base class is :class:`~.Optimizer`, where :class:`~.SGD` corresponds to the stochastic gradient descent algorithm, which is the optimizer that will be used in this tutorial."

#: ../../source/getting-started/beginner/megengine-basics.rst:314
msgid "调用 :meth:`.Optimizer.step` 进行一次参数更新，调用 :meth:`.Optimizer.clear_grad` 可以清空 :attr:`.Tensor.grad`."
msgstr "Call :meth:`.Optimizer.step` for a parameter update, and call :meth:`.Optimizer.clear_grad` to clear :attr:`.Tensor.grad`."

#: ../../source/getting-started/beginner/megengine-basics.rst:319
msgid "许多初学者容易忘记在新一轮的参数更新时清空梯度，导致得到了不正确的结果。"
msgstr "Many beginners tend to forget to clear the gradients on a new round of parameter updates, resulting in incorrect results."

#: ../../source/getting-started/beginner/megengine-basics.rst:323
msgid ":class:`~.Optimizer` 接受的输入类型必须是 :class:`~.Parameter` 而非 :class:`~.Tensor`, 否则报错。"
msgstr ":class:`~.Optimizer` accepts input type must be :class:`~.Parameter` instead of :class:`~.Tensor`, otherwise an error will be reported."

#: ../../source/getting-started/beginner/megengine-basics.rst:331
msgid "想要了解更多细节，可以参考 :ref:`optimizer-guide` 。"
msgstr "For more details, see :ref:`optimizer-guide`."

#: ../../source/getting-started/beginner/megengine-basics.rst:334
msgid "优化目标的选取"
msgstr "Selection of optimization goals"

#: ../../source/getting-started/beginner/megengine-basics.rst:336
msgid "想要提升模型的预测效果，我们需要有一个合适的优化目标。"
msgstr "To improve the prediction performance of the model, we need to have a suitable optimization objective."

#: ../../source/getting-started/beginner/megengine-basics.rst:338
msgid "但请注意，上面用于举例的表达式仅用于理解计算图， 其输出值 :math:`y` 往往并不是实际需要被优化的对象， 它仅仅是模型的输出，单纯地优化这个值没有任何意义。"
msgstr "But please note that the expression used for the example above is only used to understand the calculation graph, and its output value :math:`y` is often not the object that actually needs to be optimized, it is only the output of the model, and it is meaningless to simply optimize this value ."

#: ../../source/getting-started/beginner/megengine-basics.rst:342
msgid "那么我们要如何去评估一个模型预测性能的好坏呢？ 核心原则是： **犯错越少，表现越好。**"
msgstr "So how do we evaluate the predictive performance of a model? The core principle is： **The less mistakes you make, the better you perform. **"

#: ../../source/getting-started/beginner/megengine-basics.rst:344
msgid "通常而言，我们需要优化的目标被称为损失（Loss），用来度量模型的输出值和实际结果之间的差异。 如果能够将损失优化到尽可能地低，就意味着模型在当前数据上的预测效果越好。 目前我们可以认为，一个在当前数据集上表现良好的模型，也能够对新输入的数据产生不错的预测效果。"
msgstr "Generally speaking, the objective we need to optimize is called the loss (Loss), which is used to measure the difference between the output value of the model and the actual result. If the loss can be optimized to be as low as possible, it means that the model predicts better on the current data. At present, we can assume that a model that performs well on the current dataset can also produce good predictions on newly input data."

#: ../../source/getting-started/beginner/megengine-basics.rst:348
msgid "这样的描述或许有些抽象，让我们直接通过实践来进行理解。"
msgstr "Such a description may be a bit abstract, let us understand it directly through practice."

#: ../../source/getting-started/beginner/megengine-basics.rst:351
msgid "练习：拟合一条直线"
msgstr "Exercise：Fitting a Line"

#: ../../source/getting-started/beginner/megengine-basics.rst:353
msgid "假设你得到了数据集 :math:`\\mathcal{D}=\\{ (x_i, y_i) \\}`, 其中 :math:`i \\in \\{1, \\ldots, 100 \\}`, 希望将来给出输入 :math:`x`, 能够预测出合适的 :math:`y` 值。"
msgstr "Suppose you got the dataset :math:`\\mathcal{D}=\\{ (x_i, y_i) \\}`, where :math:`i \\in \\{1, \\ldots, 100 \\}`, hope to give input :math:`x` in the future , can predict the appropriate :math:`y` value."

#: ../../source/getting-started/beginner/megengine-basics.rst:358
msgid "下面是随机生成这些数据点的代码实现："
msgstr "Below is the code implementation to randomly generate these data points："

#: ../../source/getting-started/beginner/megengine-basics.rst:377
msgid "可以发现数据点是基于直线 :math:`y = 5.0 * x + 2.0` 加上一些随机噪声生成的。"
msgstr "It can be found that the data points are generated based on the line :math:`y = 5.0 * x + 2.0` plus some random noise."

#: ../../source/getting-started/beginner/megengine-basics.rst:379
msgid "但是在本教程中，我们应当假设自己没有这样的上帝视角， 所能获得的仅仅是这些数据点的坐标，并不知道理想情况下的 :math:`w=5.0` 以及 :math:`b=2.0`, 只能通过已有的数据去迭代更新参数。 通过损失或者其它的手段来判断最终模型的好坏（比如直线的拟合程度）， 在后续教程中会向你展示更加科学的做法。"
msgstr "But in this tutorial, we should assume that we don't have such a God's perspective, and all we can get is the coordinates of these data points, and we don't know the ideal :math:`w=5.0` and :math:`b=2.0`, only The parameters can be iteratively updated through the existing data. Use loss or other means to judge the quality of the final model (such as the fit of the straight line), and will show you a more scientific approach in subsequent tutorials."

#: ../../source/getting-started/beginner/megengine-basics.rst:391
msgid "通过可视化分析发现（如上图）：这些点的分布很适合用一条直线 :math:`f(x) = w * x + b` 去进行拟合。"
msgstr "Through visual analysis, it is found (as shown above：that the distribution of these points is very suitable for fitting with a straight line :math:`f(x) = w * x + b`."

#: ../../source/getting-started/beginner/megengine-basics.rst:396
msgid "所有的样本点的横坐标 :math:`x` 经过我们的模型后会得到一个预测输出 :math:`\\hat{y} = f(x)`."
msgstr "The abscissa :math:`x` of all sample points will get a predicted output :math:`\\hat{y} = f(x)` after passing through our model."

#: ../../source/getting-started/beginner/megengine-basics.rst:398
msgid "在本教程中，我们将采取的梯度下降策略是批梯度下降（Batch Gradient Descent）, 即每次迭代时都将在所有数据点上进行预测的损失累积起来得到整体损失后求平均，以此作为优化目标去计算梯度和优化参数。 这样的好处是可以避免噪声数据点带来的干扰，每次更新参数时会朝着整体更加均衡的方向去优化。 以及从计算效率角度来看，可以充分利用一种叫做 **向量化（Vectorization）** 的特性，节约时间（拓展材料中进行了验证）。"
msgstr "In this tutorial, the gradient descent strategy we will adopt is Batch Gradient Descent, that is, at each iteration, the losses predicted on all data points are accumulated to obtain the overall loss and then averaged, which is used as the The optimization objective is to compute gradients and optimize parameters. The advantage of this is that the interference caused by noisy data points can be avoided, and each time the parameters are updated, they will be optimized in a more balanced direction. And from a computational efficiency point of view, you can take advantage of a feature called **Vectorization** to save time (verified in the extended material)."

#: ../../source/getting-started/beginner/megengine-basics.rst:404
msgid "设计与实现损失函数"
msgstr "Design and implement a loss function"

#: ../../source/getting-started/beginner/megengine-basics.rst:406
msgid "对于这样的模型，如何度量输出值 :math:`\\hat{y} = f(x)` 与真实值 :math:`y` 之间的损失 :math:`l` 呢？ 请顺着下面的思路进行思考："
msgstr "For such a model, how to measure the loss :math:`l` between the output value :math:`\\hat{y} = f(x)` and the true value :math:`y`? Please think along the following lines of thought："

#: ../../source/getting-started/beginner/megengine-basics.rst:409
msgid "最容易想到的做法是直接计算误差（Error），即对每个 :math:`(x_i, y_i)` 和 :math:`\\hat{y_i}` 有 :math:`l_i = l(\\hat{y_i},y_i) = \\hat{y_i} - y_i`."
msgstr "The easiest way to think of it is to directly calculate the error (Error), that is, for every :math:`(x_i, y_i)` and :math:`\\hat{y_i}` there are :math:`l_i = l(\\hat{y_i},y_i) = \\hat{y_i} - y_i`."

#: ../../source/getting-started/beginner/megengine-basics.rst:412
msgid "这样的想法很自然，问题在于对于回归问题，上述形式得到的损失 :math:`l_i` 是有正有负的， 在我们计算平均损失 :math:`l = \\frac{1}{n} \\sum_{i}^{n} （\\hat{y_i} - y_i)` 时会将一些正负值进行抵消， 比如对于 :math:`y_1 = 50, \\hat{y_1} = 100` 和 :math:`y2 = 50, \\hat{y_2} = 0`, 得到的平均损失为 :math:`l = \\frac{1}{2} \\big( (100 - 50) + (0 - 50) \\big) = 0`, 这并不是我们想要的效果。"
msgstr "This kind of thinking is natural, the problem is that for regression problems, the loss :math:`l_i` obtained in the above form is positive and negative, when we calculate the average loss :math:`l = \\frac{1}{n} \\sum_{i}^{n} (\\hat{y_i} - y_i)` will cancel some positive and negative values, for example, for :math:`y_1 = 50, \\hat{y_1} = 100` and :math:`y2 = 50, \\hat{y_2} = 0`, the average loss is :math:`l = \\frac{1}{2} \\big( (100 - 50) + (0 - 50) \\big) = 0`, which is not what we want."

#: ../../source/getting-started/beginner/megengine-basics.rst:417
msgid "我们希望单个样本上的误差应该是可累积的，因此它需要是正值，同时方便后续计算。"
msgstr "We hope that the error on a single sample should be cumulative, so it needs to be positive while facilitating subsequent calculations."

#: ../../source/getting-started/beginner/megengine-basics.rst:419
msgid "可以尝试的改进是使用平均绝对误差（Mean Absolute Error, MAE）: :math:`l = \\frac{1}{n} \\sum_{i}^{n} |\\hat{y_i} - y_i|`"
msgstr "An improvement to try is to use Mean Absolute Error (MAE): :math:`l = \\frac{1}{n} \\sum_{i}^{n} |\\hat{y_i} - y_i|`"

#: ../../source/getting-started/beginner/megengine-basics.rst:421
msgid "但注意到，我们优化模型使用的是梯度下降法，这要求目标函数（即损失函数）尽可能地连续可导，且易于求导和计算。 因此我们在回归问题中更常见的损失函数是平均平方误差（Mean Squared Error, MSE）:"
msgstr "But note that we use gradient descent to optimize the model, which requires the objective function (ie, the loss function) to be as continuously derivable as possible, and easy to derive and calculate. So our more common loss function in regression problems is Mean Squared Error (MSE):"

#: ../../source/getting-started/beginner/megengine-basics.rst:424
msgid "l = \\frac{1}{n} \\sum_{i}^{n} (\\hat{y_i} - y_i)^2"
msgstr "l = \\frac{1}{n} \\sum_{i}^{n} (\\hat{y_i} - y_i)^2"

#: ../../source/getting-started/beginner/megengine-basics.rst:430
msgid "一些机器学习课程中可能会为了方便求导时抵消掉平方带来的系数 2，在前面乘上 :math:`\\frac{1}{2}`, 本教程中没有这样做（因为 MegEngine 支持自动求导，可以和手动求导过程的代码进行对比）；"
msgstr "In some machine learning courses, for the convenience of derivation, the coefficient 2 brought by the square may be offset, and multiplied by :math:`\\frac{1}{2}` in front, which is not done in this tutorial (because MegEngine supports automatic derivation, which can be compared with manual Compare the code of the derivation process);"

#: ../../source/getting-started/beginner/megengine-basics.rst:433
msgid "另外我们可以从概率统计视角解释为何选用 MSE 作为损失函数： 假定误差满足平均值 :math:`\\mu = 0` 的正态分布，那么 MSE 就是对参数的极大似然估计。 详细的解释可以看 CS229 的 `讲义 <https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf>`_ 。"
msgstr "In addition, we can explain why MSE is chosen as the loss function： from the perspective of probability and statistics. Assuming that the error satisfies the normal distribution of the mean :math:`\\mu = 0`, then MSE is the maximum likelihood estimation of the parameters. For a detailed explanation, see `Lecture <https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf>`_ of CS229."

#: ../../source/getting-started/beginner/megengine-basics.rst:438
msgid "如果你不了解上面这几点细节，不用担心，这不会影响到我们完成本教程的任务。"
msgstr "If you don't understand the above details, don't worry, it won't affect our task of completing this tutorial."

#: ../../source/getting-started/beginner/megengine-basics.rst:440
msgid "我们假定现在通过模型得到了 4 个样本上的预测结果 ``pred``, 现在来计算它与真实值 ``real`` 之间的 MSE 损失："
msgstr "We assume that the prediction result ``pred`` on 4 samples is now obtained by the model, and now calculate the MSE loss：between it and the real value ``real``"

#: ../../source/getting-started/beginner/megengine-basics.rst:448
msgid "在 MegEngine 中对常见的损失函数也进行了封装，这里我们可以使用 :func:`~.nn.square_loss`:"
msgstr "Common loss functions are also encapsulated in MegEngine, here we can use :func:`~.nn.square_loss`:"

#: ../../source/getting-started/beginner/megengine-basics.rst:454
msgid "注意：由于损失函数（Loss function）是深度学习中提出的概念，因此相关接口应当通过 :mod:`.functional.nn` 调用。"
msgstr "Note：Since the loss function is a concept proposed in deep learning, the related interface should be called through :mod:`.functional.nn`."

#: ../../source/getting-started/beginner/megengine-basics.rst:458
msgid "如果你不理解上面的操作，请参考 :ref:`element-wise-operations` 或浏览对应的 API 文档；"
msgstr "If you do not understand the above operations, please refer to :ref:`element-wise-operations` or browse the corresponding API documentation;"

#: ../../source/getting-started/beginner/megengine-basics.rst:459
msgid "更多的常见损失函数，可以在 :ref:`loss-functions` 找到。"
msgstr "More common loss functions can be found at :ref:`loss-functions`."

#: ../../source/getting-started/beginner/megengine-basics.rst:462
msgid "完整代码实现"
msgstr "Complete code implementation"

#: ../../source/getting-started/beginner/megengine-basics.rst:464
msgid "我们同时给出 NumPy 实现和 MegEngine 实现作为对比："
msgstr "We also give the NumPy implementation and the MegEngine implementation as a comparison："

#: ../../source/getting-started/beginner/megengine-basics.rst:466
msgid "在 NumPy 实现中需要手动推导 :math:`\\frac{\\partial l}{\\partial w}` 与 :math:`\\frac{\\partial l}{\\partial b}`, 而在 MegEngine 中只需要调用 ``gm.backward(loss)`` 即可;"
msgstr "In the NumPy implementation, :math:`\\frac{\\partial l}{\\partial w}` and :math:`\\frac{\\partial l}{\\partial b}` need to be manually deduced, while in MegEngine just call ``gm. backward(loss)`` can;"

#: ../../source/getting-started/beginner/megengine-basics.rst:468
msgid "输入数据 :math:`x` 是形状为 :math:`(100,)` 的向量（1 维数组）， 与标量 :math:`w` 和 :math:`b` 进行运算时，后者会广播到相同的形状，再进行计算。 这样利用了向量化的特性，计算效率更高，相关细节可以参考 :ref:`tensor-broadcasting` 。"
msgstr "Input data :math:`x` is a vector (1D array) of shape :math:`(100,)`, when operating with scalars :math:`w` and :math:`b`, the latter is broadcast to the same shape, and then calculate. In this way, the vectorization feature is used, and the calculation efficiency is higher. For details, please refer to :ref:`tensor-broadcasting` ."

#: ../../source/getting-started/beginner/megengine-basics.rst:477
msgid "NumPy"
msgstr "NumPy"

#: ../../source/getting-started/beginner/megengine-basics.rst:518
msgid "MegEngine"
msgstr "MegEngine"

#: ../../source/getting-started/beginner/megengine-basics.rst:555
msgid "二者应该会得到一样的输出。"
msgstr "Both should get the same output."

#: ../../source/getting-started/beginner/megengine-basics.rst:557
msgid "由于我们使用的是批梯度下降策略，每次迭代（Iteration）都是基于所有数据计算得到的平均损失和梯度进行的。 为了进行多次迭代，我们要重复多趟（Epochs）训练（把数据完整过一遍，称为完成一个 Epoch 的训练）。 而在批梯度下降策略下，每趟训练参数只会更新一个 Iter, 后面我们会遇到一个 Epoch 迭代多次的情况， 这些术语在深度学习领域的交流中非常常见，会在后续的教程中被反复提到。"
msgstr "Since we are using a batch gradient descent strategy, each iteration (Iteration) is based on the average loss and gradient computed over all the data. In order to perform multiple iterations, we have to repeat the training multiple times (Epochs) (passing the data completely, called completing an Epoch training). Under the batch gradient descent strategy, only one Iter is updated for each training parameter. Later, we will encounter a situation where an Epoch iterates multiple times. These terms are very common in the communication in the field of deep learning and will be used in subsequent tutorials. mentioned repeatedly."

#: ../../source/getting-started/beginner/megengine-basics.rst:562
msgid "可以发现，经过 5 趟训练（经给定任务 T过 5 次迭代），我们的损失在不断地下降，参数 :math:`w` 和 :math:`b` 也在不断变化。"
msgstr "It can be found that after 5 epochs of training (5 iterations for a given task T), our loss is constantly decreasing, and the parameters :math:`w` and :math:`b` are also constantly changing."

#: ../../source/getting-started/beginner/megengine-basics.rst:572
msgid "通过一些可视化手段，可以直观地看到我们的直线拟合程度还是很不错的。"
msgstr "Through some visualization means, we can intuitively see that our straight line fitting degree is still very good."

#: ../../source/getting-started/beginner/megengine-basics.rst:576
msgid "这是我们 MegEngine 之旅的一小步，我们已经成功地用 MegEngine 完成了直线拟合的任务！"
msgstr "This is a small step in our MegEngine journey, we have successfully completed the task of line fitting with MegEngine!"

#: ../../source/getting-started/beginner/megengine-basics.rst:580
msgid "本教程的对应源码： :docs:`examples/beginner/megengine-basic-fit-line.py`"
msgstr "The corresponding source code of this tutorial： :docs:`examples/beginner/megengine-basic-fit-line.py`"

#: ../../source/getting-started/beginner/megengine-basics.rst:583
msgid "总结：一元线性回归"
msgstr "Summary：Unary Linear Regression"

#: ../../source/getting-started/beginner/megengine-basics.rst:585
msgid "我们尝试用专业的术语来定义：回归分析只涉及到两个变量的，称一元回归分析。 如果只有一个自变量 :math:`X`, 而且因变量 :math:`Y` 和自变量 :math:`X` 之间的数量变化关系呈近似线性关系， 就可以建立一元线性回归方程，由自变量 :math:`X` 的值来预测因变量 :math:`Y` 的值，这就是一元线性回归预测。 一元线性回归模型 :math:`y_{i}=\\alpha+\\beta x_{i}+\\varepsilon_{i}` 是最简单的机器学习模型，非常适合入门。 其中随机扰动项 :math:`\\varepsilon_{i}` 是无法直接观测的随机变量，也即我们上面生成数据时引入的噪声。 我们根据观察已有的数据点去学习出 :math:`w` 和 :math:`b`, 得到了样本回归方程 :math:`\\hat{y}_{i}= wx_{i}+b` 作为一元线性回归预测模型。"
msgstr "We try to use professional terms to define：regression analysis involving only two variables, called univariate regression analysis. If there is only one independent variable :math:`X`, and the quantitative relationship between the dependent variable :math:`Y` and the independent variable :math:`X` is approximately linear, a univariate linear regression equation can be established, from the independent variable :math:`X` to predict the value of the dependent variable :math:`Y`, which is the univariate linear regression prediction. The univariate linear regression model :math:`y_{i}=\\alpha+\\beta x_{i}+\\varepsilon_{i}` is the simplest machine learning model and is great for getting started. The random disturbance term :math:`\\varepsilon_{i}` is a random variable that cannot be directly observed, that is, the noise introduced when we generate the data above. We learn :math:`w` and :math:`b` according to the existing data points, and get the sample regression equation :math:`\\hat{y}_{i}= wx_{i}+b` as a univariate linear regression prediction model."

#: ../../source/getting-started/beginner/megengine-basics.rst:593
msgid "一元线性回归方程的参数估计通常会用到最小平方法（也叫最小二乘法，Least squares method） 求解正规方程的形式去求得解析解（Closed-form expression），本教程不会介绍这种做法； 我们这里选择的方法是使用梯度下降法去迭代优化调参， 一是为了展示 MegEngine 中的基本功能如 :class:`~.GradManager` 和 :class:`~.Optimizer` 的使用， 二是为了以后能够更自然地对神经网络这样的非线性模型进行参数优化，届时最小二乘法将不再适用。"
msgstr "The parameter estimation of the univariate linear regression equation usually uses the least squares method (also called the least squares method, Least squares method) to solve the form of the normal equation to obtain the closed-form expression. This method will not be introduced in this tutorial. ; The method we choose here is to use the gradient descent method to iteratively optimize the parameters. One is to show the use of basic functions in MegEngine such as :class:`~.GradManager` and :class:`~.Optimizer`, and the second is to be more natural in the future. In order to optimize the parameters of nonlinear models such as neural networks, the least squares method will no longer be applicable."

#: ../../source/getting-started/beginner/megengine-basics.rst:599
msgid "这时候可以提及 Tom Mitchell 在 《 `Machine Learning <http://www.cs.cmu.edu/~tom/mlbook.html>`_ :footcite:p:`10.5555/541177`》 一书中对 “机器学习” 的定义："
msgstr "At this time, you can refer to Tom Mitchell's definition of \"machine learning\" in \"`Machine Learning <http://www.cs.cmu.edu/~tom/mlbook.html>`_ :footcite:p:`10.5555/541177`\"："

#: ../../source/getting-started/beginner/megengine-basics.rst:603
msgid "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."
msgstr "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."

#: ../../source/getting-started/beginner/megengine-basics.rst:607
msgid "如果一个计算机程序能够根据经验 E 提升在某类任务 T 上的性能 P, 则我们说程序从经验 E 中进行了学习。"
msgstr "A computer program is said to have learned from experience E if it can improve its performance P on some class of tasks T based on experience E."

#: ../../source/getting-started/beginner/megengine-basics.rst:610
msgid "在本教程中，我们的任务 T 是尝试拟合一条直线，经验 E 来自于我们已有的数据点， 根据数据点的分布，我们自然而然地想到了选择一元线性模型来预测输出， 我们评估模型好坏（性能 P）时用到了 MSE 损失作为目标函数，并用梯度下降算法来优化损失。 在下一个教程中，我们将接触到多元线性回归模型，并对机器学习的概念有更加深刻的认识。 在此之前，你可能需要花费一些时间去消化吸收已经出现的知识，多多练习。"
msgstr "In this tutorial, our task T is to try to fit a straight line. The experience E comes from the data points we have. According to the distribution of data points, we naturally think of choosing a univariate linear model to predict the output. We evaluate the model well When it is bad (performance P), the MSE loss is used as the objective function, and the gradient descent algorithm is used to optimize the loss. In the next tutorial, we will be exposed to multiple linear regression models and gain a deeper understanding of machine learning concepts. Before that, you may need to spend some time digesting and absorbing the knowledge that has emerged, and practice a lot."

#: ../../source/getting-started/beginner/megengine-basics.rst:616
msgid "任务，模型与优化算法"
msgstr "Tasks, Models and Optimization Algorithms"

#: ../../source/getting-started/beginner/megengine-basics.rst:618
msgid "机器学习领域有着非常多种类的模型，优化算法也并非只有梯度下降这一种。 我们在后面的教程中会接触到多元线性回归模型、以及线性分类模型， 从线性模型过渡到深度学习中的全连接神经网络模型； 不同的模型适用于不同的机器学习任务，因此模型选择很重要。 深度学习中使用的模型被称为神经网络，神经网络的魅力之一在于： 它能够被应用于许多任务，并且有时候能取得比传统机器学习模型好很多的效果。 但它模型结构并不复杂，优化模型的流程和本教程大同小异。 回忆一下，任何神经网络模型都能够表达成计算图，而我们已经初窥其奥妙。"
msgstr "There are many types of models in the field of machine learning, and gradient descent is not the only optimization algorithm. We will be exposed to multiple linear regression models and linear classification models in later tutorials, transitioning from linear models to fully connected neural network models in deep learning; different models are suitable for different machine learning tasks, so model selection is very important . The models used in deep learning are called neural networks, and one of the： of neural networks is that they can be applied to many tasks, and sometimes achieve much better results than traditional machine learning models. However, its model structure is not complicated, and the process of optimizing the model is similar to this tutorial. Recall that any neural network model can be expressed as a computational graph, and we've already seen it for the first time."

#: ../../source/getting-started/beginner/megengine-basics.rst:627
msgid "尝试调整超参数"
msgstr "Try tuning hyperparameters"

#: ../../source/getting-started/beginner/megengine-basics.rst:629
msgid "我们提到了一些概念如超参数（Hyperparameter），超参数是需要人为进行设定，通常无法由模型自己学得的参数。 你或许已经发现了，我们在每次迭代参数 :math:`w` 和 :math:`b` 时，使用的是同样的学习率。 经过 5 次迭代后，参数 :math:`w` 已经距离理想情况很接近了，而参数 :math:`b` 还需继续更新。 尝试改变 `lr` 的值，或者增加训练的 `Epoch` 数，看损失值能否进一步地降低。"
msgstr "We mentioned some concepts such as hyperparameters. Hyperparameters are parameters that need to be set manually and usually cannot be learned by the model itself. As you may have noticed, we are using the same learning rate for each iteration with parameters :math:`w` and :math:`b`. After 5 iterations, the parameter :math:`w` is very close to the ideal situation, and the parameter :math:`b` needs to continue to be updated. Try changing the value of `lr`, or increase the number of `Epoch` for training, and see if the loss can be reduced further."

#: ../../source/getting-started/beginner/megengine-basics.rst:634
msgid "损失越低，一定意味着越好吗？"
msgstr "Does lower loss necessarily mean better?"

#: ../../source/getting-started/beginner/megengine-basics.rst:636
msgid "既然我们选择了将损失作为优化目标，理想情况下我们的模型应该拟合现有数据中尽可能多的个点来降低损失。 但局限之处在于，我们得到的这些点始终是训练数据，对于一个机器学习任务， 我们可能会在训练模型时使用数据集 A, 而在实际使用模型时用到了来自现实世界的数据集 B. 在这种时候，将训练模型时的损失优化到极致反而可能会导致过拟合（Overfitting）。"
msgstr "Since we have chosen the loss as the optimization objective, ideally our model should fit as many points in the existing data as possible to reduce the loss. But the limitation is that the points we get are always training data, and for a machine learning task, we might use dataset A when training the model, and dataset B from the real world when actually using the model. At this time, optimizing the loss when training the model to the extreme may lead to overfitting."

#: ../../source/getting-started/beginner/megengine-basics.rst:643
msgid "Christopher M Bishop `Pattern Recognition and Machine Learning <https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>`_ :footcite:p:`10.5555/1162264` - Figure 1.4"
msgstr "Christopher M Bishop `Pattern Recognition and Machine Learning <https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>`_ :footcite:p:`10.5555/1162264` - Figure 1.4"

#: ../../source/getting-started/beginner/megengine-basics.rst:647
msgid "上图中的数据点分布其实来自于三角函数加上一些噪声，我们选择多项式回归模型并进行优化， 希望多项式曲线能够尽可能拟合数据点。可以发现当迭代次数过多时，会出现最后一张图的情况。 这个时候虽然在现有数据点上的拟合程度达到了百分百（损失为 0），但对于新输入的数据， 其预测性能可能还不如早期的训练情况。因此，不能光靠训练过程中的损失函数来作为模型性能的评估指标。"
msgstr "The distribution of data points in the above figure actually comes from the trigonometric function plus some noise. We choose a polynomial regression model and optimize it, hoping that the polynomial curve can fit the data points as much as possible. It can be found that when there are too many iterations, the last picture will appear. At this time, although the fitting degree on the existing data points has reached 100% (loss is 0), the prediction performance for the newly input data may not be as good as the early training situation. Therefore, the loss function in the training process cannot be used as an evaluation indicator for model performance."

#: ../../source/getting-started/beginner/megengine-basics.rst:652
msgid "我们在后续的教程中，会给出更加科学的解决方案。"
msgstr "We will give more scientific solutions in subsequent tutorials."

#: ../../source/getting-started/beginner/megengine-basics.rst:655
msgid "拓展材料"
msgstr "Expansion material"

#: ../../source/getting-started/beginner/megengine-basics.rst:659
msgid "在 NumPy 内部，向量化运算的速度是优于 for 循环的，我们很容易验证这一点："
msgstr "Inside：, vectorized operations are faster than for loops, and we can easily verify this."

#: ../../source/getting-started/beginner/megengine-basics.rst:689
msgid "背后是利用 SIMD 进行数据并行，互联网上有非常多博客详细地进行了解释，推荐阅读："
msgstr "Behind it is the use of SIMD for data parallelism. There are many blogs on the Internet that explain it in detail. It is recommended to read："

#: ../../source/getting-started/beginner/megengine-basics.rst:691
msgid "`Why is vectorization, faster in general, than loops? <https://stackoverflow.com/questions/35091979/why-is-vectorization-faster-in-general-than-loops>`_"
msgstr "`Why is vectorization, faster in general, than loops? <https://stackoverflow.com/questions/35091979/why-is-vectorization-faster-in-general-than-loops>`_"

#: ../../source/getting-started/beginner/megengine-basics.rst:693
msgid "`Nuts and Bolts of NumPy Optimization Part 1: Understanding Vectorization and Broadcasting <https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/>`_"
msgstr "`Nuts and Bolts of NumPy Optimization Part 1: Understanding Vectorization and Broadcasting <https://blog.paperspace.com/numpy-optimization-vectorization-and-broadcasting/>`_"

#: ../../source/getting-started/beginner/megengine-basics.rst:696
msgid "同样地，向量化的代码在 MegEngine 中也会比 for 循环写法更快，尤其是利用 GPU 并行计算时。"
msgstr "Likewise, vectorized code can be written faster in MegEngine than for loops, especially when using GPU parallelism."

#: ../../source/getting-started/beginner/megengine-basics.rst:700
msgid "Scikit-learn 是非常有名的 Python 机器学习库，里面实现了许多经典机器学习算法。 在 Scikit-learn 的模型选择文档中，给出了解释模型欠拟合和过拟合的代码："
msgstr "Scikit-learn is a very well-known Python machine learning library that implements many classic machine learning algorithms. In Scikit-learn's model selection documentation, code to explain model underfitting and overfitting is given："

#: ../../source/getting-started/beginner/megengine-basics.rst:703
msgid "https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html"
msgstr "https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html"

#: ../../source/getting-started/beginner/megengine-basics.rst:705
msgid "感兴趣的读者可以借此去了解一下 Scikit-learn, 我们在下一个教程中会用到它提供的数据集接口。"
msgstr "Interested readers can take this to learn about Scikit-learn, we will use the dataset interface provided by it in the next tutorial."

#: ../../source/getting-started/beginner/megengine-basics.rst:708
msgid "参考文献"
msgstr "references"

#~ msgid "那么我们要如何去评估一个模型预测性能的好坏呢？ 核心原则是： **犯错越少，也就越好。**"
#~ msgstr ""

#~ msgid ""
#~ "假设你得到了 100 个二维点 :math:`(x_i, y_i)`, 其中"
#~ " :math:`i \\in [0, 100)`, 希望将来给出输入数据 "
#~ ":math:`x`, 能够预测出合适的 :math:`y` 值。"
#~ msgstr ""

#~ msgid "一元线性回归模型 :math:`f(x)=w*x+b` 是最简单的机器学习模型，我们借助这个模型完成了直线拟合这一任务。"
#~ msgstr ""

#~ msgid ""
#~ "在本教程中，我们的任务 T 是尝试拟合一条直线，经验 E 来自于我们已有的数据点， "
#~ "根据数据点的分布，我们自然而然地想到了选择一元线性模型来预测输出， 我们评估模型好坏（性能 P）时用到了 "
#~ "MSE 损失作为目标函数，并用梯度下降算法来优化损失。"
#~ msgstr ""

#~ msgid ""
#~ "机器学习领域有着非常多种类的模型，优化算法也并非只有梯度下降这一种。 "
#~ "我们在后面的教程中会接触到多元线性回归模型、以及线性分类模型，从线性模型过渡到深度学习； "
#~ "不同的模型适用于不同的机器学习任务，因此模型选择很重要。 深度学习中使用的模型被称为神经网络，神经网络的魅力之一在于： "
#~ "它能够被应用于许多任务，并且有时候能取得比传统机器学习模型好很多的效果。 但它模型结构并不复杂，优化模型的流程和本教程大同小异。"
#~ " 回忆一下，任何神经网络模型都能够表达成计算图，而我们已经初窥其奥妙。"
#~ msgstr ""

#~ msgid ""
#~ "Charles R. Harris, K. Jarrod Millman,"
#~ " Stéfan J. van der Walt, Ralf "
#~ "Gommers, Pauli Virtanen, David Cournapeau, "
#~ "Eric Wieser, Julian Taylor, Sebastian "
#~ "Berg, Nathaniel J. Smith, Robert Kern,"
#~ " Matti Picus, Stephan Hoyer, Marten "
#~ "H. van Kerkwijk, Matthew Brett, Allan"
#~ " Haldane, Jaime Fernández del Río, "
#~ "Mark Wiebe, Pearu Peterson, Pierre "
#~ "Gérard-Marchant, Kevin Sheppard, Tyler "
#~ "Reddy, Warren Weckesser, Hameer Abbasi, "
#~ "Christoph Gohlke, and Travis E. "
#~ "Oliphant. Array programming with NumPy. "
#~ "Nature, 585(7825):357–362, September 2020. "
#~ "URL: https://doi.org/10.1038/s41586-020-2649-2, "
#~ "doi:10.1038/s41586-020-2649-2.Thomas M. Mitchell. "
#~ "Machine Learning. McGraw-Hill, Inc., "
#~ "USA, 1 edition, 1997. ISBN "
#~ "0070428077.Christopher M. Bishop. Pattern "
#~ "Recognition and Machine Learning (Information"
#~ " Science and Statistics). Springer-Verlag,"
#~ " Berlin, Heidelberg, 2006. ISBN 0387310738."
#~ msgstr ""
#~ "Charles R. Harris, K. Jarrod Millman,"
#~ " Stéfan J. van der Walt, Ralf "
#~ "Gommers, Pauli Virtanen, David Cournapeau, "
#~ "Eric Wieser, Julian Taylor, Sebastian "
#~ "Berg, Nathaniel J. Smith, Robert Kern,"
#~ " Matti Picus, Stephan Hoyer, Marten "
#~ "H. van Kerkwijk, Matthew Brett, Allan"
#~ " Haldane, Jaime Fernández del Río, "
#~ "Mark Wiebe, Pearu Peterson, Pierre "
#~ "Gérard-Marchant, Kevin Sheppard, Tyler "
#~ "Reddy, Warren Weckesser, Hameer Abbasi, "
#~ "Christoph Gohlke, and Travis E. "
#~ "Oliphant. Array programming with NumPy. "
#~ "Nature, 585(7825):357–362, September 2020. "
#~ "URL: https://doi.org/10.1038/s41586-020-2649-2, "
#~ "doi:10.1038/s41586-020-2649-2.Thomas M. Mitchell. "
#~ "Machine Learning. McGraw-Hill, Inc., "
#~ "USA, 1 edition, 1997. ISBN 0070428077."
#~ " Christopher M. Bishop. Pattern Recognition"
#~ " and Machine Learning (Information Science"
#~ " and Statistics). Springer-Verlag, Berlin,"
#~ " Heidelberg, 2006 . ISBN 0387310738."

