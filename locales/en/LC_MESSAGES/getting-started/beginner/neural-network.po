msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-12-24 23:51+0800\n"
"PO-Revision-Date: 2023-04-11 06:00\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/getting-started/beginner/neural-network.po\n"
"X-Crowdin-File-ID: 8388\n"

#: ../../source/getting-started/beginner/neural-network.rst:5
msgid "MegEngine 实现神经网络"
msgstr "MegEngine implements neural network"

#: ../../source/getting-started/beginner/neural-network.rst:6
msgid "本教程涉及的内容"
msgstr "What this tutorial covers"

#: ../../source/getting-started/beginner/neural-network.rst:9
msgid "思考线性模型的局限性，考虑如何解决线性不可分问题，引出 “激活函数” 的概念；"
msgstr "Think about the limitations of linear models, consider how to solve the linear inseparability problem, and introduce the concept of \"activation function\";"

#: ../../source/getting-started/beginner/neural-network.rst:10
msgid "对神经元（Neural）以及全连接（Fully connected）网络模型结构有一个基本理解；"
msgstr "Have a basic understanding of neural and fully connected network model structures;"

#: ../../source/getting-started/beginner/neural-network.rst:11
msgid "接触到不同的参数初始化策略，学会使用 :mod:`~.module` 模块提升模型设计效率；"
msgstr "Expose to different parameter initialization strategies, learn to use :mod:`~.module` module to improve model design efficiency;"

#: ../../source/getting-started/beginner/neural-network.rst:12
msgid "根据前面的介绍，使用 MegEngine 实现两层全连接神经网络，完成 Fashion-MNIST 图片分类任务。"
msgstr "According to the previous introduction, use MegEngine to implement a two-layer fully connected neural network to complete the Fashion-MNIST image classification task."

#: ../../source/getting-started/beginner/neural-network.rst:15
msgid "获取原始数据集"
msgstr "Get the original dataset"

#: ../../source/getting-started/beginner/neural-network.rst:17
msgid "在上一个教程中，我们使用了 MegEngine 的 :mod:`.data.dataset` 模块来获取 :class:`~.MNIST` 数据集， 并使用线性分类器取得了超过 90% 的分类准确率。接下来我们将在与之类似的 Fashion-MNIST 数据集上， 使用完全一样的模型结构和优化策略，看线性模型是否依旧能取得一样出色的效果。"
msgstr "In the last tutorial, we used MegEngine's :mod:`.data.dataset` module to obtain :class:`~.MNIST` dataset and achieved over 90% classification accuracy using a linear classifier. Next, we will use the exact same model structure and optimization strategy on the similar Fashion-MNIST dataset to see if the linear model can still achieve the same good results."

#: ../../source/getting-started/beginner/neural-network.rst:21
msgid "通常经过 5 轮训练，使用线性分类器（Logistic 回归模型）在 Fashion-MNIST 上能达到 83% 的准确率。"
msgstr "Usually after 5 rounds of training, using a linear classifier (Logistic regression model) can achieve 83% accuracy on Fashion-MNIST."

#: ../../source/getting-started/beginner/neural-network.rst:25
msgid "`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ :footcite:p:`xiao2017` 是一个替代MNIST手写数字集的图像数据集。它是由 Zalando（一家德国的时尚科技公司）旗下的研究部门提供。 其涵盖了来自 10 种类别的共 7 万个不同商品的正面图片。Fashion-MNIST 的大小、格式和训练集/测试集划分与原始的 MNIST 完全一致。 60000/10000的 训练测试数据划分，28x28 的灰度图片。你可以直接用它来测试你的机器学习和深度学习算法性能， 且 **不需要** 改动任何的代码。"
msgstr "`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ :footcite:p:`xiao2017` is an image dataset that replaces the MNIST handwritten digits set. It is provided by the research division of Zalando, a German fashion technology company. It covers a total of 70,000 different product front images from 10 categories. The size, format, and train/test split of Fashion-MNIST are exactly the same as the original MNIST. 60000/10000 division of training and testing data, 28x28 grayscale images. You can use it directly to test the performance of your machine learning and deep learning algorithms without **needing** to change any code."

#: ../../source/getting-started/beginner/neural-network.rst:31
msgid "这一部分的源码可以在 :docs:`examples/beginner/linear-classification-fashion.py` 找到，区别在于："
msgstr "The source code of this part can be found in :docs:`examples/beginner/linear-classification-fashion.py`, the difference is："

#: ../../source/getting-started/beginner/neural-network.rst:33
msgid "MegEngine 在 :mod:`.data.dataset` 模块提供了 :class:`~.MNIST` 接口，而 Fashion-MNIST 需要手动下载；"
msgstr "MegEngine provides :class:`~.MNIST` interface in :mod:`.data.dataset` module, while Fashion-MNIST needs to be downloaded manually;"

#: ../../source/getting-started/beginner/neural-network.rst:34
msgid "借助下面的 ``load_mnist`` 函数得到 ndarray 格式的数据集，需要借助 :class:`~.ArrayDataset` 进行封装。"
msgstr "Use the following ``load_mnist`` function to get a data set in ndarray format, which needs to be encapsulated by :class:`~.ArrayDataset`."

#: ../../source/getting-started/beginner/neural-network.rst:38
msgid "可以借助下面的代码直接对比两个源码文件之间的区别："
msgstr "You can use the following code to directly compare the difference between the two source code files："

#: ../../source/getting-started/beginner/neural-network.rst:45
msgid "这样的结果相较于人类水平其实并不算理想，我们需要设计出更好的机器学习模型。"
msgstr "Such results are not ideal compared to human level, and we need to design better machine learning models."

#: ../../source/getting-started/beginner/neural-network.rst:48
msgid "线性模型的局限性"
msgstr "Limitations of Linear Models"

#: ../../source/getting-started/beginner/neural-network.rst:50
msgid "线性模型比较简单，因此也存在着诸多局限性。 比如处理分类问题时，其生成的决策边界是一个超平面， 这意味着理想情况下，样本点在特征空间中是线性可分的。 最典型的反例是，它无法解决异或（XOR）运算问题："
msgstr "Linear models are simpler and therefore have many limitations. For example, when dealing with classification problems, the generated decision boundary is a hyperplane, which means that ideally, the sample points are linearly separable in the feature space. The most typical counter-example is that it cannot solve the exclusive-or (XOR) operation problem："

#: ../../source/getting-started/beginner/neural-network.rst:59
msgid "四个样本组成的数据集"
msgstr "A dataset of four samples"

#: ../../source/getting-started/beginner/neural-network.rst:64
msgid "输入 :math:`\\boldsymbol{x}_i`"
msgstr "Enter :math:`\\boldsymbol{x}_i`"

#: ../../source/getting-started/beginner/neural-network.rst:65
msgid "特征 :math:`x_{i1}`"
msgstr "feature :math:`{i1}"

#: ../../source/getting-started/beginner/neural-network.rst:66
msgid "特征 :math:`x_{i2}`"
msgstr "feature :math:`{i2}"

#: ../../source/getting-started/beginner/neural-network.rst:67
msgid "输出 :math:`y_i`"
msgstr "output :math:`y_i`"

#: ../../source/getting-started/beginner/neural-network.rst:69
msgid ":math:`\\boldsymbol{x}_1`"
msgstr ":math:`\\boldsymbol{x}_1`"

#: ../../source/getting-started/beginner/neural-network.rst:70
#: ../../source/getting-started/beginner/neural-network.rst:71
#: ../../source/getting-started/beginner/neural-network.rst:72
#: ../../source/getting-started/beginner/neural-network.rst:74
#: ../../source/getting-started/beginner/neural-network.rst:79
#: ../../source/getting-started/beginner/neural-network.rst:84
msgid "0"
msgstr "0"

#: ../../source/getting-started/beginner/neural-network.rst:73
msgid ":math:`\\boldsymbol{x}_2`"
msgstr ":math:`\\boldsymbol{x}_2`"

#: ../../source/getting-started/beginner/neural-network.rst:75
#: ../../source/getting-started/beginner/neural-network.rst:76
#: ../../source/getting-started/beginner/neural-network.rst:78
#: ../../source/getting-started/beginner/neural-network.rst:80
#: ../../source/getting-started/beginner/neural-network.rst:82
#: ../../source/getting-started/beginner/neural-network.rst:83
msgid "1"
msgstr "1"

#: ../../source/getting-started/beginner/neural-network.rst:77
msgid ":math:`\\boldsymbol{x}_3`"
msgstr ":math:`\\boldsymbol{x}_3`"

#: ../../source/getting-started/beginner/neural-network.rst:81
msgid ":math:`\\boldsymbol{x}_4`"
msgstr ":math:`\\boldsymbol{x}_4`"

#: ../../source/getting-started/beginner/neural-network.rst:87
msgid "二维空间表示"
msgstr "2D space representation"

#: ../../source/getting-started/beginner/neural-network.rst:91
msgid "由于数据线性不可分，我们找不到这样一个线性决策边界，能够将这两类样本很好地分隔开。"
msgstr "Since the data are linearly inseparable, we cannot find such a linear decision boundary that separates the two classes of samples well."

#: ../../source/getting-started/beginner/neural-network.rst:93
msgid "我们即将从线性模型过渡到神经网络模型，你会发现一切早已悄然发生。"
msgstr "We are about to transition from a linear model to a neural network model, and you will find that everything has already happened quietly."

#: ../../source/getting-started/beginner/neural-network.rst:97
msgid "引入非线性因素"
msgstr "Introduce nonlinear factors"

#: ../../source/getting-started/beginner/neural-network.rst:99
msgid "回忆一下，在上一个教程中，我们的线性分类器在前向计算时会有一个将线性预测映射到概率值的操作（链接函数）。 由于这一步计算对模型中样本的决策边界并没有影响，因此我们依旧可以认为这是广义线性模型（更术语的解释是， 我们对观测到的样本假设依旧服从某个指数族分布，本教程不会介绍太多的数学细节）。"
msgstr "Recall that from the last tutorial, our linear classifier had an operation (link function) that maps linear predictions to probability values in the forward computation. Since this step of calculation has no effect on the decision boundary of the samples in the model, we can still consider this as a generalized linear model (a more terminological interpretation is that we assume that the observed samples still obey an exponential family distribution, which is not discussed in this tutorial. would go into too many mathematical details)."

#: ../../source/getting-started/beginner/neural-network.rst:103
msgid "是时候告诉你一个秘密了：线性模型本身其实可以被看做是最简单的单层神经网络！"
msgstr "It's time to tell you a secret. The linear model itself can be：of as the simplest single-layer neural network!"

#: ../../source/getting-started/beginner/neural-network.rst:108
msgid "如果将 MNIST 图片样本展平后的特征向量 :math:`\\boldsymbol{x}` 看作是输入层（Input Layer）， 那么对于二分类问题，我们可以当做这里有一个神经元，负责完成线性预测 :math:`z=\\boldsymbol{w}^T \\boldsymbol{x} + b` 和链接函数 :math:`a=\\sigma(z)` 相关的计算。 在二分类问题中，输出层（Output Layer）只要一个神经元就可以完成工作，多分类问题则需要多个神经元。"
msgstr "If the MNIST image sample flattened feature vector :math:`\\boldsymbol{x}` is regarded as the input layer (Input Layer), then for the binary classification problem, we can regard it as a neuron here, responsible for completing the linear prediction :math:`z= \\boldsymbol{w}^T \\boldsymbol{x} + b` Calculations associated with link function :math:`a=\\sigma(z)`. In binary classification problems, the output layer only needs one neuron to do the job, while multi-classification problems require multiple neurons."

#: ../../source/getting-started/beginner/neural-network.rst:115
msgid "大脑的基本计算单元是神经元（Neuron）。在人类神经系统中可以发现大约 860 亿个神经元， 它们与大约 :math:`10^{14}` ~ :math:`10^{15}` 个突触（Synapses）相连。 下图显示了生物神经元（左）和常见数学模型（右）的示例。 每个神经元从其树突（Dendrites）接收输入信号，并沿其唯一的轴突（Axon）产生输出信号。 轴突最终分支出来并通过突触连接到其它神经元的突触。在神经元计算模型中， 沿着轴突传播的信号（如 :math:`x_0` ）将基于突触的突触强度（如 :math:`w_0` ） 与其它神经元的树突进行乘法交互（如 :math:`w_0 x_0`）。突触强度（也就是权重 :math:`w` ） 是可学习的，且可以控制一个神经元对另一个神经元的影响程度（以及可以控制方向， 比如正权重代表兴奋，负权重代表抑制）。在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。 如果最终之和高于某个阈值，那么神经元将会 **激活（Activation）** ，向其轴突输出一个峰值信号。"
msgstr "The basic computing unit of the brain is the neuron. About 86 billion neurons can be found in the human nervous system, which are connected with about :math:`10^{14}` ~ :math:`10^{15}` synapses. The figure below shows an example of a biological neuron (left) and a common mathematical model (right). Each neuron receives input signals from its dendrites (Dendrites) and produces output signals along its unique axon (Axon). The axons eventually branch out and connect to the synapses of other neurons through synapses. In a neuron computational model, a signal propagating along an axon (e.g. :math:` :math::math:w_0 x_0`). The synaptic strength (aka weight :math:`w` ) is learnable and can control the degree of influence (and direction) of one neuron on another neuron, such as positive weights for excitation and negative weights for inhibition. In the basic model, dendrites transmit signals to the cell body, where the signals add up. If the final sum is above a certain threshold, the neuron will activate, outputting a spike to its axon."

#: ../../source/getting-started/beginner/neural-network.rst:128
msgid "*（注：本段解释和图片引用自 CS231n 讲义中的* `部分材料 <https://cs231n.github.io/neural-networks-1/>`_ *。）*"
msgstr "*(Note：The explanations and pictures in this paragraph are quoted from <https://cs231n.github.io/neural-networks-1/>`section material 1`_* in the CS231n handout.)*"

#: ../../source/getting-started/beginner/neural-network.rst:132
msgid "对于多分类问题，单层的神经网络结构显示如下（这里假定输入数据特征维度为 16）："
msgstr "For multi-classification problems, the single-layer neural network structure is shown below (it is assumed that the input data feature dimension is 16)："

#: ../../source/getting-started/beginner/neural-network.rst:136
msgid "图中输出层中的每个神经元都需要完成线性计算与非线性的映射。"
msgstr "Each neuron in the output layer in the figure needs to complete linear computation and nonlinear mapping."

#: ../../source/getting-started/beginner/neural-network.rst:138
msgid "*（注：本图片使用* `NN SVG <multilayer perceptron>`_ *制作生成。）*"
msgstr "*(Note：This image is generated using *`NN SVG <multilayer perceptron>`_*.)*"

#: ../../source/getting-started/beginner/neural-network.rst:141
msgid "激活函数与隐藏层"
msgstr "Activation function and hidden layer"

#: ../../source/getting-started/beginner/neural-network.rst:143
msgid "在神经元计算模型中，非线性函数被称为激活函数（Activation function）， 历史上常被使用的激活函数即我们见过的 Sigmoid 函数 :math:`\\sigma(\\cdot)`. 这给我们带来了启发，激活函数是非线性的，这也意味着通过多个神经元突触之间的连接，给我们的计算模型引入了非线性。 我们可以通过引入一个隐藏层（Hidden Layer）来做到这一点："
msgstr "In the neuron computing model, the nonlinear function is called the activation function (Activation function), and the activation function that is often used in history is the Sigmoid function we have seen :math:`\\sigma(\\cdot)`. This brings us to For inspiration, the activation function is nonlinear, which also means that nonlinearity is introduced into our computational model through the connections between the synapses of multiple neurons. We can do this by introducing a Hidden Layer："

#: ../../source/getting-started/beginner/neural-network.rst:150
msgid "隐藏层中带有 12 个神经元，每个神经元需要负责进行相应的非线性计算，决定是否激活。"
msgstr "There are 12 neurons in the hidden layer, and each neuron needs to be responsible for the corresponding nonlinear calculation and decide whether to activate or not."

#: ../../source/getting-started/beginner/neural-network.rst:152
msgid "我们来规范一下术语，在神经网络模型中，上面的神经网络被称为 2 层全连接神经网络。 输入层是样本特征，不发生实际计算，因此不算入模型层数。 隐藏层可以有多个，每层的神经元个数需要人为设定。 由于线性层的神经元是与上一层的所有输入完全连接在一起的，因此也叫做全连接层（Full connected layer, FC Layer）。 正是因为在全连接层后面加上激活函数，借此让神经网络具有了非线性计算的能力。"
msgstr "Let's standardize the terminology, in the neural network model, the neural network above is called a 2-layer fully connected neural network. The input layer is a sample feature, and no actual calculation occurs, so it is not included in the number of model layers. There can be multiple hidden layers, and the number of neurons in each layer needs to be manually set. Since the neurons of the linear layer are completely connected with all the inputs of the previous layer, it is also called a fully connected layer (FC Layer). It is precisely because the activation function is added behind the fully connected layer that the neural network has the ability of nonlinear computing."

#: ../../source/getting-started/beginner/neural-network.rst:158
msgid "使用 MegEngine 来模拟一下这个计算过程，直觉感受一下（这里只关注形状变化）："
msgstr "Use MegEngine to simulate this calculation process and feel it intuitively (only focus on shape changes here)："

#: ../../source/getting-started/beginner/neural-network.rst:177
msgid "在 MegEngine 中，常见的非线性激活函数被实现在 :mod:`.functional.nn` 模块中。 由于神经网络中可能存在着大量的非线性计算，又不像分类器一样要求输出映射到概率区间， 因此更加常用的激活函数是 :func:`~.relu` 函数等，全称修正线性单元（Rectified linear unit，ReLU）。 相较于 :func:`~.sigmoid` 函数，它有着计算和求导简单的特点，又满足非线性计算的特性要求。 更加具体的解释，可以查看不同激活函数的 API 文档。"
msgstr "In MegEngine, common nonlinear activation functions are implemented in the :mod:`.functional.nn` module. Since there may be a large number of nonlinear calculations in the neural network, and unlike the classifier, which requires the output to be mapped to the probability interval, the more commonly used activation functions are the :func:`~. , ReLU). Compared with the :func:`~.sigmoid` function, it has the characteristics of simple calculation and derivation, and meets the characteristic requirements of nonlinear calculation. For a more specific explanation, check out the API documentation for the different activation functions."

#: ../../source/getting-started/beginner/neural-network.rst:183
msgid "深度学习领域，有许多针对激活函数的研究和设计，在本教程中方便起见，均使用 ReLU 激活函数。"
msgstr "In the field of deep learning, there are many researches and designs on activation functions. For the convenience of this tutorial, the ReLU activation function is used."

#: ../../source/getting-started/beginner/neural-network.rst:186
msgid "多层神经网络"
msgstr "Multilayer Neural Network"

#: ../../source/getting-started/beginner/neural-network.rst:188
msgid "除了激活函数的选用，定义全连接神经网络的步骤主要在设计隐藏层层数和每层神经元的个数。"
msgstr "In addition to the selection of the activation function, the steps to define a fully connected neural network are mainly to design the number of hidden layers and the number of neurons in each layer."

#: ../../source/getting-started/beginner/neural-network.rst:190
msgid "我们可以通过堆叠更多的隐藏层，使得模型具有更强的学习能力和表达能力。 从这个角度来看，线性模型中的变换（准确来说是仿射变换，但在这里我们强调的是非线性和线性的区别） 不论如何叠加，最终都可以用等效的变换来表示，即矩阵运算中 :math:`C=AB` 的形式。 尽管我们通过激活函数引入了非线性，但问题也随之出现了："
msgstr "We can make the model have stronger learning ability and expressive ability by stacking more hidden layers. From this point of view, the transformation in the linear model (accurately speaking, the affine transformation, but here we emphasize the difference between nonlinear and linear), no matter how superimposed, can finally be represented by an equivalent transformation, that is, a matrix The form of :math:`C=AB` in arithmetic. Although we introduce nonlinearity through the activation function, the problem also arises："

#: ../../source/getting-started/beginner/neural-network.rst:195
msgid "我们需要对更多的模型参数进行管理，本教程中会给出 MegEngine 中的解决方案；"
msgstr "We need to manage more model parameters, the solution in MegEngine will be given in this tutorial;"

#: ../../source/getting-started/beginner/neural-network.rst:196
msgid "神经网络模型理论上可以逼近任意函数，使用更深的网络通常意味着更强的近似能力。 但我们不能胡乱设计，还需要对模型中的参数量（计算量）以及模型最终的性能之间进行权衡取舍。"
msgstr "Neural network models can theoretically approximate arbitrary functions, and using deeper networks usually means stronger approximation capabilities. But we can't design randomly, and we also need to make trade-offs between the amount of parameters in the model (the amount of calculation) and the final performance of the model."

#: ../../source/getting-started/beginner/neural-network.rst:201
msgid "目前接触到的这种仅由全连接层（与激活函数）组成神经网络模型架构， 在某些材料中也被称为多层感知机（Multilayer perceptron, MLP）。 其出发点是针对感知机算法进行改进，进而得到了 MLP. 二者实质上指代相同。 我们解决二分类问题使用的是 Logistic 回归，因此没有采用感知机这种称呼。"
msgstr "At present, the neural network model architecture that is only composed of fully connected layers (and activation functions) is also called Multilayer perceptron (MLP) in some materials. The starting point is to improve the perceptron algorithm, and then get the MLP. The two essentially refer to the same. We use Logistic regression to solve the binary classification problem, so we do not use the term perceptron."

#: ../../source/getting-started/beginner/neural-network.rst:207
msgid "随机初始化策略"
msgstr "random initialization strategy"

#: ../../source/getting-started/beginner/neural-network.rst:209
msgid "我们需要注意到全连接层的一些特点，比如层中的每一个神经元都会与前一层中所有输入进行运算。 回忆一下，我们在介绍线性模型时，提到了模型中的参数将从一个初始值不断地进行迭代优化， 并且采取了最简单的初始化策略，全零初始化，即将模型中所有参数的值初始化为 0. 这种做法在单层模型输出 + 损失函数的情况下是有效的，但对多层神经网络而言将导致问题。"
msgstr "We need to pay attention to some characteristics of fully connected layers, such as each neuron in the layer will operate with all the inputs in the previous layer. Recall that when we introduced the linear model, we mentioned that the parameters in the model will be iteratively optimized from an initial value, and the simplest initialization strategy is adopted, all-zero initialization, that is, the values of all parameters in the model are initialized to 0 . This approach works in the case of single-layer model output + loss function, but will cause problems for multi-layer neural networks."

#: ../../source/getting-started/beginner/neural-network.rst:214
msgid "假设我们将隐藏层中的所有神经元参数初始化为零，这其实意味着所有的神经元都在做同样的事情："
msgstr "Suppose we initialize all the neuron parameters in the hidden layer to zero, which actually means that all the neurons are doing the same："

#: ../../source/getting-started/beginner/neural-network.rst:216
msgid "前向计算时，由于前一层的输入是同样的，因此同一层的神经元的前向计算的输出都将一样;"
msgstr "During forward calculation, since the input of the previous layer is the same, the output of the forward calculation of the neurons in the same layer will be the same;"

#: ../../source/getting-started/beginner/neural-network.rst:217
msgid "经过激活函数时，由于激活函数 ReLU 没有随机性，因此也会得到同样的输出，并传给下一层；"
msgstr "When passing through the activation function, since the activation function ReLU has no randomness, the same output will be obtained and passed to the next layer;"

#: ../../source/getting-started/beginner/neural-network.rst:218
msgid "反向计算时，所有的参数将得到同样的梯度，在学习率一致的情况下，参数更新后也是一致的。"
msgstr "In reverse calculation, all parameters will get the same gradient, and if the learning rate is the same, the parameters will be the same after updating."

#: ../../source/getting-started/beginner/neural-network.rst:220
msgid "这就导致全连接层中的所有神经元都在做一样的事情，表达能力大大减弱。解决办法是，使用随机初始化策略。"
msgstr "This results in that all neurons in the fully connected layer are doing the same thing, and the ability to express is greatly reduced. The solution is to use a random initialization strategy."

#: ../../source/getting-started/beginner/neural-network.rst:222
msgid "MegEngine 生成随机数据"
msgstr "MegEngine generates random data"

#: ../../source/getting-started/beginner/neural-network.rst:224
msgid "在 MegEngine 中提供了 :mod:`~.megengine.random` 模块来随机生成 Tensor 数据："
msgstr ":mod:`~.megengine.random` module is provided in MegEngine to randomly generate Tensor："

#: ../../source/getting-started/beginner/neural-network.rst:238
msgid "其中 :func:`.random.seed` 可以设置一个随机种子，方便在一些情况下复现随机状态。 我们借助 :func:`~.random.normal` 接口从标准正态分布（均值为 0, 标准差为 1） 中随机生成了形状为 :math:`(3, 3)` 的 Tensor."
msgstr "Among them, :func:`.random.seed` can set a random seed, which is convenient to reproduce the random state in some cases. We randomly generated a Tensor of shape :math:`(3, 3)` from a standard normal distribution (mean 0, standard deviation 1) using the :func:`~.random.normal` interface."

#: ../../source/getting-started/beginner/neural-network.rst:244
msgid "假定我们需要在原来线性分类模型的基础上加入一层神经元个数为 256 的隐藏层："
msgstr "Suppose we need to add a hidden layer：with 256 neurons to the original linear classification model"

#: ../../source/getting-started/beginner/neural-network.rst:266
msgid "可以明显地感受到，当模型结构变得越来越复杂时，代码中将充斥着大量的重复性内容。"
msgstr "It can be clearly felt that when the model structure becomes more and more complex, the code will be filled with a lot of repetitive content."

#: ../../source/getting-started/beginner/neural-network.rst:268
msgid "作为一个深度学习框架，MegEngine 自然需要提供一种方便的做法，来对模型进行设计。"
msgstr "As a deep learning framework, MegEngine naturally needs to provide a convenient way to design models."

#: ../../source/getting-started/beginner/neural-network.rst:271
msgid "使用 Module 定义模型"
msgstr "Use Module to define the model"

#: ../../source/getting-started/beginner/neural-network.rst:274
msgid "本教程这一小节的介绍比较精简，完整内容请参考 :ref:`module-guide` 。"
msgstr "The introduction of this section of this tutorial is relatively concise, please refer to :ref:`module-guide` for the complete content."

#: ../../source/getting-started/beginner/neural-network.rst:276
msgid "MegEngine 中的 :mod:`~.module` 模块为神经网络模型中的结构提供了一类抽象机制， 一切均为 :class:`~.module.Module` 类，除了为常见模块实现了默认的随机初始化策略外， 并提供了常见的方法（如即将介绍的 :meth:`.Module.parameters` ）。 这样可以方便用户专注于设计网络结构，从重复的参数初始化和管理方式等细节中解脱出来。"
msgstr "The :mod:`~.module` module in MegEngine provides a class of abstract mechanism for the structure in the neural network model, everything is :class:`~.module.Module` class, except for the implementation of the default random initialization strategy for common modules, And provides common methods (such as the upcoming :meth:`.Module.parameters`). This allows users to focus on designing the network structure, freeing them from details such as repetitive parameter initialization and management methods."

#: ../../source/getting-started/beginner/neural-network.rst:281
msgid "例如对于像 :func:`~.matmul` 这样的线性层运算，可以使用 :class:`~.module.Linear` 进行表示："
msgstr "For example, for a linear layer operation like :func:`~.matmul`, you can use :class:`~.module.Linear` to represent："

#: ../../source/getting-started/beginner/neural-network.rst:290
msgid "可以发现 ``fc`` 模块中有着对应形状的权重（Weight）和偏置（Bias）参数，且自动完成了初始化。"
msgstr "It can be found that the ``fc`` module has the weight (Weight) and bias (Bias) parameters of the corresponding shape, and the initialization is automatically completed."

#: ../../source/getting-started/beginner/neural-network.rst:298
msgid "查阅 API 文档可知，对于输入的样本 :math:`x`, 计算过程是 :math:`y=xW^T+b`."
msgstr "According to the API documentation, for the input sample :math:`x`, the calculation process is :math:`y=xW^T+b`."

#: ../../source/getting-started/beginner/neural-network.rst:300
msgid "简单验证一下 :class:`~.module.Linear` 的运算结果，与 :func:`~.matmul` 得到的结果是一致的（在浮点误差范围内）："
msgstr "Simply verify that the operation result of :class:`~.module.Linear` is consistent with the result obtained by :func:`~.matmul` (within the floating point error range)："

#: ../../source/getting-started/beginner/neural-network.rst:332
msgid ":class:`~.module.Linear` 作为内置的 :class:`~.module.Module`, 我们不需要人为地初始化， 该过程在其对应的 ``__init__`` 方法中已经实现，也允许被用户自己的实现覆盖掉。 一些常见的初始化策略在 :mod:`.module.init` 模块中进行了提供，在本教程中不会进行具体介绍。"
msgstr ":class:`~.module.Linear` As a built-in :class:`~.module.Module`, we don't need to initialize it manually, this process has been implemented in its corresponding ``__init__`` method, and it is also allowed to be implemented by users themselves overwrite. Some common initialization strategies are provided in the :mod:`.module.init` module and will not be covered in detail in this tutorial."

#: ../../source/getting-started/beginner/neural-network.rst:336
msgid "Module 允许像搭积木一样进行嵌套实现，因此在本教程中的全连接神经网络可以这样实现："
msgstr "Module allows nested implementations like building blocks, so the fully connected neural network in this tutorial can be implemented：this."

#: ../../source/getting-started/beginner/neural-network.rst:338
msgid "所有网络结构都源自基类 ``M.Module``. 在构造函数中，一定要先调用 ``super().__init__()``."
msgstr "All network structures are derived from the base class ``M.Module``. In the constructor, you must first call ``super().__init__()``."

#: ../../source/getting-started/beginner/neural-network.rst:339
msgid "在构造函数中，声明要使用的所有层/模块；"
msgstr "In the constructor, declare all layers/modules to be used;"

#: ../../source/getting-started/beginner/neural-network.rst:340
msgid "在 ``forward`` 函数中，定义模型将如何运行，从输入到输出。"
msgstr "In the ``forward`` function, define how the model will run, from input to output."

#: ../../source/getting-started/beginner/neural-network.rst:355
msgid "可以借助 :meth:`.Module.parameters` 得到模型的参数的迭代器，提供给梯度管理器和优化器："
msgstr "An iterator of model parameters can be obtained with the help of :meth:`.Module.parameters`, which is provided to the gradient manager and optimizer："

#: ../../source/getting-started/beginner/neural-network.rst:361
msgid "练习：前馈神经网络"
msgstr "Practice：Feedforward Neural Networks"

#: ../../source/getting-started/beginner/neural-network.rst:363
msgid "前馈神经网络是一种最简单的神经网络，各神经元分层排列，每个神经元只与前一层的神经元相连。 接收前一层的输出，并输出给下一层，各层间没有反馈。是应用最广泛、发展最迅速的人工神经网络之一。"
msgstr "A feedforward neural network is the simplest type of neural network. Each neuron is arranged in layers, and each neuron is only connected to the neurons in the previous layer. Receive the output of the previous layer and output it to the next layer without feedback between layers. It is one of the most widely used and fastest-growing artificial neural networks."

#: ../../source/getting-started/beginner/neural-network.rst:366
msgid "简而言之，前馈神经网络中除了全连接层 :class:`~.module.Linear` 不再含有其它类型的结构， 让我们用 MegEngine 进行实现："
msgstr "In short, the feedforward neural network contains no other types of structures except the fully connected layer :class:`~.module.Linear`, let's implement it with MegEngine："

#: ../../source/getting-started/beginner/neural-network.rst:373
msgid "本教程的对应源码： :docs:`examples/beginner/neural-network.py`"
msgstr "The corresponding source code of this tutorial： :docs:`examples/beginner/neural-network.py`"

#: ../../source/getting-started/beginner/neural-network.rst:376
msgid "经过 5 轮训练，通常会得到一个正确率超过 83% （线性分类器）的神经网络模型。 在本教程中我们仅仅希望证明模型中引入非线性能具有更好的表达能力和预测性能， 因此没有花时间再去调整超参数并继续优化我们的模型。 Fashion-MNIST 数据集官方维护了一个 `基准测试 <http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/>`_ 结果， 可以发现其中有使用 MLPClassifier 得到的测试结果，能达到 87.7%. 我们可以根据相关模型的说明，进行模型和实验结果的复现，得到性能相当的神经网络模型。"
msgstr "After 5 rounds of training, a neural network model with an accuracy rate of over 83% (linear classifier) is usually obtained. In this tutorial we just want to demonstrate that introducing nonlinearity into the model leads to better expressiveness and predictive performance, so we didn't spend time tuning hyperparameters and continuing to optimize our model. The Fashion-MNIST dataset officially maintains a `benchmark test <http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/>result. It can be found that there are test results obtained by using MLPClassifier, which can reach 87.7%. We can reproduce the model and experimental results according to the description of the relevant model. A neural network model with comparable performance is obtained."

#: ../../source/getting-started/beginner/neural-network.rst:384
msgid "实际上，我们从接触到神经网络模型的那一刻开始，就已经开始进入 “调参（Tuning hyperparameters）” 模式了。 神经网络模型需要正确的代码和超参数设计，才能够取得非常好的效果。 对于刚刚接触到神经网络的 MegEngine 初学者，多尝试多编码是最推荐的提升手段。 在旷视科技研究院内部将训练一个模型的过程称为 “炼丹”，如今这个词已成为行业黑话。 在完成 MegEngine 入门教程的过程中，其实就是在积累最基本的炼丹经验。"
msgstr "In fact, from the moment we touch the neural network model, we have already started to enter the \"tuning hyperparameters\" mode. Neural network models require the right code and hyperparameter design to achieve very good results. For MegEngine beginners who have just come into contact with neural networks, trying more coding is the most recommended way to improve. Inside the Megvii Technology Research Institute, the process of training a model is called \"alchemy\", and now this term has become an industry slang. In the process of completing the MegEngine introductory tutorial, you are actually accumulating the most basic alchemy experience."

#: ../../source/getting-started/beginner/neural-network.rst:391
msgid "总结：再探计算图"
msgstr "Summary：and then explore the calculation diagram"

#: ../../source/getting-started/beginner/neural-network.rst:393
msgid "我们在第一篇教程中提到过计算图，现在来回忆一下："
msgstr "We mentioned computational graphs in the first tutorial, now let's recall："

#: ../../source/getting-started/beginner/neural-network.rst:395
msgid "MegEngine 是基于计算图（Computing Graph）的深度神经网络学习框架；"
msgstr "MegEngine is a deep neural network learning framework based on Computing Graph;"

#: ../../source/getting-started/beginner/neural-network.rst:396
msgid "在深度学习领域，任何复杂的深度神经网络模型本质上都可以用一个计算图表示出来。"
msgstr "In the field of deep learning, any complex deep neural network model can essentially be represented by a computational graph."

#: ../../source/getting-started/beginner/neural-network.rst:398
msgid "当我们运行本教程中的神经网络模型训练代码时，在脑海中可以去做进一步的想象， 将这个全连接神经网络表达成计算图，应该是什么样的？ 我们在可视化一个神经网络模型结构时，通常关注的是数据节点的变化过程， 但计算图中的计算节点，或者说算子（Operator），也十分关键。 假定我们的算子只能为 :func:`~.functional.matmul` / :class:`~.module.Linear` 这样的线性运算， 那么模型对输入数据的形状也会产生限制，它必须表达成一个特征向量（即 1-d 张量）。 当我们面临更加复杂的数据表征形式，比如 RGB 3 通道的彩色图片，继续使用全连接神经网络还能奏效吗？"
msgstr "When we run the neural network model training code in this tutorial, we can further imagine in our minds, what should this fully connected neural network be expressed as a computational graph? When we visualize the structure of a neural network model, we usually focus on the change process of data nodes, but the computing nodes, or operators, in the calculation graph are also very critical. Assuming that our operator can only be a linear operation such as :func:`~.functional.matmul` / :class:`~.module.Linear`, then the model will also impose restrictions on the shape of the input data, which must be expressed as a feature vector ( i.e. a 1-d tensor). When we face more complex data representation forms, such as RGB 3-channel color images, can we continue to use fully connected neural networks?"

#: ../../source/getting-started/beginner/neural-network.rst:406
msgid "我们在下个教程中会进行实验，接触到 CIFAR 10 彩色图片数据集，并看看全连接网络能否奏效。 届时我们会引入一种新的算子（暂时先保持神秘），发现设计神经网络其实就像搭积木一样，会有很多种不同的有效结构， 它们适用于不同的情景，而在设计算子时，一些传统领域的知识有时会很有帮助。"
msgstr "In the next tutorial we will experiment with the CIFAR 10 color image dataset and see if a fully connected network works. At that time, we will introduce a new operator (keep it mysterious for now), and find that designing a neural network is actually like building blocks, there will be many different effective structures, which are suitable for different scenarios, and when designing an operator , some traditional domain knowledge can sometimes be helpful."

#: ../../source/getting-started/beginner/neural-network.rst:411
msgid "拓展材料"
msgstr "Expansion material"

#: ../../source/getting-started/beginner/neural-network.rst:415
msgid "我们借助过可视化的形式去理解了计算机视觉中灰度图的基本表示。 可视化是我们理解数据，甚至是理解一门新知识、新概念的有效辅助手段。 我相信你已经理解了包括 “全连接神经网络（多层感知机）” 、 “梯度下降” 、“反向传播” 在内的深度学习基本概念， 而 3Blue1Brown 的视频可以作为非常好的补充："
msgstr "We have used the form of visualization to understand the basic representation of grayscale images in computer vision. Visualization is an effective aid for us to understand data, or even understand a new knowledge and concept. I believe you already understand the basic concepts of deep learning including \"Fully Connected Neural Network (Multilayer Perceptron)\", \"Gradient Descent\", \"Backpropagation\", and 3Blue1Brown's video can be a very good supplement："

#: ../../source/getting-started/beginner/neural-network.rst:428
msgid "更多 3Blue1Brown 的深度学习视频"
msgstr "More Deep Learning Videos from 3Blue1Brown"

#: ../../source/getting-started/beginner/neural-network.rst:430
msgid "`深度学习之神经网络的结构 <https://www.bilibili.com/video/BV1bx411M7Zx>`_"
msgstr "`The structure of deep learning neural network <https://www.bilibili.com/video/BV1bx411M7Zx>"

#: ../../source/getting-started/beginner/neural-network.rst:431
msgid "`深度学习之梯度下降法 <https://www.bilibili.com/video/BV1Ux411j7ri>`_"
msgstr "`Gradient descent method of deep learning <https://www.bilibili.com/video/BV1Ux411j7ri>`_"

#: ../../source/getting-started/beginner/neural-network.rst:432
msgid "`深度学习之直观理解反向传播 <https://www.bilibili.com/video/BV16x411V7Qg>`_"
msgstr "`Intuitive understanding of deep learning back propagation <https://www.bilibili.com/video/BV16x411V7Qg>"

#: ../../source/getting-started/beginner/neural-network.rst:436
msgid "`MNIST <https://www.kaggle.com/c/digit-recognizer/>`_ 和 `FashionMNIST <https://www.kaggle.com/zalando-research/fashionmnist>`_ 这样的经典数据集在 Kaggle 上也有提供，你可以尝试使用 MegEngine 实现全连接神经网络模型， 尝试调整模型结构（层数、每层神经元个数）和超参数（学习率、训练轮数等等）， 尽你所能地在这两个数据集上将准确率百分比点刷高，感受一下炼丹的魅力。"
msgstr "Classic datasets such as `MNIST <https://www.kaggle.com/c/digit-recognizer/>`_ and `FashionMNIST <https://www.kaggle.com/zalando-research/fashionmnist>`_ are also available on Kaggle, you can try to use MegEngine to implement a fully connected neural network model, try to adjust the model structure (number of layers, number of neurons per layer) and Hyperparameters (learning rate, number of training rounds, etc.), try your best to increase the accuracy percentage on these two data sets, and feel the charm of alchemy."

#: ../../source/getting-started/beginner/neural-network.rst:442
msgid "Kaggle 中也有许多其它人公开写好的代码，是非常不错的学习和借鉴材料。 你可能会看到一些人使用了另一种叫做 “卷积神经网络” 的模型取得了更好的效果， 这就是我们将会在下一个教程中接触到的新内容。"
msgstr "There are also many other openly written code in Kaggle, which is a very good learning and reference material. You may see some people get better results using another model called \"Convolutional Neural Networks\", and that's what we'll be covering in the next tutorial."

#: ../../source/getting-started/beginner/neural-network.rst:448
msgid "你可能在很多材料中听说过对人工智能发展历史的介绍，里面一定会提到人工智能的几次寒冬。"
msgstr "You may have heard the introduction to the history of artificial intelligence development in many materials, and it must mention several cold winters of artificial intelligence."

#: ../../source/getting-started/beginner/neural-network.rst:450
msgid "“感知机无法解决异或问题！” 这就是一次历史的转折点。"
msgstr "\"The perceptron can't solve the XOR problem!\" This was a turning point in history."

#: ../../source/getting-started/beginner/neural-network.rst:452
msgid "感知机（Perceptron）这是一种在 1950 年代末和 1960 年代初开发出的人工神经网络， 心理学家 Frank Rosenblatt 在 1957 年发布了第一个 “感知机” 模型， Marvin Minsky 和 Seymour Papert 在 1969 年出版了《感知器：计算几何导论》以作纪念。 但在这本书中对感知机做出了悲观预测，它甚至无法解决最简单的分类（异或）问题！ 这导致了当时的人工智能研究方向发生了转变，导致了 1980 年代的人工智能的冬天。"
msgstr "Perceptron This is an artificial neural network developed in the late 1950s and early 1960s, psychologist Frank Rosenblatt released the first \"perceptron\" model in 1957, Marvin Minsky and Seymour Papert in 1969 Published \"Introduction to Perceptron：Computational Geometry\" to commemorate. But the perceptron is pessimistically predicted in this book, it can't solve even the simplest classification (XOR) problem! This led to a shift in the direction of AI research at the time, leading to the AI winter of the 1980s."

#: ../../source/getting-started/beginner/neural-network.rst:458
msgid "尝试一下，设计出一个最简单的 XOR 神经网络模型，要求能够完全正确地得到输出。"
msgstr "Try it out and design the simplest possible XOR neural network model, which requires getting the output exactly right."

#: ../../source/getting-started/beginner/neural-network.rst:460
msgid "*（注：有兴趣的话，可以去了解一下人工智能的发展历史，一些纪录片和文章相当引人入胜。）*"
msgstr "*(Note：If you are interested, you can learn about the development history of artificial intelligence. Some documentaries and articles are quite fascinating.)*"

#: ../../source/getting-started/beginner/neural-network.rst:464
msgid "学习一个东西的最有效的办法之一是亲自去实现它（哪怕是 Naive 版本），有难度，也更花时间。"
msgstr "One of the most effective ways to learn something is to implement it yourself (even the Naive version), which is harder and more time consuming."

#: ../../source/getting-started/beginner/neural-network.rst:466
msgid "你可以尝试完全使用 NumPy 实现一个全连接神经网络（Kaggle 平台上有许多这样的例子）， 不用太复杂，两层就可以了。你可以选择参考 MegEngine 中已有的设计进行实现，也可以完全自由发挥。 不要求实现自动微分系统，可以手动地将各个算子的 ``backward`` 逻辑实现出来； 也不要求利用 GPU 进行加速（毕竟需要额外学习许多知识，比如 Nvidia 的 CUDA 编程）。"
msgstr "You can try to implement a fully connected neural network entirely in NumPy (there are many examples of this on the Kaggle platform), not too complicated, just two layers. You can choose to refer to the existing design in MegEngine to implement, or you can use it completely. It is not required to implement an automatic differentiation system, and the ``backward' logic of each operator can be implemented manually; nor does it require the use of GPU for acceleration (after all, a lot of additional knowledge is required, such as Nvidia's CUDA programming)."

#: ../../source/getting-started/beginner/neural-network.rst:472
msgid "参考文献"
msgstr "references"

#~ msgid ""
#~ "Han Xiao, Kashif Rasul, and Roland "
#~ "Vollgraf. Fashion-mnist: a novel image"
#~ " dataset for benchmarking machine learning"
#~ " algorithms. 2017. arXiv:cs.LG/1708.07747."
#~ msgstr ""

