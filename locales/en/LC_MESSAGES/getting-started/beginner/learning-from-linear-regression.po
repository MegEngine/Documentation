msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-21 10:52+0800\n"
"PO-Revision-Date: 2021-08-19 02:04\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/getting-started/beginner/learning-from-linear-regression.po\n"
"X-Crowdin-File-ID: 2842\n"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:9
msgid "一个稍微复杂些的线性回归模型"
msgstr "A slightly more complicated linear regression model"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:12
msgid "|image0| `在 MegStudio 运行 <https://studio.brainpp.com/project/4642>`__"
msgstr "|image0| `Run <https://studio.brainpp.com/project/4642>in MegStudio `__"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:16
msgid "image0"
msgstr "image0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:12
msgid "|image1| `查看源文件 <https://github.com/MegEngine/Documentation/blob/main/source/getting-started/beginner/learning-from-linear-regression.ipynb>`__"
msgstr "|image1| `View source file <https://github.com/MegEngine/Documentation/blob/main/source/getting-started/beginner/learning-from-linear-regression.ipynb>`__"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:17
msgid "image1"
msgstr "image1"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:28
msgid "我们已经学习了如何去训练一个简单的线性回归模型 :math:`y = w * x + b`, 接下来我们将稍微升级一下难度："
msgstr "We have learned how to train a simple linear regression model :math:`y = w * x + b`, next we will slightly upgrade the difficulty："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:30
msgid "我们对单个样本数据的表示形式将不再是简单的标量 :math:`x`, 而是升级为多维的向量 :math:`\\mathbf {x}` 表示；"
msgstr "We form a single sample of data will no longer be a simple scalar :math:`x`, but multi-dimensional vector upgrade :math:` \\ mathbf {x}`representation;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:31
msgid "我们将接触到更多不同形式的梯度下降策略，从而接触一个新的超参数 ``batch_size``;"
msgstr "We will be exposed to more different forms of gradient descent strategies, and thus a new hyperparameter ``batch_size'';"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:32
msgid "我们将尝试将数据集封装成 MegEngine 支持的 ``Dataset`` 类，方便在 ``Dataloader`` 中进行各种预处理操作；"
msgstr "We will try to encapsulate the data set into the ``Dataset`` class supported by MegEngine to facilitate various preprocessing operations in the ``Dataloader``;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:33
msgid "同时，我们的前向计算过程将变成矩阵运算形式，能够帮助你更好地理解向量化实现的优点；"
msgstr "At the same time, our forward calculation process will become a matrix operation form, which can help you better understand the advantages of vectorization;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:34
msgid "在这个过程中，一些遗留问题会得到解答，同时我们将接触到一些新的机器学习概念。"
msgstr "In this process, some remaining questions will be answered, and we will be exposed to some new machine learning concepts."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:36
msgid "本教程的话题将围绕着\\ **“数据（Data）”**\\ 进行，希望你能够有所收获～"
msgstr "The topic of this tutorial will revolve around \\ **\"Data\"**\\, I hope you can gain something~"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:38
msgid "请先运行下面的代码，检验你的环境中是否已经安装好 MegEngine（\\ `安装教程 <https://megengine.org.cn/doc/stable/zh/user-guide/install/>`__\\ ）："
msgstr "Please run the following code first to verify whether MegEngine has been installed in your environment (\\ `Installation Tutorial <https://megengine.org.cn/doc/stable/zh/user-guide/install/>`__\\)："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:80
msgid "接下来，我们将对将要使用的数据集 `波士顿房价数据集 <https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html>`__ 进行一定的了解。"
msgstr "Next, we will have a certain understanding of the data set `Boston House Price Data Set <https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html>`__ to be used."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:92
msgid "波士顿房价数据集的获取和分析"
msgstr "Acquisition and Analysis of Boston Housing Price Data Set"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:94
msgid "获取到真实的数据集原始文件后，往往需要做大量的数据格式处理工作，变成易于计算机理解的形式，才能被各种框架和库使用。"
msgstr "After obtaining the original file of the real data set, it is often necessary to do a lot of data format processing work and turn it into a form that is easy for the computer to understand before it can be used by various frameworks and libraries."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:96
msgid "但目前我们的重心不在此处，使用 Python 机器学习库 `scikit-learn <https://scikit-learn.org/stable/index.html>`__\\ ，可以快速地获得波士顿房价数据集:"
msgstr "But currently our focus is not here. Using the Python machine learning library `scikit-learn <https://scikit-learn.org/stable/index.html>`__\\, we can quickly get the Boston housing price data set:"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:151
msgid "我们已经认识过了数据 ``data`` 和标签 ``label``\\ （标签有时也叫目标 ``target`` ），那么 ``feature`` 是什么？"
msgstr "We have already known the data ``data`` and the label ``label``\\ (the label is sometimes called the target ``target``), so what is the ``feature``?"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:153
msgid "我们在描述一个事物的时候，通常会寻找其属性（Attribute）或者说特征（Feature）："
msgstr "When we describe a thing, we usually look for its attributes (Attribute) or features (Feature)："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:155
msgid "比如我们描述一只柴犬的长相，会说这只柴的鼻子如何、耳朵如何、毛发如何等等"
msgstr "For example, when we describe the appearance of a Shiba Inu, we will say what the nose, ears, hair and so on of this Shiba are."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:156
msgid "又比如在游戏宠物的属性经常有生命值、魔法值、攻击力、防御力等属性"
msgstr "For example, the attributes of game pets often have attributes such as health, magic, attack power, and defense power."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:157
msgid "类比到一些支持自定义角色的游戏，这些特征就变成许多可量化和调整的数据"
msgstr "Analogy to some games that support custom characters, these features become a lot of quantifiable and adjustable data"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:159
msgid "实际上，你并不需要在意波士顿房价数据集中选用了哪些特征，我们在本教程中关注的更多是“特征维度变多”这一情况。"
msgstr "In fact, you don't need to care about which features are used in the Boston housing price data set. In this tutorial, we are more concerned with the situation of \"increasing feature dimensions\"."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:161
msgid "为了方便后续的交流，我们需要引入一些数学符号来描述它们，不用害怕，理解起来非常简单："
msgstr "In order to facilitate subsequent exchanges, we need to introduce some mathematical notation to describe them, do not be afraid, very simple to understand："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:163
msgid "波士顿房价数据集的样本容量为 506，记为 :math:`n`; （Number）"
msgstr "The sample size of the Boston housing price data set is 506, denoted as :math:`n`; (Number)"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:164
msgid "单个的样本可以用一个向量 :math:`\\mathbf {x}=(x_{1}, x_{2}, \\ldots , x_{d})` 来表示，称为“特征向量”，里面的每个元素对应着该样本的某一维特征；"
msgstr "A single sample can be represented by a vector :math:`\\mathbf {x}=(x_{1}, x_{2}, \\ldots, x_{d})`, called a \"feature vector\", each element in it corresponds to a certain dimension of the sample feature;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:165
msgid "波士顿房价数据集的特征向量有 13 个维度，包括住宅平均房间数、城镇师生比例等等，记为 :math:`d`. （Dimensionality）"
msgstr "The feature vector of the Boston housing price data set has 13 dimensions, including the average number of rooms in a house, the ratio of urban teachers and students, etc., which are recorded as :math:`d`. (Dimensionality)"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:166
msgid "数据集 ``data`` 由 :math:`n` 行特征向量组成，每个特征向量有 :math:`d` 维度特征，因此整个数据集可以用一个形状为 :math:`(n, d)` 的数据矩阵 :math:`X` 来表示；"
msgstr "The data set ``data`` consists of :math:`n` row feature vectors, each feature vector has :math:`d` dimension features, so the entire data set can use a :math:`(n, d)` :math:`X` to indicate;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:167
msgid "标签 ``label`` 的每个元素是一个标量值，记录着房价值，因此它本身是一个含有 :math:`n` 个元素的标签向量 :math:`\\mathbf {y}`."
msgstr "`` Label`` each element tag is a scalar value, the recording room value, it is itself a containing :math:tag `n` vector elements :math:` \\ mathbf {y}`."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:169
msgid "X = \\begin{bmatrix}\n"
"- \\mathbf {x}_1 - \\\\\n"
"- \\mathbf {x}_2 - \\\\\n"
"\\vdots \\\\\n"
"- \\mathbf {x}_n -\n"
"\\end{bmatrix}\n"
"= \\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots  &         &        & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\quad\n"
"\\mathbf {y} =  (y_{1}, y_{2}, \\ldots, y_{n})"
msgstr "X = \\begin{bmatrix}\n"
"\\mathbf {x}_1-\\\\\n"
"\\mathbf {x}_2-\\\\\n"
"\\vdots \\\\\n"
"\\mathbf {x}_n-\n"
"\\end{bmatrix}\n"
"= \\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots & & & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\ quad\n"
"\\mathbf {y} = (y_{1}, y_{2}, \\ldots, y_{n})"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:187
msgid "其中 :math:`x_{i,j}` 表示第 :math:`i` 个样本 :math:`\\mathbf {x}_i` 的第 :math:`j` 维特征，\\ :math:`y_i` 为样本 :math:`\\mathbf {x}_i` 对应的标签。"
msgstr "Wherein :math:`X_{i,j}` represents :math:`i` samples :math:` mathbf \\ {x}of the _i` :math:`j` dimension feature, \\ :math:` y_i` sample :math:`\\ mathbf {x}_i` corresponding label."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:189
msgid "**不同的人会对数学符号的使用有着不同的约定，一定需要搞清楚这些符号的定义，或者在交流时采用比较通用的定义方式**"
msgstr "**Different people have different conventions on the use of mathematical symbols. You must clarify the definition of these symbols, or use a more general definition when communicating**"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:190
msgid "为了简洁美观，我们这里用粗体表示向量 :math:`\\mathbf {x}`; 而在手写时粗细体并不容易辨别，所以板书时常用上标箭头来表示向量 :math:`\\vec {x}`."
msgstr "For the sake of simplicity and beauty, we use bold to represent the vector :math:`\\mathbf {x}`; while the weight is not easy to distinguish when handwriting, so superscript arrows are often used to represent the vector :math:`\\vec {x}`in writing on the blackboard."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:236
msgid "数据集的几种类型"
msgstr "Several types of data sets"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:238
msgid "在之前的线性回归模型教程中，我们在最后通过可视化效果对模型的拟合程度进行了判断，但这并不是评估模型性能通常的做法。"
msgstr "In the previous linear regression model tutorial, we finally judged the fit of the model through the visualization effect, but this is not the usual way to evaluate the performance of the model."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:240
msgid "对于常见的机器学习任务，用作基准（Benchmark）的数据集通常会被分为训练集（Training dataset）和测试集（Test dataset）两部分；"
msgstr "For common machine learning tasks, the data set used as a benchmark (benchmark) is usually divided into two parts: training set (Training dataset) and test set (Test dataset);"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:242
msgid "我们利用训练集的数据，通过优化损失函数的形式将模型训练好，在测试集上对模型进行测试，根据选用的评估指标（Metrics）评价模型性能"
msgstr "We use the data of the training set to train the model by optimizing the loss function, test the model on the test set, and evaluate the performance of the model according to the selected evaluation indicators (Metrics)"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:243
msgid "测试集的数据不能参与模型训练，因为我们希望测试集数据代表着将来会被用于预测的真实数据，它们现在是“不可见的”，当作标签未知"
msgstr "The test set data cannot participate in model training, because we hope that the test set data represents the real data that will be used for prediction in the future. They are now \"invisible\" and treated as unknown labels."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:245
msgid "训练模型时，通常还会在训练集的基础上划分出验证集（Validation dataset）或者说开发集（Develop dataset），方便进行控制和调试："
msgstr "Training model, generally also divided on the basis of training set on a validation set (Validation dataset) or development set (Develop dataset), convenient control and debugging："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:247
msgid "在一些机器学习竞赛中，比赛方会将测试集中一部分拿出来做为 Public Leaderboard 评分和排名，剩下的部分作为 Private Leaderboard 的评分和排名。"
msgstr "In some machine learning competitions, the competition team will take out part of the test set as the public leaderboard score and ranking, and the remaining part will be used as the private leaderboard score and ranking."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:248
msgid "选手可以根据 Public Leaderboard 上的评估结果及时地对算法和超参数进行调整优化，但最终的排名将以 Private Leaderboard 为准"
msgstr "Players can adjust and optimize the algorithm and hyperparameters in time according to the evaluation results on the Public Leaderboard, but the final ranking will be based on the Private Leaderboard"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:249
msgid "对应地，可以将 Public Leaderboard 所使用的数据集理解为验证集，将 Private Leaderboard 所使用的数据集理解为测试集"
msgstr "Correspondingly, the data set used by Public Leaderboard can be understood as a verification set, and the data set used by Private Leaderboard can be understood as a test set"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:250
msgid "为了避免在短时间内引入过多密集的新知识，我们目前将不会进行验证集（开发集）的代码实践和讨论"
msgstr "In order to avoid introducing too much intensive new knowledge in a short time, we will not conduct code practice and discussion on the verification set (development set) at present"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:252
msgid "数据集该如何划分是一个值得讨论与研究的话题，它是数据预处理的一个环节，目前我们只需要对它有一个基本概念，通过不断实践来加深理解。"
msgstr "How to divide the data set is a topic worthy of discussion and research. It is a part of data preprocessing. At present, we only need to have a basic concept of it and deepen our understanding through continuous practice."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:254
msgid "对于波士顿房价数据集，我们可以将 506 张样本划分为 500 张训练样本，6 张测试样本："
msgstr "For the Boston housing price data set, we can divide 506 samples into 500 training samples and 6 test samples："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:305
msgid "模型定义、训练和测试"
msgstr "Model definition, training and testing"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:307
msgid "对于单个的样本 :math:`\\mathbf {x}`, 想要预测输出 :math:`y`, 即尝试找到映射关系 :math:`f: \\mathbf {x} \\in \\mathbb {R}^{d} \\mapsto y`, 同样可以建立线性模型："
msgstr "For a single sample :math:`\\mathbf {x}`, if you want to predict output :math:`y`, try to find the mapping relationship :math:`f: \\mathbf {x} \\in \\mathbb {R}^{d} \\mapsto y`, you can also build a linear model："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:309
msgid "\\begin{aligned}\n"
"y &= f(\\mathbf {x}) = \\mathbf {w} \\cdot \\mathbf {x} + b \\\\\n"
"& = (w_{1}, w_{2}, \\ldots, w_{13}) \\cdot (x_{1}, x_{2}, \\ldots, x_{13}) + b\\\\\n"
"& = w_{1} x_{1} + w_{2} x_{2} + \\ldots + w_{13} x_{13} + b\n"
"\\end{aligned}"
msgstr "\\begin{aligned}\n"
"y &= f(\\mathbf {x}) = \\mathbf {w} \\cdot \\mathbf {x} + b \\\\\n"
"& = (w_{1}, w_{2}, \\ldots, w_{13}) \\cdot (x_{1}, x_{2}, \\ldots, x_{13}) + b\\\\\n"
"& = w_{1} x_{1} + w_{2} x_{2} + \\ldots + w_{13} x_{13} + b\n"
"\\end{aligned}"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:318
msgid "**注意在表示不同的乘积（Product）时，不仅使用的符号有所不同，数学概念和编程代码之间又有所区别** ，以 NumPy 为例："
msgstr "** Note that represent different product (Product) in the symbol not only use different, there are differences ** between mathematical concepts and programming code, for example to NumPy："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:320
msgid "上个教程中用的星乘 :math:`*` 符号指的是元素间相乘（有时用 :math:`\\odot` 表示），也适用于标量乘法，编程对应于 ``np.multiply()`` 方法"
msgstr ":math:`*` symbol used in the previous tutorial refers to the multiplication between elements (sometimes represented by :math:`\\odot`), and it also applies to scalar multiplication. The programming corresponds to the ``np.multiply()`` method"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:321
msgid "对于点乘我们用 :math:`\\cdot` 符号表示，编程对应于 ``np.dot()`` 方法，在作用于两个向量时它等同于内积 ``np.inner()`` 方法"
msgstr "For the dot product, we use the :math:`\\cdot` symbol. Programming corresponds to the ``np.dot()`` method. When acting on two vectors, it is equivalent to the inner product ``np.inner()'' method"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:322
msgid "根据传入参数的形状，\\ ``np.dot()`` 的行为也可以等同于矩阵乘法，对应于 ``numpy.matmul()`` 方法或 ``@`` 运算符"
msgstr "According to the shape of the incoming parameters, the behavior of \\ ``np.dot()'' can also be equivalent to matrix multiplication, corresponding to the ``numpy.matmul()'' method or the ``@`` operator"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:323
msgid "严谨地说，点积或者数量积只是内积的一种特例，\\ **但我们在编程时，应该习惯先查询文档中的说明，并进行简单验证**"
msgstr "Strictly speaking, dot product or quantitative product is just a special case of inner product, \\ ** But when we are programming, we should be used to query the description in the document first and perform simple verification **"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:325
msgid "比如样本 :math:`\\mathbf {x}` 和参数 :math:`\\mathbf {w}` 都是 13 维的向量，二者的点积结果是一个标量（ ``shape`` 为空元组）："
msgstr "Sample example :math:`\\ mathbf {x}` and Parameter :math:`\\ mathbf {w}` are 13-dimensional vector dot product of the two things is a scalar ( `` shape`` empty tuple)："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:372
msgid "我们之前已经实现过类似的训练过程，现在一起来看下面这份新实现的代码："
msgstr "We have implemented a similar training process before, now let’s take a look at the newly implemented code below.："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:456
msgid "模型评估（Model Metric）"
msgstr "Model Metric"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:458
msgid "现在，我们可以使用训练得到的 ``w`` 和 ``b`` 在测试数据上进行评估，一些时候损失函数可以被直接用于评估，比如训练时使用的 MSE."
msgstr "Now, we can use the ``w'' and ``b'' obtained from training to evaluate on the test data. Sometimes the loss function can be directly used for evaluation, such as the MSE used in training."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:460
msgid "但进行评估时，使用的评估指标可以不同，这里我们可以选择使用平均绝对误差（Mean Absolute Error, MAE）作为评估指标："
msgstr "But are assessed for indicators used may be different, where we can choose to use the mean absolute error (Mean Absolute Error, MAE) as an evaluation index："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:462
msgid "\\ell(y_{pred}, y_{real})= \\frac{1}{n }\\sum_{i=1}^{n}\\left | \\hat{y}_{i}-{y}_{i}\\right |"
msgstr "\\ ELL (Y_{pred}, Y_{real}) = \\ FRAC{1}{n }\\ sum_ {I} = ^. 1{n}\\ left | \\ Hat{y}_{i}-{y}_{i}\\right |"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:531
msgid "不同的梯度下降形式"
msgstr "Different forms of gradient descent"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:533
msgid "我们可以发现：上面的代码中，我们每训练一个完整的 ``epoch``, 将根据所有样本的平均梯度进行一次参数更新。"
msgstr "We can find that：, every time we train a complete epoch, we will update the parameters according to the average gradient of all samples."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:535
msgid "这种通过对所有的样本的计算来求解梯度的方法叫做 **批梯度下降法(Batch Gradient Descent)**."
msgstr "This method of solving the gradient by calculating all samples is called Batch Gradient Descent."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:536
msgid "当碰到样本容量特别大的情况时，可能会导致无法一次性将所有数据给读入内存，遇到内存用尽（Out of memory，OOM）的情况。"
msgstr "When encountering a situation where the sample size is particularly large, it may be impossible to read all the data into the memory at one time, and encounter the situation of out of memory (OOM)."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:538
msgid "这时你可能会想：“其实我们完全可以在每个样本经过前向传播计算损失、反向传播计算得到梯度后时，就立即对参数进行更新呀！”"
msgstr "At this time, you may think：\"In fact, we can update the parameters immediately after each sample is passed through the forward propagation calculation loss and the back propagation calculation to obtain the gradient!\""

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:540
msgid "Bingo~ 这种思路叫做 **随机梯度下降（Stochastic Gradient Descent，常缩写成 SGD)** ，动手改写后，整体代码实现如下："
msgstr "Bingo~ This idea is called Stochastic Gradient Descent (Stochastic Gradient Descent, often abbreviated as SGD)**, after the hands-on rewriting, the overall code implementation is as follows："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:615
msgid "可以看到，在同样的训练周期内，使用随机梯度下降得到的训练损失更低，即损失收敛得更快。这是因为参数的实际更新次数要多得多。"
msgstr "It can be seen that in the same training period, the training loss obtained by using stochastic gradient descent is lower, that is, the loss converges faster. This is because the actual update times of the parameters are much higher."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:617
msgid "接下来我们用随机梯度下降得到的参数 ``w`` 和 ``b`` 进行测试："
msgstr "Next, we use the parameters ``w'' and ``b'' obtained by stochastic gradient descent to test："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:682
msgid "可以看到，虽然我们在训练集上的损失远低于使用批梯度下降的损失，但测试时得到的损失反而略高于批梯度下降的测试结果。为什么会这样？"
msgstr "It can be seen that although our loss on the training set is much lower than the loss of using batch gradient descent, the loss obtained during the test is slightly higher than the test result of batch gradient descent. Why is this happening?"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:684
msgid "**抛开数据、模型、损失函数和评估指标本身的因素不谈，背后的原因可能是：**"
msgstr "** Aside from the factors of data, model, loss function and evaluation index itself, the reason behind it may be：**"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:686
msgid "使用随机梯度下降时，考虑到噪声数据的存在，并不一定参数每次更新都是朝着模型整体最优化的方向；"
msgstr "When using stochastic gradient descent, considering the existence of noisy data, it is not necessary that each parameter update is in the direction of the overall optimization of the model;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:687
msgid "若样本噪声较多，很容易陷入局部最优解而收敛到不理想的状态；"
msgstr "If the sample is noisy, it is easy to fall into the local optimal solution and converge to an undesirable state;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:688
msgid "如果更新次数过多，还容易出现在训练数据过拟合的情况，导致泛化能力变差。"
msgstr "If the number of updates is too many, it is easy to overfit the training data, resulting in poor generalization ability."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:690
msgid "既然两种梯度下降策略各有千秋，因此有一种折衷的方式，即采用\\ **小批量梯度下降法（Mini-Batch Gradient Descent）**\\ ："
msgstr "Since the two gradient descent strategies have their own merits, there is a compromise, that is, using \\ **Mini-Batch Gradient Descent **\\ ："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:692
msgid "我们设定一个 ``batch_size`` 值，将数据划分为多个 ``batch``\\ ，每个 ``batch`` 的数据采取批梯度下降策略来更新参数；"
msgstr "We set a ``batch_size`` value, divide the data into multiple ``batch``\\, each ``batch`` data adopts a batch gradient descent strategy to update the parameters;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:693
msgid "设置合适的 ``batch_size`` 值，既可以避免出现内存爆掉的情况，也使得损失可能更加平滑地收敛；"
msgstr "Setting an appropriate ``batch_size'' value can not only avoid the memory explosion, but also make the loss possible to converge more smoothly;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:694
msgid "不难发现 ``batch_size`` 是一个超参数；当 ``batch_size=1`` 时，小批量梯度下降其实就等同于随机梯度下降"
msgstr "It is not difficult to find that ``batch_size'' is a hyperparameter; when ``batch_size=1``, mini-batch gradient descent is actually equivalent to stochastic gradient descent"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:696
msgid "注意：MegEngine 的优化器 ``Optimizer`` 中实现的 ``SGD`` 优化策略，实际上就是对小批量梯度下降法逻辑的实现。"
msgstr "Note that the ``SGD'' optimization strategy implemented in the optimizer ``Optimizer'' of："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:698
msgid "这种折衷方案的效果比“真”随机梯度下降要好得多，\\ **因为可以利用向量化加速批数据的运算，而不是分别计算每个样本。**"
msgstr "The effect of this compromise is much better than the \"true\" stochastic gradient descent, \\ **Because vectorization can be used to speed up the calculation of batch data instead of calculating each sample separately. **"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:700
msgid "在这里我们先卖个关子，把向量化的介绍放在更后面，因为我们的当务之急是：获取小批量数据（Mini-batch data）."
msgstr "Here we first secrecy, to quantify the introduction on a more behind, because our priority is：to obtain small quantities of data (Mini-batch data)."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:712
msgid "采样器（Sampler）"
msgstr "Sampler"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:714
msgid "想要从完整的数据集中获得小批量的数据，则需要对已有数据进行采样。"
msgstr "If you want to obtain small batches of data from a complete data set, you need to sample the existing data."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:716
msgid "在 MegEngine 的 ``data`` 模块中，提供了多种采样器 ``Sampler``\\ ："
msgstr "In the ``data`` module of MegEngine, a variety of samplers ``Sampler``\\ ："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:718
msgid "对于训练数据，通常使用顺序采样器 ``SequentialSampler``, 我们即将用到；"
msgstr "For training data, the sequential sampler ``SequentialSampler`` is usually used, which we will use soon;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:719
msgid "对于测试数据，通常使用 ``RandomSampler`` 进行随机采样，在本教程的测试部分就能见到；"
msgstr "For test data, ``RandomSampler'' is usually used for random sampling, which can be seen in the test part of this tutorial;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:720
msgid "对于 ``dataset`` 的样本容量不是 ``batch_size`` 整数倍的情况，采样器也能进行很好的处理"
msgstr "For the case where the sample size of ``dataset'' is not an integer multiple of ``batch_size``, the sampler can also handle it well"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:774
msgid "但需要注意到，采样器 ``Sampler`` 返回的是索引值，要得到划分后的批数据，还需要结合使用 MegEngine 中的 ``Dataloader`` 模块。"
msgstr "But it should be noted that the sampler ``Sampler`` returns the index value. To get the divided batch data, you need to use the ``Dataloader'' module in MegEngine."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:776
msgid "同时也要注意，如果想要使用 ``Dataloader``, 需要先将原始数据集变成 MegEngine 支持的 ``Dataset`` 对象。"
msgstr "Also note that if you want to use ``Dataloader``, you need to first change the original data set into a ``Dataset`` object supported by MegEngine."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:788
msgid "利用 Dataset 封装一个数据集"
msgstr "Use Dataset to encapsulate a data set"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:790
msgid "``Dataset`` 是 MegEngine 中表示数据集最原始的的抽象类，可被其它类继承（如 ``StreamDataset``\\ ），可阅读 ``Dataset`` 模块的文档了解更多的细节。"
msgstr "``Dataset`` is the most primitive abstract class in MegEngine, which can be inherited by other classes (such as ``StreamDataset``\\ ). You can read the documentation of the ``Dataset`` module for more details."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:792
msgid "通常我们自定义的数据集类应该继承 ``Dataset`` 类，并重写下列方法："
msgstr "We usually custom dataset class should inherit `` Dataset`` class and override the following methods："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:794
msgid "``__init__()`` ：一般在其中实现读取数据源文件的功能。也可以添加任何其它的必要功能；"
msgstr "``__init__()'' ：generally realizes the function of reading the data source file in it. You can also add any other necessary functions;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:795
msgid "``__getitem__()`` ：通过索引操作来获取数据集中某一个样本，使得可以通过 for 循环来遍历整个数据集；"
msgstr "``__getitem__()`` ：Get a sample in the data set through index operation, so that the entire data set can be traversed through a for loop;"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:796
msgid "``__len__()`` ：返回数据集大小"
msgstr "``__len__() `` ：returns the size of the data set"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:851
msgid "其实，对于这种单个或多个 NumPy 数组构成的数据，在 MegEngine 中也可以使用 ``ArrayDataset`` 对它进行初始化，它将自动完成以上方法的重写："
msgstr "In fact, for a single or a plurality of such data NumPy arrays configured, may be used in the `` ArrayDataset`` MegEngine initialize it, it will automatically overwrite the above method："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:895
msgid "数据载入器（Dataloader）"
msgstr "Dataloader"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:897
msgid "接下来便可以通过 ``Dataloader`` 来生成批数据，每个元素由对应的 ``batch_data`` 和 ``batch_label`` 组成："
msgstr "Then it can be `` Dataloader`` next life bulk data, each element by a corresponding `` batch_data`` composition and `` batch_label``："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:944
msgid "接下来我们一起来看看，使用批数据为什么能够加速整体的运算效率。"
msgstr "Next, let's take a look at why the use of batch data can speed up the overall computing efficiency."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:947
msgid "通过向量化加速运算"
msgstr "Speed up calculations through vectorization"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:949
msgid "在 NumPy 内部，向量化运算的速度是优于 ``for`` 循环的，我们很容易验证这一点："
msgstr "Inside NumPy, to quantify the rate of operation is better than `` for`` cycle, we can easily verify this："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1007
msgid "重新阅读模型训练的代码，不难发现，每个 ``epoch`` 内部存在着 ``for`` 循环，根据模型定义的 :math:`y_i = \\mathbf {w} \\cdot \\mathbf {x_i} + b` 进行了 :math:`n` 次计算。"
msgstr "Re-reading the code of the model training, it is not difficult to find that there is a ``for'' loop inside each ``epoch``, according to the model definition :math:`y_i = \\mathbf {w} \\cdot \\mathbf {x_i} + b` for :math:`n` calculations."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1009
msgid "我们在前面已经将数据集表示成了形状为 :math:`(n, d)` 的数据矩阵 :math:`X`, 将标签表示成了 :math:`y` 向量，这启发我们一次性完成所有样本的前向计算过程："
msgstr "We have previously represented the data set as :math::math:`(n, d)`, and represented the label as a :math:`y` vector, which inspired us to complete the forward direction of all samples at once Calculation process："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1011
msgid "(y_{1}, y_{2}, \\ldots, y_{n}) =\n"
"\\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots  &         &        & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\cdot (w_{1}, w_{2}, \\ldots, w_{d}) +\n"
"b"
msgstr "(y_{1}, y_{2}, \\ldots, y_{n}) =\n"
"\\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots & & & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\cdot (w_{1}, w_{2}, \\ldots, w_{d}) +\n"
"b"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1024
msgid "一种比较容易理解的形式是将其看成是矩阵运算 :math:`Y=XW+B`\\ ："
msgstr "An easier to understand form is to think of it as a matrix operation :math:`Y=XW+B`\\ ："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1026
msgid "\\begin{bmatrix}\n"
"y_{1} \\\\\n"
"y_{2} \\\\\n"
"\\vdots \\\\\n"
"y_{n}\n"
"\\end{bmatrix} =\n"
"\\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots  &         &        & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\begin{bmatrix}\n"
"w_1 \\\\\n"
"w_2 \\\\\n"
"\\vdots \\\\\n"
"w_d\n"
"\\end{bmatrix} +\n"
"\\begin{bmatrix}\n"
"b \\\\\n"
"b \\\\\n"
"\\vdots \\\\\n"
"b\n"
"\\end{bmatrix}"
msgstr "\\begin{bmatrix}\n"
"y_{1} \\\\\n"
"y_{2} \\\\\n"
"\\vdots \\\\\n"
"y_{n}\n"
"\\end{bmatrix} =\n"
"\\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots & & & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\begin{bmatrix}\n"
"w_1 \\\\\n\n"
"\\vdots \\ \\\n"
"w_d\n"
"\\end{bmatrix} +\n"
"\\begin{bmatrix}\n"
"b \\\\\n"
"b \\\\\n"
"\\vdots \\\\\n"
"b\n"
"\\end{bmatrix}"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1054
msgid "形状为 :math:`(n,d)` 的矩阵 :math:`X` 和维度为 :math:`(d,)` 的向量 :math:`w` 进行点乘，此时不妨理解成 :math:`w` 变成了一个形状为 :math:`(d, 1)` 的矩阵 :math:`W`"
msgstr ":math:`X` with shape :math:`(n,d)` :math::math:`(d,)`. At this time, it may be understood that :math:`w` becomes a shape A matrix of :math:`(d, 1)` :math:`W`"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1055
msgid "两个矩阵进行乘法运算，此时的 ``np.dot(X, w)`` 等效于 ``np.matmul(X, W)``, 底层效率比 ``for`` 循环快许多，得到形状为 :math:`(n, 1)` 的中间矩阵 :math:`P`"
msgstr "Two matrices are multiplied. At this time, ``np.dot(X, w)'' is equivalent to ``np.matmul(X, W)``, and the underlying efficiency is much faster than the ``for'' loop. :math:`P` with shape :math:`(n, 1)`"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1056
msgid "中间矩阵 :math:`P` 和标量 :math:`b` 相加，此时标量 :math:`b` 广播成 :math:`(n, 1)` 的矩阵 :math:`B` 进行矩阵加法，得到形状为 :math:`(n,1)` 的标签矩阵 :math:`Y`"
msgstr "Intermediate matrix :math:`P` scalar :math:` B` addition, the scalar case :math:`B` to broadcast :math:` (n-,. 1) `matrix :math:` B` adding a matrix to obtain shape :math:`(n-, 1) `'s label matrix :math:`Y`"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1057
msgid "显然，这样的处理逻辑还需要将标签矩阵 :math:`Y` 去掉冗余的那一维，变成形状为 :math:`(n, )` 的标签向量 :math:`y`"
msgstr "Obviously, such processing logic also needs to remove the redundant dimension of the :math: :math:`y` of :math:`(n, )`"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1059
msgid "**矩阵/张量运算的思路在深度学习中十分常见，在后续会被反复提及。**"
msgstr "**The idea of matrix/tensor operation is very common in deep learning and will be mentioned repeatedly in the follow-up. **"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1061
msgid "NumPy 的\\ `官方文档 <https://numpy.org/doc/stable/reference/generated/numpy.dot.html>`__\\ 中写着此时 ``np.dot()`` 的真实逻辑："
msgstr "NumPy's \\ `Official document <https://numpy.org/doc/stable/reference/generated/numpy.dot.html>`__\\ writes the true logic of ``np.dot()'' at this time："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1063
msgid "If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b."
msgstr "If a is an ND array and b is a 1-D array, it is a sum product over the last axis of a and b."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1064
msgid "如果 ``a`` 是 N 维度数组且 ``b`` 是 1 维数组，将对 ``a`` 和 ``b`` 最后一轴上对元素进行乘积并求和。"
msgstr "If ``a`` is an N-dimensional array and ``b`` is a 1-dimensional array, the elements of ``a`` and ``b`` will be multiplied and summed on the last axis."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1066
msgid "这样的计算逻辑也可以由爱因斯坦求和约定 ``np.einsum('ij,j->i', X, w)`` 来实现，感兴趣的读者可查阅 ``np.einsum()`` 文档。"
msgstr "Such calculation logic can also be realized by Einstein's summation agreement ``np.einsum('ij,j->i', X, w)'', interested readers can refer to ``np.einsum() `` Documentation."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1068
msgid "我们可以设计一套简单的测试样例，来看一下 ``np.dot()``, ``np.einsum()`` 和 ``np.matmul()`` 相较于 ``for`` 循环是否能得到同样的结果："
msgstr "We can design a set of simple test examples, take a look at ``np.dot()``, ``np.einsum()`` and ``np.matmul()'' compared to ``for` `Whether the loop can get the same result："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1126
msgid "接下来加大数据规模，测试向量化是否起到了加速效果："
msgstr "Then increase the size of data, test to quantify whether played accelerating effect："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1202
msgid "可以发现 ``dot()`` 和 ``matmul()`` 的向量化实现都有明显的加速效果，\\ ``einsum()`` 虽然是全能型选手，但也以更多的开销作为了代价，一般不做考虑。"
msgstr "It can be found that the vectorization of ``dot()'' and ``matmul()`` has obvious acceleration effect. Although \\``einsum()'' is an all-rounder, it also takes more overhead as a The price is generally not considered."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1205
msgid "NumPy 实现"
msgstr "NumPy implementation"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1207
msgid "我们先尝试使用 NumPy 的 ``dot()`` 实现一下向量化的批梯度下降的代码："
msgstr "Let's try to use NumPy the `` dot () `` implement the code to quantify the decline batch gradient："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1276
msgid "上面的 ``loss`` 收敛状况与我们最开始实现的 ``sum_grad`` 的数值一致，可以自行测试一下 ``for`` 循环写法和向量化写法的用时差异。"
msgstr "The convergence of the above ``loss'' is consistent with the value of ``sum_grad'' that we implemented at the beginning. You can test the time difference between the ``for'' loop writing and the vectorization writing by yourself."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1278
msgid "同时我们也可以发现，当前向传播使用批数据变成矩阵计算带来极大加速的同时，也为我们的反向传播梯度计算提出了更高的要求："
msgstr "We can also find the current matrix calculation at the same time become a tremendous acceleration to spread the use of batch data, also put forward higher requirements for our back-propagation gradient calculation："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1280
msgid "尽管存在着链式法则，但是对于不熟悉矩阵求导的人来说，还是很难理解一些梯度公式推导的过程，比如为什么出现了转置 ``data.T``"
msgstr "Despite the existence of the chain rule, it is still difficult for people who are not familiar with matrix derivation to understand the process of deriving some gradient formulas, such as why there is a transposition ``data.T''"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1281
msgid "当前向传播的计算变得更加复杂，由框架实现自动求导 ``autograde`` 机制就显得更加必要和方便"
msgstr "The calculation of the current propagation has become more complicated, and the automatic derivative ``autograde'' mechanism realized by the framework becomes more necessary and convenient."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1284
msgid "MegEngine 实现"
msgstr "MegEngine implementation"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1286
msgid "最后，让我们利用上小批量（Mini-batch）的数据把完整的训练流程代码在 MegEngine 中实现："
msgstr "Finally, let us use small quantities (Mini-batch) of data to complete the training process in the code implemented in MegEngine："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1288
msgid "为了后续教程理解的连贯性，我们做一些改变，使用 ``F.matmul()`` 来代替 ``np.dot()``\\ ，前面已经证明了这种情况下的计算结果值是等价的，但形状不同"
msgstr "For the continuity of understanding in subsequent tutorials, we make some changes and use ``F.matmul()'' instead of ``np.dot()``\\. It has been proved that the calculation result value in this case is equal Price, but different shape"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1289
msgid "对应地，我们的 ``w`` 将由向量变为矩阵，\\ ``b`` 在进行加法操作时将自动进行广播变成矩阵，输出 ``pred`` 也是一个矩阵，即 2 维 Tensor"
msgstr "Correspondingly, our ``w`` will be changed from a vector to a matrix, and \\``b`` will be automatically broadcasted into a matrix when performing an addition operation. The output ``pred`` is also a matrix, that is, a 2-dimensional Tensor"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1290
msgid "在计算单个 ``batch`` 的 ``loss`` 时，内部通过 ``sum()`` 计算已经去掉了冗余的维度，最终得到的 ``loss`` 是一个 0 维 Tensor"
msgstr "When calculating the ``loss`` of a single ``batch``, the redundant dimensions have been removed through the ``sum()'' calculation internally, and the final ``loss`` is a 0-dimensional Tensor"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1387
msgid "对我们训练好的模型进行测试："
msgstr "We trained on the test model："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1450
msgid "可以发现，使用小批量梯度下降策略更新参数，最终在测试时得到的平均绝对值损失比单独采用批梯度下降和随机梯度下降策略时还要好一些。"
msgstr "It can be found that using the small batch gradient descent strategy to update the parameters, the final average absolute value loss obtained during the test is better than when the batch gradient descent and stochastic gradient descent strategies are used alone."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1462
msgid "总结回顾"
msgstr "Summary review"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1464
msgid "有些时候，添加一些额外的条件，看似简单的问题就变得更加复杂有趣起来了，启发我们进行更多的思考和探索。"
msgstr "Sometimes, by adding some additional conditions, seemingly simple problems become more complicated and interesting, inspiring us to think and explore more."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1466
msgid "我们需要及时总结学习过的知识，并在之后的时间内不断通过实践来强化记忆，这次的教程中出现了以下新的概念："
msgstr "We need to summarize the knowledge we have learned in time, and continue to strengthen the memory through practice in the future. The following new concepts appeared in this tutorial.："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1468
msgid "数据集（Dataset）：想要将现实中的数据集变成能够被 MegEngine 使用的格式，必须将其处理成 ``MapDataset`` 或者 ``StreamDateset`` 类"
msgstr "Dataset (Dataset)：If you want to convert the actual data set into a format that can be used by MegEngine, you must process it into a ``MapDataset'' or ``StreamDateset'' class"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1469
msgid "继承基类的同时，还需要实现 ``__init__()``, ``__getitem__`` 和 ``__len__()`` 三种内置方法"
msgstr "While inheriting the base class, you also need to implement the three built-in methods__init__()``, ``__getitem__`` and ``__len__"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1470
msgid "我们对事物的抽象——特征（Feature）有了一定的认识，并接触了一些数学形式的表达如数据矩阵 :math:`X` 和标签向量 :math:`y` 等"
msgstr "We have a certain understanding of the abstraction of things—Features, and have been exposed to some mathematical expressions such as data matrix :math:`X` and label vector :math:`y`, etc."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1471
msgid "我们接触到了训练集（Trainining dataset）、测试集（Test dataset）和验证集（Validation dataset）/ 开发集（Develop dataset）的概念"
msgstr "We are exposed to the concepts of Training dataset, Test dataset and Validation dataset/Develop dataset"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1472
msgid "除了训练时我们会设计一个损失函数外，在测试模型性能时，我们也需要设计一个评估指标（Metric），不同任务的指标不尽相同"
msgstr "In addition to designing a loss function during training, we also need to design an evaluation metric (Metric) when testing the performance of the model. The metrics for different tasks are not the same."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1473
msgid "采样器（Sampler）：可以帮助我们自动地获得一个采样序列，常见的有顺序采样器 ``SequentialSampler`` 和随机采样器 ``RandomSampler``"
msgstr "Sampler (Sampler)：can help us automatically obtain a sampling sequence. Commonly used are the sequential sampler ``SequentialSampler`` and the random sampler ``RandomSampler``"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1474
msgid "数据载入器（Dataloader）：根据 ``Dataset`` 和 ``Sampler`` 来获得对应的小批量数据（Mini-batch data）"
msgstr "Dataloader (Dataloader)：According to ``Dataset`` and ``Sampler`` to obtain the corresponding mini-batch data (Mini-batch data)"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1475
msgid "我们认识了一个新的超参数 ``batch_size``\\ ，并且对不同形式的梯度下降各自的优缺点有了一定的认识"
msgstr "We have recognized a new hyperparameter ``batch_size``\\, and have a certain understanding of the advantages and disadvantages of different forms of gradient descent"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1476
msgid "我们理解了批数据向量化计算带来的好处，能够大大加快计算效率；同时为了避免推导向量化的梯度，实现自动求导机制是相当必要的"
msgstr "We understand the benefits of batch data vectorization calculations, which can greatly speed up the calculation efficiency; at the same time, in order to avoid pushing the quantized gradient, it is quite necessary to implement an automatic derivative mechanism."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1478
msgid "我们要养成及时查阅文档的好习惯，比如 NumPy 的 ``np.dot()`` 在接受的 ndarray 形状不同时，运算逻辑也将发生变化"
msgstr "We have to develop a good habit of checking documents in time, such as NumPy's ``np.dot()'' when the shape of the accepted ndarray is different, the operation logic will also change"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1480
msgid "在写代码时，Tensor 的维度形状变化是尤其需要关注的地方，理清楚逻辑可以减少遇到相关报错的可能"
msgstr "When writing code, the dimensional shape change of Tensor is a place that needs special attention. Clearing the logic can reduce the possibility of encountering related errors."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1481
msgid "不同库和框架之间，相同 API 命名背后的实现可能完全不同"
msgstr "Between different libraries and frameworks, the implementation behind the same API naming may be completely different"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1493
msgid "问题思考"
msgstr "Problem thinking"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1495
msgid "旧问题的解决往往伴随着新问题的诞生，让我们一起来思考一下："
msgstr "Solve old problems often associated with the birth of new issues, let us work together to think about："

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1497
msgid "我们在标记和收集数据的时候，特征的选取是否科学，数据集的规模应该要有多大？数据集的划分又应该选用什么样的比例？"
msgstr "When we label and collect data, is the selection of features scientific, and how big should the data set be? What proportion should be used for the division of the data set?"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1498
msgid "这次见着了 ``batch_size``, 它的设定是否有经验可循？接触到的超参数越来越多了，验证集和开发集是如何帮助我们对合适的超参数进行选取的？"
msgstr "I saw ``batch_size`` this time. Is there any experience to follow in its setting? There are more and more hyper-parameters. How do the validation set and development set help us select the appropriate hyper-parameters?"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1499
msgid "我们对 NumPy 支持的 ndarray 的数据如何导入 MegEngine 已经有了经验，更复杂的数据集（图片、视频、音频）要如何处理呢？"
msgstr "We have experience in how to import the ndarray data supported by NumPy into MegEngine. How to deal with more complex data sets (pictures, videos, audios)?"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1500
msgid "噪声数据会对参数的学习进行干扰，\\ ``Dataloader`` 中是否有对应的预处理方法来做一些统计意义上的处理？"
msgstr "Noise data will interfere with the learning of parameters. Is there a corresponding preprocessing method in ``Dataloader`` to do some statistical processing?"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1501
msgid "我们知道了向量化可以加速运算，在硬件底层是如何实现“加速”这一效果的？"
msgstr "We know that vectorization can speed up calculations. How to achieve the effect of \"speeding up\" at the bottom of the hardware?"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1504
msgid "关于数学表示的习惯"
msgstr "About the habit of mathematical expression"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1506
msgid "由于数据矩阵 :math:`X` 的形状排布为 :math:`(n, d)`, 因此 MegEngine 中实现的矩阵形式的线性回归为 :math:`\\hat {Y} = f(X) = XW+B` （这里将 :math:`B` 视为 :math:`b` 广播后的矩阵）"
msgstr "Since the shape of the data matrix :math:`X` is arranged as :math:`(n, d)`, the linear regression of the matrix form implemented in MegEngine is :math:`\\hat {Y} = f(X) = XW+B` (here :math:`B` is regarded as :math:`b` broadcast)"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1508
msgid "然而在线性代数中为了方便运算表示，向量通常默认为列向量 :math:`\\mathbf {x} = (x_1; x_2; \\ldots x_n)`, 单样本线性回归为 :math:`f(\\mathbf {x};\\mathbf {w},b) = \\mathbf {w}^T\\mathbf {x} + b`\\ ，"
msgstr "However, in linear algebra, in order to facilitate the calculation and expression, the vector usually defaults to the column vector :math:`\\mathbf {x} = (x_1; x_2; \\ldots x_n)`, and the one-sample linear regression is :math:`f(\\mathbf {x};\\mathbf {w}, b) = \\mathbf {w}^T\\mathbf {x} + b`\\,"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1510
msgid "对应有形状为 :math:`(d, n)` 的数据矩阵："
msgstr "Corresponding to a：:math:`(d, n)`"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1512
msgid "X = \\begin{bmatrix}\n"
"| & | & & | \\\\\n"
"\\mathbf {x}_1 & \\mathbf {x}_2 & \\cdots & \\mathbf {x}_n  \\\\\n"
"| & | & & |\n"
"\\end{bmatrix}\n"
"= \\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n"
"\\vdots  &         &        & \\vdots \\\\\n"
"x_{d,1} & x_{d,2} & \\cdots & x_{d,n}\n"
"\\end{bmatrix}"
msgstr "X = \\begin{bmatrix}\n"
"| & | & & | \\\\\n"
"\\mathbf {x}_1 & \\mathbf {x}_2 & \\cdots & \\mathbf {x}_n \\\\\n"
"| & | & & |\n"
"\\end{bmatrix}\n"
"= \\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n"
"\\vdots & & & \\vdots \\\\\n"
"x_{d,1} & x_{d,2} & \\cdots & x_{d,n}\n"
"\\end{bmatrix}"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1527
msgid "因此可以得到 :math:`\\hat {Y} = f(X) = W^{T}X+B`, 这反而是比较常见的形式（类似于 :math:`y=Ax+b`\\ ）。"
msgstr "So you can get :math:`\\hat {Y} = f(X) = W^{T}X+B`, which is a more common form (similar to :math:`y=Ax+b`\\ )."

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1529
msgid "为什么我们在教程中要使用 :math:`\\hat {Y} = f(X) = XW+B` 这种不常见的表述形式呢？数据布局不同会有什么影响？"
msgstr "Why do we use :math:`\\hat {Y} = f(X) = XW+B` in this unusual form of expression in the tutorial? What is the impact of different data layouts?"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1531
msgid "深度学习，简单开发。我们鼓励你在实践中不断思考，并启发自己去探索直觉性或理论性的解释。"
msgstr "Deep learning, simple development. We encourage you to keep thinking in practice and inspire yourself to explore intuitive or theoretical explanations."

#~ msgid ""
#~ "|image0| `在官网查看 <https://megengine.org.cn/doc/stable/zh"
#~ "/getting-started/beginner/learning-from-linear-"
#~ "regression.html>`__"
#~ msgstr ""

#~ msgid "|image1| `在 MegStudio 运行 <https://studio.brainpp.com/project/4642>`__"
#~ msgstr ""

#~ msgid ""
#~ "|image2| `在 GitHub 查看 "
#~ "<https://github.com/MegEngine/Documentation/blob/main/source"
#~ "/getting-started/beginner/learning-from-linear-"
#~ "regression.ipynb>`__"
#~ msgstr ""

#~ msgid "NumPy 的官方文档中写着此时 ``np.dot()`` 的真实逻辑："
#~ msgstr ""
#~ "NumPy's official document states that "
#~ "the real logic of ``np.dot()'' at "
#~ "this time is："

