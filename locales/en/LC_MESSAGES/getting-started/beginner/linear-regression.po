msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-14 16:12+0800\n"
"PO-Revision-Date: 2023-04-21 09:34\n"
"Last-Translator: \n"
"Language-Team: English\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /dev/locales/en/LC_MESSAGES/getting-started/beginner/linear-regression.po\n"
"X-Crowdin-File-ID: 9851\n"
"Language: en_US\n"

#: ../../source/getting-started/beginner/linear-regression.rst:5
msgid "MegEngine 实现线性回归"
msgstr "MegEngine implements linear regression"

#: ../../source/getting-started/beginner/linear-regression.rst:6
msgid "本教程涉及的内容"
msgstr "What this tutorial covers"

#: ../../source/getting-started/beginner/linear-regression.rst:9
msgid "理解机器学习中数据（Data）和数据集（Dataset）有关的概念，用 MegEngine 进行封装；"
msgstr "Understand the concepts related to data (Data) and data set (Dataset) in machine learning, and use MegEngine to encapsulate it;"

#: ../../source/getting-started/beginner/linear-regression.rst:10
msgid "理解小批量梯度下降（Mini-batch Gradient Descent），以及 :class:`~.DataLoader` 的基础使用;"
msgstr "Understand Mini-batch Gradient Descent, and the basic usage of :class:`~.DataLoader`;"

#: ../../source/getting-started/beginner/linear-regression.rst:11
msgid "根据前面的介绍，使用 MegEngine 完成加利福尼亚住房数据集的房价预测任务。"
msgstr "According to the previous introduction, use MegEngine to complete the housing price prediction task of the California housing dataset."

#: ../../source/getting-started/beginner/linear-regression.rst:13
msgid "本教程中将接触到更多 Tensor 操作与运算的接口"
msgstr "In this tutorial, you will be exposed to more interfaces of Tensor operations and operations"

#: ../../source/getting-started/beginner/linear-regression.rst:16
msgid "需要用到一些线性代数知识，下面这些材料用于在阅读教程感到吃力时，进行回顾："
msgstr "Some knowledge of linear algebra is required, the following materials are used to review when reading the："

#: ../../source/getting-started/beginner/linear-regression.rst:18
msgid "CS229 - `Linear Algebra Review and Reference <https://cs229.stanford.edu/section/cs229-linalg.pdf>`_"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:19
msgid "Kaare Brandt Petersen - `The Matrix Cookbook <https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf>`_"
msgstr "Kaare Brandt Petersen - `The Matrix Cookbook <https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf>"

#: ../../source/getting-started/beginner/linear-regression.rst:20
msgid "3Blue1Brown - `线性代数的本质 [Bilibili] <https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528927>`_ / `Essence of linear algebra [YouTube] <https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab>`_"
msgstr "3Blue1Brown - `Essence of linear algebra [Bilibili] <https://space.bilibili.com/88461692/channel/seriesdetail?sid=1528927>`_ / `Essence of linear algebra [YouTube] <https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab>`_"

#: ../../source/getting-started/beginner/linear-regression.rst:25
msgid "本教程中所用的数据集已经由波士顿住房数据集替换成加利福尼亚住房数据集。"
msgstr "The dataset used in this tutorial has been replaced by the Boston housing dataset with the California housing dataset."

#: ../../source/getting-started/beginner/linear-regression.rst:28
msgid "获取原始数据集"
msgstr "Get the original dataset"

#: ../../source/getting-started/beginner/linear-regression.rst:30
msgid "在上一个教程《 :ref:`megengine-basics` 》 中，我们尝试用 MegEngine 去拟合一条直线。 但在现实世界中，我们所面临的任务可能没有这么简单，数据的表示也不是这么直接的抽象。 因此在这个教程中，我们将更完成一个更加复杂的线性回归任务，将用到 `加利福尼亚住房数据集 <https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html>`_ ， 完成房价预测任务。"
msgstr "In the last tutorial \" :ref:`megengine-basics` \", we tried to fit a straight line with MegEngine. But in the real world, the tasks we face may not be so simple, and the representation of data is not so straightforward and abstract. Therefore, in this tutorial, we will complete a more complex linear regression task, which will use the `California Housing Dataset <https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html>to complete the house price prediction task."

#: ../../source/getting-started/beginner/linear-regression.rst:36
msgid "我们将使用到 `Scikit-learn <https://scikit-learn.org/stable/index.html>`_ （请自行安装）中的接口来获取加利福尼亚住房数据："
msgstr "We will use the interface to `Scikit-learn <https://scikit-learn.org/stable/index.html>`_ (please install it yourself) to get California housing data："

#: ../../source/getting-started/beginner/linear-regression.rst:46
msgid "注：数据集将从 `StatLib <https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html>`_ 获取，请确保能正常访问该网站。 得到的 ``X`` 和 ``y`` 将会是 NumPy 的 ndarray 格式。"
msgstr "Note：The dataset will be obtained from `StatLib <https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html>`_, please ensure that you can access the website normally. The resulting ``X`` and ``y`` will be in NumPy's ndarray format."

#: ../../source/getting-started/beginner/linear-regression.rst:49
msgid "社区中有许多公开的数据集供我们学习和研究使用，因此许多 Python 库中会封装好获取一些数据集的接口，方便用户调用。 我们在后续的教程中还会看到 MegEngine 中提供的 :mod:`.data.dataset` 模块，提供了类似的功能。"
msgstr "There are many public datasets in the community for our study and research, so many Python libraries will encapsulate the interface to obtain some datasets, which is convenient for users to call. We'll also see the `.data.dataset` module provided in :mod:in subsequent tutorials, which provides similar functionality."

#: ../../source/getting-started/beginner/linear-regression.rst:56
msgid "了解数据集信息"
msgstr "Learn about dataset information"

#: ../../source/getting-started/beginner/linear-regression.rst:58
msgid "回忆一下，在上一个教程中，我们的输入数据形状为 :math:`(100, )`, 表示共有 100 个样本， 每个样本只含有一个自变量，因此也叫一元线性回归； 而此处得到的 :math:`X` 的形状为 :math:`(20640, 8)`, 表明共有 20640 个样本，后面的 8 意味着什么呢？ 很容易联想到，这里存在着 8 个自变量，即我们将用多元线性回归模型来完成房价预测。"
msgstr "Recall that in the previous tutorial, our input data shape was :math:`(100, )`, which means there are 100 samples in total, and each sample contains only one independent variable, so it is also called univariate linear regression; and here we get :math:`X` has a shape of :math:`(20640, 8)`, indicating that there are 20640 samples in total, what does the latter 8 mean? It is easy to think that there are 8 independent variables here, that is, we will use the multiple linear regression model to complete the housing price prediction."

#: ../../source/getting-started/beginner/linear-regression.rst:65
msgid "查阅 `California Housing dataset <https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset>`_ 文档可知： 加州住房数据集中一共有 20640 个示例（Instances），每个示例包括 8 个属性（Attributes）。 其中属性信息如下："
msgstr "Consult the `California Housing dataset <https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset>`_ document to know that there are： total of 20640 examples (Instances) in the California Housing dataset, and each example includes 8 attributes (Attributes). The attribute information is as follows："

#: ../../source/getting-started/beginner/linear-regression.rst:70
msgid "``MedInc``: median income in block group"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:71
msgid "``HouseAge``: median house age in block group"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:72
msgid "``AveRooms``: average number of rooms per household"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:73
msgid "``AveBedrms``: average number of bedrooms per household"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:74
msgid "``Population``: block group population"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:75
msgid "``AveOccup``: average number of household members"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:76
msgid "``Latitude``: block group latitude"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:77
msgid "``Longitude``: block group longitude"
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:79
msgid "每个属性都用一个具体数值进行了表示，而且告知我们没有缺失的属性值。"
msgstr "Each attribute is represented by a specific value and informs us that there are no missing attribute values."

#: ../../source/getting-started/beginner/linear-regression.rst:81
msgid "本教程中刻意缺失的流程"
msgstr "Processes intentionally missing from this tutorial"

#: ../../source/getting-started/beginner/linear-regression.rst:84
msgid "有过机器学习或数据挖掘项目经验的用户可能知道， 通常我们会对拿到的原始数据进行探索性数据分析（Exploratory Data Analysis，EDA）， 帮助我们后期更好地进行特征工程和选择模型。感兴趣的读者可以自行了解， 但这些步骤不是本教程中关注的重点，介绍这些流程反而会加大学习难度。"
msgstr "Users who have experience in machine learning or data mining projects may know that we usually perform Exploratory Data Analysis (EDA) on the raw data obtained to help us better perform feature engineering and select models later. Interested readers can understand by themselves, but these steps are not the focus of this tutorial, and introducing these processes will increase the difficulty of learning."

#: ../../source/getting-started/beginner/linear-regression.rst:90
msgid "数据的相关概念"
msgstr "Data related concepts"

#: ../../source/getting-started/beginner/linear-regression.rst:92
msgid "我们已经初步了解了加利福尼亚住房数据集的相关信息，接下来可以探讨数据（Data）的相关概念。"
msgstr "We have a preliminary understanding of the relevant information of the California housing data set, and then we can explore the related concepts of data (Data)."

#: ../../source/getting-started/beginner/linear-regression.rst:94
msgid "想要让计算机帮助我们解决现实问题，就需要对问题进行建模，抽象成计算机容易理解的形式。 我们已经知道机器学习是通过数据进行学习的（Learning from data），因此数据的表征很关键。 我们在描述一个事物的时候，通常会寻找其属性（Attribute）或者说特征（Feature）："
msgstr "In order for computers to help us solve real-world problems, we need to model the problem and abstract it into a form that the computer can easily understand. We already know that machine learning is learning from data (Learning from data), so the representation of data is critical. When we describe a thing, we usually look for its attributes (Attribute) or features (Feature)："

#: ../../source/getting-started/beginner/linear-regression.rst:98
msgid "比如我们描述一只柴犬的长相，会说这只柴的鼻子如何、耳朵如何、毛发如何等等；"
msgstr "For example, when we describe the appearance of a Shiba Inu, we will say how the Shiba's nose, ears, hair, etc.;"

#: ../../source/getting-started/beginner/linear-regression.rst:99
msgid "又比如在电子游戏中，角色的属性经常有生命值、魔法值、攻击力、防御力等属性；"
msgstr "Another example is in video games, the attributes of characters often include health, magic, attack, defense and other attributes;"

#: ../../source/getting-started/beginner/linear-regression.rst:100
msgid "在计算机中，这些信息都需要用离散数据进行表示，最常见的做法就是量化成数值。"
msgstr "In the computer, all this information needs to be represented by discrete data, and the most common way is to quantify it into numerical values."

#: ../../source/getting-started/beginner/linear-regression.rst:104
msgid "我们在旧的教程中使用了波士顿房屋数据集来完成房价预测任务，但现在已经被弃用。 如《 `racist data destruction? <https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>`_ 》 一文中的调查，该数据集的作者设计了一个不可逆变量“B”，假设种族隔离对房价有积极影响。 这样的数据集是存在伦理道德争议的，因此不应当被广泛使用。"
msgstr "We used the Boston housing dataset for the house price prediction task in an old tutorial, but it is now deprecated. As surveyed in the paper `racist data destruction? <https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>`_ , the authors of this dataset designed an irreversible variable \"B\" assuming that apartheid had a positive effect on house prices. Such datasets are ethically controversial and should not be widely used."

#: ../../source/getting-started/beginner/linear-regression.rst:110
msgid "尽管换用了数据集，但本教程中关注的重点不是数据特征的选取是否科学，关心的是特征数量变多的这一情况："
msgstr "Although the data set is replaced, the focus in this tutorial is not whether the selection of data features is scientific, but the situation where the：of features increases."

#: ../../source/getting-started/beginner/linear-regression.rst:116
msgid "住房数据集的中的每个示例我们也称之为样本（Sample），因此样本容量 :math:`n` 为 20640. 每个样本中记录的属性信息可以用一个特征向量 :math:`\\boldsymbol{x}=\\left(x_{1}, x_{2}, \\ldots, x_{d}\\right)` 来表示， 里面的每个元素对应着该样本的某一维特征，我们将特征的维数简记为 :math:`d`, 其值为 8. 因此我们的数据集可以用一个数据矩阵 :math:`X` 来表示，预测目标又叫做标记（Label）："
msgstr "Each example in the housing dataset is also called a sample, so the sample size :math:`n` is 20640. The property information recorded in each sample can be represented by a feature vector :math:`\\boldsymbol{x}=\\left (x_{1}, x_{2}, \\ldots, x_{d}\\right)` to indicate that each element in it corresponds to a certain dimension feature of the sample, we abbreviate the dimension of the feature as :math:`d`, its value is 8. Therefore, our dataset can be represented by a data matrix :math:`X`, and the prediction target is also called a label (Label)："

#: ../../source/getting-started/beginner/linear-regression.rst:122
msgid "X=\\left[\\begin{array}{c}\n"
" -\\boldsymbol{x}_{1}^{T}- \\\\\n"
" -\\boldsymbol{x}_{2}^{T}- \\\\\n"
" \\vdots \\\\\n"
" -\\boldsymbol{x}_{n}^{T}-\n"
" \\end{array}\\right]\n"
" =\\left[\\begin{array}{cccc}\n"
" x_{1,1} & x_{1,2} & \\cdots & x_{1, d} \\\\\n"
" x_{2,1} & x_{2,2} & \\cdots & x_{2, d} \\\\\n"
" \\vdots & & & \\vdots \\\\\n"
" x_{n, 1} & x_{n, 2} & \\cdots & x_{n, d}\n"
" \\end{array}\\right] \\quad\n"
" \\boldsymbol{y}=\\left(y_{1}, y_{2}, \\ldots, y_{n}\\right)"
msgstr "X=\\left[\\begin{array}{c}\n"
" -\\boldsymbol{x}_{1}^{T}- \\\\\n"
" -\\boldsymbol{x}_{2}^{T}- \\\\\n"
" \\vdots \\\\\n"
" -\\boldsymbol{x}_{n}^{T}-\n"
" \\ end{array}\\right]\n"
" =\\left[\\begin{array}{cccc}\n"
" x_{1,1} & x_{1,2} & \\cdots & x_{1, d} \\\\\n"
" x_{2,1} & x_{2,2} & \\cdots & x_{2, d} \\\\\n"
" \\vdots & & & \\vdots \\\\\n"
" x_{n, 1} & x_{n, 2} & \\cdots & x_{n, d}\n"
" \\end{array}\\right] \\quad\n"
" \\boldsymbol{y}=\\left(y_{1}, y_{2}, \\ldots, y_{n}\\right)"

#: ../../source/getting-started/beginner/linear-regression.rst:138
msgid "其中 :math:`x_{i,j}` 表示第 :math:`i` 个样本的第 :math:`j` 维特征， 标量 :math:`y_i` 为样本 :math:`\\boldsymbol{x}_{i}` 对应的标记值， :math:`(\\boldsymbol{x}, y)` 组成了样例（Example）。 在进行矩阵运算时，向量默认是列向量"
msgstr "where :math:`x_{i,j}` represents the :math:`j` dimension feature of the :math:`i` sample, scalar :math:`y_i` is the label value corresponding to sample :math:`\\boldsymbol{x}_{i}`, :math:`(\\boldsymbol{x}, y)` forms the Example. When performing matrix operations, the vector defaults to a column vector"

#: ../../source/getting-started/beginner/linear-regression.rst:145
msgid "计算：乘法形式的讨论"
msgstr "A discussion of computing the multiplication form of："

#: ../../source/getting-started/beginner/linear-regression.rst:147
msgid "我们任务是用线性回归模型 :math:`Y=X \\beta+\\varepsilon` 来预测房价，其中 :math:`\\varepsilon` 是随机扰动。 与一元线性回归的区别在于，此时的自变量 :math:`x` 有多个，我们的参数 :math:`\\boldsymbol{w}` 也由标量变成了向量， 对单个样本 :math:`\\boldsymbol{x}` 有："
msgstr "Our task is to predict house prices using a linear regression model :math:`Y=X \\beta+\\varepsilon`, where :math:`\\varepsilon` is a random perturbation. The difference from univariate linear regression is that there are multiple independent variables :math:`x` at this time, our parameter :math:`\\boldsymbol{w}` is also changed from a scalar to a vector, for a single sample :math:`\\boldsymbol{x}` has："

#: ../../source/getting-started/beginner/linear-regression.rst:151
msgid "\\begin{aligned}\n"
"\\hat{y} &=f(\\boldsymbol{x})=\\boldsymbol{w} \\cdot \\boldsymbol{x}+b \\\\\n"
"  &=\\left(w_{1}, w_{2}, \\ldots, w_{d}\\right) \\cdot\\left(x_{1}, x_{2}, \\ldots, x_{d}\\right)+b \\\\\n"
"  &=w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{d} x_{d}+b\n"
"\\end{aligned}"
msgstr "\\begin{aligned}\n"
"\\hat{y} &=f(\\boldsymbol{x})=\\boldsymbol{w} \\cdot \\boldsymbol{x}+b \\\\\n"
"  &=\\left(w_{1}, w_{2}, \\ldots, w_{d}\\right) \\cdot \\left(x_{1}, x_{2}, \\ldots, x_{d}\\right)+b \\\\\n"
"  &=w_{1} x_{1}+w_{2} x_{2}+\\ldots+w_{d} x_{d}+b\n"
"\\end{aligned}"

#: ../../source/getting-started/beginner/linear-regression.rst:159
msgid "两个向量点积将得到一个标量，与标量 :math:`b` 相加后得到的值就是预测的房价。"
msgstr "The dot product of the two vectors will result in a scalar that, when added to the scalar :math:`b`, is the predicted house price."

#: ../../source/getting-started/beginner/linear-regression.rst:161
msgid "在 MegEngine 中，向量点积操作的接口是 :func:`.functional.dot`:"
msgstr "In MegEngine, the interface for vector dot product operations is :func:`.functional.dot`:"

#: ../../source/getting-started/beginner/linear-regression.rst:178
msgid "为了利用向量化的特性（避免写出 for 循环），我们希望对整批数据 :math:`X` 有："
msgstr "To take advantage of vectorization (to avoid writing out for loops), we want to have：for the entire batch of data :math:`X`"

#: ../../source/getting-started/beginner/linear-regression.rst:180
msgid "\\hat{\\boldsymbol{y}}=\n"
"\\left(\\hat{y}_{1}, \\hat{y}_{2}, \\ldots, \\hat{y}_{n}\\right)\n"
"=\\left[\\begin{array}{cccc}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1, d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2, d} \\\\\n"
"\\vdots & & & \\vdots \\\\\n"
"x_{n, 1} & x_{n, 2} & \\cdots & x_{n, d}\n"
"\\end{array}\\right] \\cdot\n"
"\\left(w_{1}, w_{2}, \\ldots, w_{d}\\right)+b"
msgstr "\\hat{\\boldsymbol{y}}=\n"
"\\left(\\hat{y}_{1}, \\hat{y}_{2}, \\ldots, \\hat{y}_{n}\\right)\n"
"=\\left[\\begin{array}{cccc}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1, d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2, d} \\\\\n"
"\\vdots & & & \\vdots \\\\\n"
"x_{n, 1} & x_{n, 2} & \\cdots & x_{n, d}\n"
"\\end{array}\\right ] \\cdot\n"
"\\left(w_{1}, w_{2}, \\ldots, w_{d}\\right)+b"

#: ../../source/getting-started/beginner/linear-regression.rst:192
msgid "NumPy 中的 :func:`numpy.dot` 接口的确支持 n 维数组与 1 维数组（向量）之间的点乘："
msgstr ":func:The `numpy.dot` interface in NumPy does support dot-by-：between n-dimensional arrays and 1-dimensional arrays (vectors)"

#: ../../source/getting-started/beginner/linear-regression.rst:207
msgid "阅读 :func:`numpy.dot` 的 API 文档会发现，其支持各种不同输入形式的点积操作。"
msgstr "Read the API documentation of ` :func:to find that it supports dot product operations with various input forms."

#: ../../source/getting-started/beginner/linear-regression.rst:209
msgid "如果输入 :math:`a` 和 :math:`b` 都是 1 维数组（向量），等同于内积 :func:`numpy.inner`;"
msgstr "If the input :math:`a` and :math:`b` are both 1-dimensional arrays (vectors), equivalent to the inner product :func:`numpy.inner`;"

#: ../../source/getting-started/beginner/linear-regression.rst:210
msgid "如果输入 :math:`a` 和 :math:`b` 都是 2 维数组（矩阵）， 等同于矩阵乘法 :func:`numpy.matmul` 或中缀运算符 ``@``;"
msgstr "If the input :math:`a` and :math:`b` are both 2-dimensional arrays (matrices), equivalent to matrix multiplication :func:`numpy.matmul` or the infix operator ``@``;"

#: ../../source/getting-started/beginner/linear-regression.rst:212
msgid "如果输入 :math:`a` 和 :math:`b` 中有一个是 0 维数组（标量）， 等同于元素乘法 :func:`numpy.multiply` 或中缀运算符 ``*``;"
msgstr "If one of the inputs :math:`a` and :math:`b` is a 0-dimensional array (scalar), equivalent to element-wise multiplication :func:`numpy.multiply` or the infix operator ``*``;"

#: ../../source/getting-started/beginner/linear-regression.rst:214
msgid "如果输入 :math:`a` 是 n 维数组，输入 :math:`b` 是 1 维数组， 则会在 :math:`a` 的最后一轴与 :math:`b` 计算和积；"
msgstr "If input :math:`a` is an n-dimensional array, and input :math:`b` is a 1-dimensional array, the sum product will be calculated on the last axis of :math:`a` and :math:`b`;"

#: ../../source/getting-started/beginner/linear-regression.rst:216
msgid "如果..."
msgstr "if..."

#: ../../source/getting-started/beginner/linear-regression.rst:218
msgid "可见这个 :func:`numpy.dot` 接口有些过于全能了，用户如果不查阅文档， 很难想到调用它时的具体行为，这与 MegEngine 的接口设计哲学不符合。 MegEngine 中强调语法透明、行为显式，尽量避免存、歧义和误用， 因此 :func:`.functional.dot` 专指向量点积，不支持其它形状的输入。"
msgstr "It can be seen that this :func:`numpy.dot` interface is a bit too versatile. If the user does not consult the documentation, it is difficult to think of the specific behavior when calling it, which is inconsistent with the interface design philosophy of MegEngine. MegEngine emphasizes transparent syntax and explicit behavior, and tries to avoid storage, ambiguity and misuse. Therefore, :func:`.functional.dot` refers to the dot product of vectors, and does not support input of other shapes."

#: ../../source/getting-started/beginner/linear-regression.rst:223
msgid "这也意味着，同名或者相似命名的接口不代表完全一样的定义实现逻辑。 我们还可以参考 MARLAB 中的 `Dot product <https://ww2.mathworks.cn/help/matlab/ref/dot.html?lang=en>`_ 会发现定义也不和 NumPy 完全一样。"
msgstr "This also means that interfaces with the same or similar names do not represent exactly the same definition and implementation logic. We can also refer to `Dot product <https://ww2.mathworks.cn/help/matlab/ref/dot.html?lang=en>`_ in MARLAB and find that the definition is not exactly the same as that of NumPy."

#: ../../source/getting-started/beginner/linear-regression.rst:228
msgid "在 MegEngine 中矩阵乘法的接口为 :func:`.functional.matmul`, 对应中缀运算符 ``@``."
msgstr "The interface for matrix multiplication in MegEngine is :func:`.functional.matmul`, corresponding to the infix operator ``@``."

#: ../../source/getting-started/beginner/linear-regression.rst:230
msgid "Python 官方提案 `PEP 465 <https://www.python.org/dev/peps/pep-0465/#rejected-alternatives-to-adding-a-new-operator>`_ 提供了矩阵乘法的专用中缀运算符 ``@`` 相关讨论和推荐语义，兼容不同形状的输入形式。 里面提到了，对于 1 维向量输入，可以通过附加一个长度为 `1` 的维度而提升为 2 维矩阵， 执行矩阵乘法操作后，再从输入出删除临时添加的维度。这样就使得 矩阵@向量 和 向量@矩阵 都变成了合法操作（假定形状兼容），且都返回一维向量。"
msgstr "The official Python proposal `PEP 465 <https://www.python.org/dev/peps/pep-0465/#rejected-alternatives-to-adding-a-new-operator>`_ provides discussion and recommended semantics for a dedicated infix operator ``@`` for matrix multiplication, compatible with input forms of different shapes. It is mentioned that for a 1-dimensional vector input, it can be promoted to a 2-dimensional matrix by appending a dimension of length `1`, and after performing the matrix multiplication operation, the temporarily added dimension is removed from the input. This makes both matrix@vector and vector@matrix legal operations (assuming compatible shapes), and both return a 1D vector."

#: ../../source/getting-started/beginner/linear-regression.rst:237
msgid "\\left[\\begin{array}{c}\n"
"y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n}\n"
"\\end{array}\\right]\n"
"=\\left[\\begin{array}{cccc}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1, d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2, d} \\\\\n"
"\\vdots & & & \\vdots \\\\\n"
"x_{n, 1} & x_{n, 2} & \\cdots & x_{n, d}\n"
"\\end{array}\\right]\n"
"\\left[\\begin{array}{c}\n"
"w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{d}\n"
"\\end{array}\\right]\n"
"+\\left[\\begin{array}{c}\n"
"b \\\\ b \\\\ \\vdots \\\\ b\n"
"\\end{array}\\right]"
msgstr "\\left[\\begin{array}{c}\n"
"y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n}\n"
"\\end{array}\\right]\n"
"=\\left[\\begin{array}{cccc}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1, d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2, d} \\\\\n"
"\\vdots & & & \\vdots \\\\\n"
"x_{n, 1} & x_{n, 2} & \\cdots & x_{n, d}\n"
"\\end{array}\\right]\n"
"\\left[\\ begin{array}{c}\n"
"w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{d}\n"
"\\end{array}\\right]\n"
"+\\left[\\begin{array}{c}\n"
"b \\\\ b \\\\ \\vdots \\\\ b\n"
"\\end{array}\\r]"

#: ../../source/getting-started/beginner/linear-regression.rst:255
msgid "以本次多元线性回归模型为例，其计算方式在 MegEngine 中就变成了 :math:`Y=XW+B`："
msgstr "Taking this multiple linear regression model as an example, its calculation method becomes :math:in MegEngine `Y=XW+B`："

#: ../../source/getting-started/beginner/linear-regression.rst:264
msgid "通过 :func:`~.expand_dims` 接口，我们将形状为 :math:`(d,)` 的零向量 :math:`w` 变成了形状为 :math:`(d,1)` 的列向量 :math:`W`;"
msgstr "Through the :func:`~.expand_dims` interface, we turn a zero vector :math:`w` of shape :math:`(d,)` into a column vector :math:`W` of shape :math:`(d,1)`;"

#: ../../source/getting-started/beginner/linear-regression.rst:266
msgid "通过 :func:`~.matmul` 接口，执行矩阵乘法 :math:`P=XW`, :math:`P` 为中间计算结果；"
msgstr "Through the :func:`~.matmul` interface, perform matrix multiplication :math:`P=XW`, :math:`P` is the intermediate calculation result;"

#: ../../source/getting-started/beginner/linear-regression.rst:267
msgid "执行 :math:`Y=P+b` 时，标量 :math:`b` 广播称为形状兼容的列向量 :math:`B`;"
msgstr "When doing :math:`Y=P+b`, scalar :math:`b` broadcasts a shape-compatible column vector :math:`B`;"

#: ../../source/getting-started/beginner/linear-regression.rst:268
msgid "通过 :func:`~.squeeze` 接口，去掉了冗余的维度，将 :math:`Y` 变回了向量 :math:`y`."
msgstr "Through the :func:`~.squeeze` interface, the redundant dimension is removed, and :math:`Y` is changed back to the vector :math:`y`."

#: ../../source/getting-started/beginner/linear-regression.rst:270
msgid "尽管 :func:`~.expand_dims` 和 :func:`~.squeeze` 的功能也可以通过 :func:`~.reshape` 来实现， 但是为了代码的可读性，在存在专用接口的情况下，我们应当尽可能地使用这些具有透明语义的接口。"
msgstr "Although the functions of :func:`~.expand_dims` and :func:`~.squeeze` can also be achieved by :func:`~.reshape`, but for code readability, we should use as much as possible when there is a dedicated interface These are interfaces with transparent semantics."

#: ../../source/getting-started/beginner/linear-regression.rst:273
msgid "由于存在着 PEP 465 的推荐定义，因此 MegEngine 的 :func:`~.matmul` 也兼容非矩阵形式的输入："
msgstr "MegEngine's :func:`~.matmul` is also compatible with non-matrix input：due to the existence of the recommended definition of PEP 465"

#: ../../source/getting-started/beginner/linear-regression.rst:281
msgid "对不同输入形状进行兼容，会使得在高维情况下的局部矩阵乘法代码非常好写，且不影响代码可读性。"
msgstr "Compatibility with different input shapes makes local matrix multiplication code very easy to write in high-dimensional cases without compromising code readability."

#: ../../source/getting-started/beginner/linear-regression.rst:283
msgid "尽管这可能与 MegEngine 接口设计哲学有些冲突，但兼容社区一致标准是最低要求。 矩阵乘法的表示在 Python 数据科学社区中已经经过长期的广泛讨论和实践验证，最终达成了共识。 这是 Python 语言层面提供的官方参考，而 NumPy 中 :func:`numpy.dot` 接口设计并没获得社区的一致承认。"
msgstr "Although this may conflict with the MegEngine interface design philosophy, compatibility with community-consistent standards is a minimum requirement. The representation of matrix multiplication has been widely discussed and practically verified in the Python data science community for a long time, and finally a consensus has been reached. This is the official reference provided at the Python language level, and the :func:`numpy.dot` interface design in NumPy has not been unanimously recognized by the community."

#: ../../source/getting-started/beginner/linear-regression.rst:287
msgid "现在我们已经实现了一份足够向量化的代码，看来可以使用梯度下降算法来优化模型参数了。"
msgstr "Now that we have implemented a sufficiently vectorized code, it appears that the gradient descent algorithm can be used to optimize the model parameters."

#: ../../source/getting-started/beginner/linear-regression.rst:290
msgid "梯度下降的几种形式"
msgstr "Several forms of gradient descent"

#: ../../source/getting-started/beginner/linear-regression.rst:292
msgid "如果对于每个输入的样本，我们都及时地计算损失并且更新模型参数，参数迭代频率会非常高 —— 这种做法叫随机梯度下降（Stochastic Gradient Descent, SGD）， 也被叫做在线梯度下降（Online Gradient Descent），这种形式的梯度下降非常容易理解。 问题在于，频繁的更新会导致梯度的变化比较跳跃（方向不稳定）； 另外，在整个训练过程中需要进行多次的循环运算，如果数据规模变得巨大，这种计算形式将变得十分低效。"
msgstr "If for each input sample, we calculate the loss and update the model parameters in time, the parameter iteration frequency will be very high - this method is called Stochastic Gradient Descent (SGD), also known as Online Gradient Descent (Online Gradient Descent) Gradient Descent), this form of gradient descent is very easy to understand. The problem is that frequent updates will cause the change of gradient to jump (the direction is unstable); in addition, multiple loop operations are required during the entire training process. If the scale of data becomes huge, this form of calculation will become very low. effect."

#: ../../source/getting-started/beginner/linear-regression.rst:298
msgid "我们在教程中已经多次强调了：要用向量化形式代替 `for` 循环形式的写法，来追求更高的计算效率。 因此我们在前向计算时，通常会选择将批数据作为输入，而不是单个样本，这其实是并行计算的思想。 除此原因外，批梯度下降还能减少异常数据带来的干扰，朝着整体更优的方向去迭代参数，损失更稳定地收敛。"
msgstr "We have emphasized many times in the tutorial that：should be written in vectorized form instead of `for` loop form to pursue higher computational efficiency. Therefore, when we calculate forward, we usually choose to use batch data as input instead of a single sample, which is actually the idea of parallel computing. In addition to this reason, batch gradient descent can also reduce the interference caused by abnormal data, iterate parameters in the direction of overall better, and the loss converges more stably."

#: ../../source/getting-started/beginner/linear-regression.rst:302
msgid "但批梯度下降也存在自身的局限性。过于稳定的梯度变化可能会导致模型过早地收敛到一组不太理想的参数。 我们对于梯度进行累加的策略也有可能引入了其它的不确定因素，影响了整个训练过程。 考虑大规模的数据输入情况，此时我们很有可能无法将整个数据集一次性加载到内存中； 即便解决了内存容量问题，在大型数据集上进行批梯度下降，参数迭代更新的速度会显著变慢。"
msgstr "But batch gradient descent also has its own limitations. Overly stable gradient changes can cause the model to prematurely converge to a less-than-ideal set of parameters. Our strategy of accumulating gradients may also introduce other uncertainties that affect the entire training process. Considering the large-scale data input situation, it is very likely that we cannot load the entire data set into memory at one time; even if the memory capacity problem is solved, batch gradient descent is performed on large data sets, and the speed of parameter iterative update will be significantly slow down."

#: ../../source/getting-started/beginner/linear-regression.rst:307
msgid "容易被忽视的是将 NumPy ndarray 转换成 MegEngine Tensor 的过程，这里其实也存在着数据搬运："
msgstr "What is easily overlooked is the process of converting NumPy ndarrays to：Tensors. There is actually data handling here."

#: ../../source/getting-started/beginner/linear-regression.rst:311
msgid "权衡取舍随机梯度下降和批梯度下降之间的优缺点，我们得到了梯度下降的另一变体：小批量梯度下降。"
msgstr "Weighing the pros and cons between stochastic gradient descent and batch gradient descent, we get another variant of gradient descent,：mini-batch gradient descent."

#: ../../source/getting-started/beginner/linear-regression.rst:314
msgid "小批量梯度下降"
msgstr "Mini-Batch Gradient Descent"

#: ../../source/getting-started/beginner/linear-regression.rst:316
msgid "小批量梯度下降（Mini-Batch Gradient Descent）在训练时会将数据集划分成多个小的分批数据， 在每批数据上计算平均损失和梯度以减少方差，然后更新模型中的参数。 它的好处是参数的迭代频率高于批量梯度下降，有助于在损失收敛时避开局部最小值； 可以控制每次加载多少数据到内存，兼顾了计算效率以及参数更新频率。"
msgstr "Mini-Batch Gradient Descent divides the dataset into multiple small batches during training, computes the average loss and gradient on each batch to reduce variance, and then updates the parameters in the model. Its advantage is that the iteration frequency of parameters is higher than that of batch gradient descent, which helps to avoid local minima when the loss converges; it can control how much data is loaded into memory each time, taking into account the computational efficiency and parameter update frequency."

#: ../../source/getting-started/beginner/linear-regression.rst:321
msgid "在深度学习领域，梯度下降通常指代的是小批量梯度下降。"
msgstr "In the field of deep learning, gradient descent usually refers to mini-batch gradient descent."

#: ../../source/getting-started/beginner/linear-regression.rst:323
msgid "问题在于，采用小批量梯度下降算法将引入另一个超参数 ``batch_size``, 表示每一批数据的规模大小。"
msgstr "The problem is that using the mini-batch gradient descent algorithm will introduce another hyperparameter ``batch_size``, which represents the size of each batch of data."

#: ../../source/getting-started/beginner/linear-regression.rst:327
msgid "梯度下降过程中，损失变化的等高线图，箭头表示梯度下降的方向。"
msgstr "A contour plot of the change in loss during gradient descent, with arrows indicating the direction of gradient descent."

#: ../../source/getting-started/beginner/linear-regression.rst:329
msgid "当 ``batch_size`` 为 1 时，等同于随机梯度下降；"
msgstr "When ``batch_size`` is 1, it is equivalent to stochastic gradient descent;"

#: ../../source/getting-started/beginner/linear-regression.rst:330
msgid "当 ``batch_size`` 为 n 时，等同于批梯度下降。"
msgstr "When ``batch_size`` is n, it is equivalent to batch gradient descent."

#: ../../source/getting-started/beginner/linear-regression.rst:332
msgid "通常我们会根据硬件架构（CPU/GPU）来决定 ``batch_size`` 值的大小，比如会设置成如 32, 64, 128 等 2 的幂。"
msgstr "Usually we will decide the size of the ``batch_size`` value according to the hardware architecture (CPU/GPU), for example, it will be set to a power of 2 such as 32, 64, 128, etc."

#: ../../source/getting-started/beginner/linear-regression.rst:335
msgid "如何获取分批数据"
msgstr "How to get batched data"

#: ../../source/getting-started/beginner/linear-regression.rst:337
msgid "在 MegEngine 中提供了 :mod:`.megengine.data` 模块, 里面提供我们想要的数据分批功能，示范代码如下："
msgstr ":mod:`.megengine.data` module is provided in MegEngine, which provides the data batching function we want. The demonstration code is as follows："

#: ../../source/getting-started/beginner/linear-regression.rst:356
msgid "在上面的代码中，我们用 :class:`~.ArrayDataset` 对 NumPy ndarray 格式的数据集进行了快速封装， 接着使用顺序采样器 :class:`~.SequentialSampler` 对 ``house_dataset`` 进行了采样， 二者用来作为参数初始化 :class:`~.DataLoader`, 最终获取到了一个可迭代的对象，每次提供 ``batch_size`` 大小的数据和标记。"
msgstr "In the above code, we use :class:`~.ArrayDataset` to quickly encapsulate the dataset in NumPy ndarray format, and then use the sequential sampler :class:`~.SequentialSampler` to sample the ``house_dataset``, and both use To initialize :class:`~.DataLoader` as a parameter, and finally get an iterable object, providing ``batch_size`` size data and tags each time."

#: ../../source/getting-started/beginner/linear-regression.rst:364
msgid "我们在上面选定的 ``batch_size`` 为 64，样本容量为 20640, 因此可以划分成 323 批数据。"
msgstr "The ``batch_size`` we selected above is 64, and the sample size is 20640, so it can be divided into 323 batches of data."

#: ../../source/getting-started/beginner/linear-regression.rst:368
msgid "我们这里的介绍有些简略，但不影响本教程的学习，目前只需要了解大概作用即可；"
msgstr "Our introduction here is a bit brief, but it does not affect the learning of this tutorial. At present, we only need to understand the general function;"

#: ../../source/getting-started/beginner/linear-regression.rst:369
msgid "如果你对背后的原理感兴趣，可以阅读 :ref:`data-guide` 中的详细介绍。"
msgstr "If you are interested in the principle behind it, you can read the detailed introduction in :ref:`data-guide`."

#: ../../source/getting-started/beginner/linear-regression.rst:372
msgid "再次思考优化目标"
msgstr "Rethink optimization goals"

#: ../../source/getting-started/beginner/linear-regression.rst:374
msgid "我们在上一个教程提到了优化目标，给出了 “犯错越少，表现越好” 的核心原则， 并选择了整体平均损失（预测值和真实值之间的误差函数）作为优化目标， 最终通过损失值是否下降以及直接观察直线的拟合程度来判断模型性能的好坏。 但仔细思考一下，我们的真实意图并不仅仅是希望我们的模型在 **已知的** 这些数据集上表现良好。"
msgstr "We mentioned the optimization objective in the previous tutorial, gave the core principle of \"make fewer mistakes, perform better\", and chose the overall average loss (error function between the predicted value and the true value) as the optimization objective, and finally passed Whether the loss value decreases or not and directly observe the fit of the straight line to judge the performance of the model. But when you think about it, our real intent is not just to want our model to perform well on **known** these datasets."

#: ../../source/getting-started/beginner/linear-regression.rst:381
msgid "警惕过拟合现象"
msgstr "Beware of overfitting"

#: ../../source/getting-started/beginner/linear-regression.rst:383
msgid "我们更加希望我们的模型具有比较好的泛化（Generalization）能力， 在面对 **全新的、从未见过的** 数据时，也能进行准确的预测。 如果单纯地去基于已经观测到的这些数据进行求解， 对于线性回归模型 :math:`\\hat{y} = \\theta^{T} \\cdot \\boldsymbol{x}`, 我们其实完全可以基于样本 :math:`X` 和标记 :math:`\\boldsymbol{y}` 求得一个解析解 :math:`\\theta = (X^{T}X)^{-1}X^{T}\\boldsymbol{y}`. 这个结果也可以从概率模型视角用极大然估计（Maximum Likelihood Estimation）求得 —— 利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。 （本教程不需要知道详细的推导和证明过程，感兴趣的读者可以查看 CS229 `课程讲义 <https://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf>`_ ）"
msgstr "We hope that our model has better generalization ability and can make accurate predictions in the face of ** brand new, never seen** data. If we simply solve the problem based on the observed data, for the linear regression model :math:`\\hat{y} = \\theta^{T} \\cdot \\boldsymbol{x}`, we can actually be based on sample :math:`X` and mark :math:`\\ boldsymbol{y}` Find an analytical solution :math:`\\theta = (X^{T}X)^{-1}X^{T}\\boldsymbol{y}`. This result can also be found from a probabilistic model perspective using Maximum Likelihood Estimation— — Using the known sample results, infer the parameter values that are most likely (with the greatest probability) to lead to such a result. (This tutorial does not need to know the detailed derivation and proof process, interested readers can check CS229 `course handout <https://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf>`_ )"

#: ../../source/getting-started/beginner/linear-regression.rst:394
msgid "过拟合指的是我们的模型在已知数据集上训练表现良好，在新样本上预测却表现不佳的情况。 如果一个模型仅仅在用于训练的数据上表现良好甚至是优异，但却在实际应用时表现不佳... 尴尬，这表明我们训练出的模型已经过拟合。导致过拟合的原因有很多， 比如训练样本数量过少，在当前模型上训练轮数（Epochs）过多等等。 对应的解决方法有很多，比如使用更大规模的训练数据集、使用更复杂的模型等等..."
msgstr "Overfitting is when our model trains well on a known dataset but predicts poorly on new samples. If a model only performs well or even excellent on the data used for training, but does not perform well in practice... embarrassing, this indicates that the model we trained has been overfitted. There are many reasons for overfitting, such as too few training samples, too many training epochs (Epochs) on the current model, and so on. There are many corresponding solutions, such as using a larger training dataset, using a more complex model, etc..."

#: ../../source/getting-started/beginner/linear-regression.rst:400
msgid "本教程中介绍的一种防止过拟合的做法是，将数据集进行划分，并及时评估模型性能。"
msgstr "One way to prevent overfitting, described in this tutorial, is to partition the dataset and evaluate model performance in time."

#: ../../source/getting-started/beginner/linear-regression.rst:403
msgid "划分数据集"
msgstr "Divide the dataset"

#: ../../source/getting-started/beginner/linear-regression.rst:405
msgid "数据集的常见划分方式"
msgstr "Common partitioning of datasets"

#: ../../source/getting-started/beginner/linear-regression.rst:408
msgid "训练集（Training dataset）：用来进行训练并且优化模型参数的数据集；"
msgstr "Training dataset (Training dataset)：The dataset used for training and optimizing model parameters;"

#: ../../source/getting-started/beginner/linear-regression.rst:409
msgid "验证集（Validation dataset）：用来在训练过程中及时评估模型性能的数据集；"
msgstr "Validation dataset：is a dataset used to evaluate model performance in time during training;"

#: ../../source/getting-started/beginner/linear-regression.rst:410
msgid "测试集（Test dataset）：模型训练完成后，最终对模型预测能力进行测试的数据集；"
msgstr "Test dataset (Test dataset)：After the model training is completed, the final data set for testing the predictive ability of the model;"

#: ../../source/getting-started/beginner/linear-regression.rst:412
msgid "验证集有时候也被叫做开发集（Develop dataset）， 它本身虽然在训练模型的过程中被用于做及时验证， 但仅用于计算损失，不会参与到反向传播和参数优化的流程中去。 如果在训练过程中发现训练集上的损失在降低，而验证集上的损失在上升， 则表示发生了过拟合现象。"
msgstr "The validation set is sometimes called the Develop dataset. Although it is used for real-time verification in the process of training the model, it is only used to calculate the loss and will not participate in the process of backpropagation and parameter optimization. go. If during training you find that the loss on the training set is decreasing and the loss on the validation set is increasing, then overfitting has occurred."

#: ../../source/getting-started/beginner/linear-regression.rst:418
msgid "验证集还能够帮助我们调整除 Epoch 外的超参数，但具体做法不会在本教程中介绍。"
msgstr "The validation set can also help us tune hyperparameters other than Epoch, but we won't cover that in this tutorial."

#: ../../source/getting-started/beginner/linear-regression.rst:420
msgid "一些公开数据集会为使用者划分好训练集与测试集（我们在后面的教程会看到）， 测试集在最终用做模型性能测试前，要被看作是 “从未见过的”、“无法使用的” 数据。"
msgstr "Some public datasets will be divided into training and test sets for users (we will see in the following tutorials), and the test set should be regarded as \"never seen\" before it is finally used for model performance testing. Unusable\" data."

#: ../../source/getting-started/beginner/linear-regression.rst:423
msgid "方便起见，我们使用 Scikit-learn 中提供的接口对住房数据集进行划分："
msgstr "For convenience, we use the interface provided in Scikit-learn to divide the housing dataset into："

#: ../../source/getting-started/beginner/linear-regression.rst:437
msgid "从原始的数据集中，我们取出了 80% 作为训练集，20% 作为测试集。"
msgstr "From the original dataset, we took 80% as the training set and 20% as the test set."

#: ../../source/getting-started/beginner/linear-regression.rst:439
msgid "接下来我们需要进一步从训练集中划分出其 25% 作为验证集："
msgstr "Next we need to further divide its 25% from the training set as the validation set："

#: ../../source/getting-started/beginner/linear-regression.rst:448
msgid "最终，我们划分出的训练集：验证集：测试集的占整体数据集比例为 3:1:1."
msgstr "Finally, the ratio of training set：, validation set：, and test set to the overall dataset is 3:1:1."

#: ../../source/getting-started/beginner/linear-regression.rst:451
msgid "练习：多元线性回归"
msgstr "Exercise：Multiple Linear Regression"

#: ../../source/getting-started/beginner/linear-regression.rst:453
msgid "准备训练集、验证集和测试集对应的 :class:`~.ArrayDataset`, :class:`~.Sampler` 以及 :class:`~.DataLoader`. 通常在训练模型时，我们会选择打乱训练集样本的顺序，但由于我们在划分数据集时调用的接口已经执行了乱序操作， 这里我们直接选择使用顺序采样器，并将批块大小设置成 128.（这个超参数你可以自己调整）"
msgstr "Prepare :class:`~.ArrayDataset`, :class:`~.Sampler` and :class:`~.DataLoader` corresponding to the training set, validation set and test set. Usually when training the model, we choose to shuffle the order of the training set samples, but Since the interface we call when dividing the dataset has performed the out-of-order operation, here we directly choose to use the sequential sampler and set the batch size to 128. (You can adjust this hyperparameter yourself)"

#: ../../source/getting-started/beginner/linear-regression.rst:459
msgid "注意在下面的代码中，我们用到了 :mod:`.data.transform` 模块， 该模块帮助我们在将数据加载时进行一些预处理（Preprocessing）变化操作， 比如归一化 :class:`~.Normalize` —— 通过计算训练集中样本的相关统计数据，对每个特征独立地进行归一化。 这样做的好处之一是优化器的学习率不用再根据输入数据的数值范围进行调整，通常可以从 0.01 开始。 且大多数机器学习算法会要求特征具有 0 均值和单位方差，否则可能表现不佳。"
msgstr "Note that in the code below, we use the :mod:`.data.transform` module, which helps us perform some preprocessing changes when loading the data, such as normalize :class:`~.Normalize` —— Each feature is normalized independently by computing the relevant statistics of the samples in the training set. One of the benefits of this is that the learning rate of the optimizer no longer needs to be adjusted according to the numerical range of the input data, and can usually start from 0.01. And most machine learning algorithms will require features to have 0 mean and unit variance, otherwise they may perform poorly."

#: ../../source/getting-started/beginner/linear-regression.rst:468
msgid "数据归一化后，其损失变化等高线图也更加均匀，在进行梯度下降时，通常能够更快地收敛。"
msgstr "After the data is normalized, its loss change contour plot is also more uniform, and when doing gradient descent, it usually converges faster."

#: ../../source/getting-started/beginner/linear-regression.rst:470
msgid "我们在上一个教程中随机生成数据时刻意地符合均匀分布，因此无需进行归一化预处理。"
msgstr "We randomly generated the data in the previous tutorial by intentionally following a uniform distribution, so no normalization preprocessing was required."

#: ../../source/getting-started/beginner/linear-regression.rst:472
msgid "我们在后续的教程中会看到更多的数据预处理操作，预处理操作有不同的类型和意义， 感兴趣的读者可以提前查阅 MegEngine 文档等材料，目前只需要对此留有一个基本印象即可。"
msgstr "We will see more data preprocessing operations in subsequent tutorials. Preprocessing operations have different types and meanings. Interested readers can refer to MegEngine documentation and other materials in advance. Can."

#: ../../source/getting-started/beginner/linear-regression.rst:494
msgid "按照上面演示过的内容，定义我们的线性回归模型，同时准备好 :class:`~.GradManager` 与 :class:`~.Optimizer` ："
msgstr "Define our linear regression model as demonstrated above, and prepare :class:`~.GradManager` and :class:`~.Optimizer` ："

#: ../../source/getting-started/beginner/linear-regression.rst:513
msgid "万事俱备，可以开始训练我们的模型了："
msgstr "Everything is ready to start training our model："

#: ../../source/getting-started/beginner/linear-regression.rst:517
msgid "回忆一下，如果是批梯度下降算法，每经过一轮（Epoch）训练，参数只会迭代更新一次。 而现在我们使用小批量梯度下降算法，在每一轮训练中，我们会有多个 ``train_step``, 或者说有多个 ``iter`` / ``batch``. 每个 Step 都会从 ``train_dataloader`` 中读取 ``batch_size`` 大小的数据， 执行我们在上个教程已经认识的前向计算、反向计算以及参数更新过程，这时得到的损失是在小批量数据上的平均损失。 根据这个平均损失，对所有的参数计算关于损失的梯度，并进行更新。接着进入下一个 Step, 用下一批数据... 循环往复。"
msgstr "Recall that if it is a batch gradient descent algorithm, the parameters will only be updated iteratively once after each epoch of training. Now we use the mini-batch gradient descent algorithm. In each round of training, we will have multiple ``train_step``, or multiple ``iter`` / ``batch``. Each step will start from ``train_dataloader`` reads data of size ``batch_size``, and performs the forward calculation, reverse calculation and parameter update process that we have known in the previous tutorial. The loss obtained at this time is on the small batch data. average loss. Based on this average loss, the gradients with respect to the loss are calculated for all parameters and updated. Then go to the next Step, use the next batch of data... The cycle repeats."

#: ../../source/getting-started/beginner/linear-regression.rst:523
msgid "为了及时地观察训练状态， 通常我们每隔一定的 ``train_step``, 就会观察一下当前这批训练数据上损失的变化情况（输出 Log 信息）。 经过一轮完整的训练后，我们会计算在整个训练数据集上的平均损失 ``training_loss`` 作为这一轮训练的结果。 然后我们要立即用验证集来评估当前模型的性能，在评估模型性能时，由于只需要进行前向计算和计算损失， 因此可以选用和损失函数（本教程中为 MSE）不同的评估指标， 比如回归任务中可以使用平均绝对误差 MAE, 在 MegEngine 中为 :func:`~.l1_loss` 接口。 为了更加严谨地对比，你也可以在训练时每个 Batch 数据计算 MSE, 而完成一轮训练后在整个训练集上使用 MAE 进行评估。"
msgstr "In order to observe the training status in time, we usually observe the change of the loss on the current batch of training data (output Log information) every certain ``train_step``. After a full training round, we calculate the average loss ``training_loss`` over the entire training dataset as the result of this round of training. Then we need to immediately use the validation set to evaluate the performance of the current model. When evaluating the performance of the model, since only forward calculation and loss calculation are required, we can choose evaluation metrics different from the loss function (MSE in this tutorial), such as Mean absolute error MAE can be used in regression tasks, which is :func:in MegEngine `~.l1_loss` interface. For a more rigorous comparison, you can also calculate MSE for each batch of data during training, and use MAE for evaluation on the entire training set after a round of training."

#: ../../source/getting-started/beginner/linear-regression.rst:572
msgid "注意：由于最后一个 Batch 的样本数量 ``len(X)`` 可能不足 Batch size 个数， 因此我们在统计损失的总体平均值时，为了严谨性，做法是将每个 Batch 整体损失累加，最后除以总样本数。"
msgstr "Note：, because the number of samples of the last Batch ``len(X)`` may be less than the number of Batch size, so when we count the overall average of the loss, for the sake of rigor, the method is to accumulate the overall loss of each Batch, and finally Divide by the total number of samples."

#: ../../source/getting-started/beginner/linear-regression.rst:575
msgid "如果发现从某一轮训练开始，验证集上的损失持续不断地上升，则有可能发生了过拟合。"
msgstr "If you find that the loss on the validation set continues to rise from a certain round of training, it is possible that overfitting has occurred."

#: ../../source/getting-started/beginner/linear-regression.rst:577
msgid "最后在测试集上进行真正的测试（和验证集的评估方式应当完全一致）："
msgstr "Finally, do the real test on the test set (the evaluation method should be exactly the same as the validation set)："

#: ../../source/getting-started/beginner/linear-regression.rst:594
msgid "在本教程中，如果损失值有收敛的趋势，则表明小批量梯度下降法成功地对线性回归模型完成了优化，我们只关注其有效性，而不关注最终效果。 如果你尝试用最小二乘法去求出本教程中线性回归模型的参数解析解，会发现理想的损失应当能收敛到 0.6 附近。 或许你在调整超参数训练模型的过程中会发现损失值出现 NaN 的情况，亦或者是损失下降到某个值后便不再继续下降，而是在一个区间摆动。 导致这些情况产生的原因有很多，比如学习率过大等等，如果你目前还不能够很好地分析出背后的原因并对症下药。 不用担心，我们会在多次的任务和代码实践中，慢慢地积累经验。"
msgstr "In this tutorial, if the loss value has a tendency to converge, it means that the mini-batch gradient descent method successfully completed the optimization of the linear regression model, and we only focus on its effectiveness, not the final effect. If you try to use the least squares method to find an analytical solution to the parameters of the linear regression model in this tutorial, you will find that the ideal loss should converge to around 0.6. Maybe you will find that the loss value is NaN in the process of adjusting the hyperparameter training model, or the loss will not continue to decline after it drops to a certain value, but swings in a range. There are many reasons for these situations, such as too large learning rate, etc. If you are not able to analyze the reasons behind it well and prescribe the remedy. Don't worry, we will slowly accumulate experience in many tasks and code practice."

#: ../../source/getting-started/beginner/linear-regression.rst:600
msgid "本教程的主要目的是帮助你掌握小批量梯度下降的实现，以及模型训练、验证和测试的完整流程。"
msgstr "The main purpose of this tutorial is to help you master the implementation of mini-batch gradient descent and the complete flow of model training, validation and testing."

#: ../../source/getting-started/beginner/linear-regression.rst:604
msgid "本教程的对应源码： :docs:`examples/beginner/linear-regression.py`"
msgstr "The corresponding source code for this tutorial： :docs:`examples/beginner/linear-regression.py`"

#: ../../source/getting-started/beginner/linear-regression.rst:607
msgid "总结：初探机器学习"
msgstr "Summary：A first look at machine learning"

#: ../../source/getting-started/beginner/linear-regression.rst:609
msgid "让我们再次回顾一下关于机器学习的定义："
msgstr "Let's：the definition of machine learning again"

#: ../../source/getting-started/beginner/linear-regression.rst:611
msgid "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."
msgstr ""

#: ../../source/getting-started/beginner/linear-regression.rst:615
msgid "如果一个计算机程序能够根据经验 E 提升在某类任务 T 上的性能 P, 则我们说程序从经验 E 中进行了学习。"
msgstr "A computer program is said to have learned from experience E if it can improve its performance P on some class of tasks T based on experience E."

#: ../../source/getting-started/beginner/linear-regression.rst:618
msgid "在本教程中，我们的任务 T 是用线性回归模型去预测房价，经验 E 来自于我们的训练数据集， 我们对原始数据集进行了划分，并且在训练集上统计出了均值和方差，方便进行输入数据的归一化预处理。 我们评估模型好坏（性能 P）时有不同的倾向，训练损失使用了 MSE, 而在评估时使用了 MAE. 为了调整超参数 Epoch, 我们划分出了验证集来及时评估模型性能，作为对最终测试的模拟。"
msgstr "In this tutorial, our task T is to use a linear regression model to predict housing prices. The experience E comes from our training data set. We divide the original data set, and count the mean and variance on the training set, which is convenient for Perform normalization preprocessing on the input data. We have different tendencies when evaluating the quality of the model (performance P), using MSE for training loss and MAE for evaluation. In order to adjust the hyperparameter Epoch, we divided the validation set to evaluate the model performance in time, as a final Simulation of the test."

#: ../../source/getting-started/beginner/linear-regression.rst:623
msgid "机器学习的流程大抵类似：数据获取（标注）；数据分析与处理；特征工程；模型选择；优化模型；验证模型性能... 每一个环节又可以衍生出更多的理论知识和实践经验，在初学阶段不必要求自己理解全部，否则容易陷入细节。 线性回归模型可以认为是机器学习领域的 Hello, World! 相信你对此已经有了一个基本的认识。"
msgstr "The process of machine learning is roughly similar to：data acquisition (labeling); data analysis and processing; feature engineering; model selection; At the beginning stage, you don't have to ask yourself to understand everything, otherwise it is easy to get caught in the details. The linear regression model can be considered as Hello, World in the field of machine learning! I believe you already have a basic understanding of it."

#: ../../source/getting-started/beginner/linear-regression.rst:627
msgid "在下一个教程中，将会接触手写数字的分类任务，并且尝试继续使用线性模型和梯度下降法来完成挑战。"
msgstr "In the next tutorial, you will be exposed to the classification task of handwritten digits and try to continue using linear models and gradient descent to complete the challenge."

#: ../../source/getting-started/beginner/linear-regression.rst:630
msgid "拓展材料"
msgstr "Expansion material"

#: ../../source/getting-started/beginner/linear-regression.rst:634
msgid "你会在不同的数学材料中看到不同的数学记号表示，搞清楚符号定义很关键。"
msgstr "You will see different representations of mathematical notation in different mathematical materials, and it is crucial to understand the notation definitions."

#: ../../source/getting-started/beginner/linear-regression.rst:636
msgid "在线性代数中向量一般定义为列向量 :math:`\\vec{x}=\\left(x_{1} ; x_{2} ; \\ldots x_{n}\\right)`, 对于单样本的线性回归有 :math:`y=\\vec{w}^{T} \\vec{x}+b`."
msgstr "In linear algebra, a vector is generally defined as a column vector :math:`\\vec{x}=\\left(x_{1} ; x_{2} ; \\ldots x_{n}\\right)`, for one-sample linear regression there is :math:`y=\\vec{w}^{T} \\vec{x}+b`."

#: ../../source/getting-started/beginner/linear-regression.rst:639
msgid "在 CS229 中的表示形式是，对于一共有 :math:`n` 个样例，特征数量为 :math:`d` 的数据集 :math:`(X,\\vec{y})` 有："
msgstr "The representation in CS229 is that for a total of :math:`n` examples, a dataset :math:`(X,\\vec{y})` with a feature number of :math:`d` has："

#: ../../source/getting-started/beginner/linear-regression.rst:641
msgid "X=\\left[\\begin{array}{c}\n"
"-\\left(x^{(1)}\\right)^{T}- \\\\\n"
"-\\left(x^{(2)}\\right)^{T}- \\\\\n"
"\\vdots \\\\\n"
"-\\left(x^{(n)}\\right)^{T}-\n"
"\\end{array}\\right] \\quad\n"
"\\vec{y}=\\left[\\begin{array}{c}\n"
"y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(n)}\n"
"\\end{array}\\right] \\quad\n"
"\\hat{\\vec{y}} = X \\theta"
msgstr "X=\\left[\\begin{array}{c}\n"
"-\\left(x^{(1)}\\right)^{T}- \\\\\n"
"-\\left(x^{(2)}\\right)^{T}- \\\\\n"
"\\vdots \\\\\n"
"-\\left(x^{(n)}\\right)^{T}-\n"
"\\end{array}\\right] \\quad\n"
"\\vec{y}=\\left[\\begin{array}{c}\n"
"y^{(1) } \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(n)}\n"
"\\end{array}\\right] \\quad\n"
"\\hat{\\vec{y}} = X \\theta"

#: ../../source/getting-started/beginner/linear-regression.rst:655
msgid "其中每个样本 :math:`x^{(i)} = (x^{(i)}_1;x^{(i)}_2;\\ldots ;x^{(i)}_n)`, :math:`x^{(i)}_j` 表示第 :math:`i` 个样本的第 :math:`j` 维特征， :math:`\\theta` 为参数。"
msgstr "where each sample :math:`x^{(i)} = (x^{(i)}_1;x^{(i)}_2;\\ldots ;x^{(i)}_n)`, :math:`x ^{(i)}_j` represents the :math:`j` dimension feature of the :math:`i` sample, and :math:`\\theta` is the parameter."

#: ../../source/getting-started/beginner/linear-regression.rst:658
msgid "注意到上面的向量用上加箭头 :math:`\\vec{x}` 的形式来表示，这是因为在板书的过程中， 粗体的 :math:`\\boldsymbol{x}` 与 :math:`x` 很难区分，因此采用了这种形式； 而在印刷体材料中，粗体字还是很容易区分的，因此本教程使用粗体罗马字表示向量。 但我们在使用 Tensor 实际编程时， 标量、向量、列向量和行向量 :ref:`有实质区别 <tensor-shape>` 。"
msgstr "Note that the above vector is represented by an arrow :math:`\\vec{x}`, this is because in the process of writing on the blackboard, the bold :math:`\\boldsymbol{x}` and :math:`x` are difficult to distinguish, so the use of This form; while in printed material, bold letters are easily distinguishable, so this tutorial uses bold roman letters to represent vectors. But when we actually program with Tensor, there are substantial differences between scalar, vector, column vector and row <tensor-shape>:ref:."

#: ../../source/getting-started/beginner/linear-regression.rst:663
msgid "本教程中没有使用上标的形式来表示索引，是因为在我们进行编程实践时， 元素索引均为 ``X[i][j]`` 这种语法，通常用第一个维度表示样本索引。 各个维度之间没有特殊差异，因此不用上下标进行区分。"
msgstr "This tutorial does not use the superscript form to represent the index, because in our programming practice, the element index is ``X[i][j]`` This syntax, usually the first dimension represents the sample index. There are no special differences between the dimensions, so no subscripts are used to distinguish them."

#: ../../source/getting-started/beginner/linear-regression.rst:669
msgid "对于回归模型，还有着其它可选的评估指标， 如 RMSE, RMSLE, R Square （R2 score）等， 本教程中只选择了易于理解的 MSE 和 MAE, 感兴趣者可自己去了解一下， 看看换用不同的评估指标，对最终模型的预测性能会有什么样的影响。"
msgstr "For the regression model, there are other optional evaluation indicators, such as RMSE, RMSLE, R Square (R2 score), etc. In this tutorial, only the easy-to-understand MSE and MAE are selected. Those who are interested can find out for themselves and see Switching to different evaluation metrics will affect the prediction performance of the final model."

#: ../../source/getting-started/beginner/linear-regression.rst:677
msgid "如果我们的任务是给定数据集，要求预测房价（不一定要使用线性回归模型）， 则完全可以使用其它的机器学习模型如随机森林等来完成目的， 但本系列 MegEngine 教程并不是一门机器学习课程，因此不会有对各种模型更加详细的介绍。"
msgstr "If our task is to predict housing prices for a given data set (not necessarily using a linear regression model), other machine learning models such as random forests can be used to accomplish the purpose, but this series of MegEngine tutorials is not a machine Learn the course, so there won't be a more detailed introduction to the various models."

#: ../../source/getting-started/beginner/linear-regression.rst:681
msgid "在 Kaagle 机器学习竞赛平台上提供了本教程中提到的加利福利亚数据集的 `修改版本 <https://www.kaggle.com/camnugent/california-housing-prices>`_ ， 非常适合感兴趣的用户去实验其它的模型和算法（有许多用户分享了自己的方案），可作为拓展练习。"
msgstr "The `modified version <https://www.kaggle.com/camnugent/california-housing-prices>of the California dataset mentioned in this tutorial is provided on the Kaagle machine learning competition platform, which is very suitable for interested users to experiment with other models and algorithms (many users have shared their own program), which can be used as an extension exercise."

