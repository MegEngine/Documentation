msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-09-01 21:02+0800\n"
"PO-Revision-Date: 2022-09-02 12:10\n"
"Last-Translator: \n"
"Language: en_US\n"
"Language-Team: English\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: en\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/getting-started/beginner/linear-classification.po\n"
"X-Crowdin-File-ID: 8382\n"

#: ../../source/getting-started/beginner/linear-classification.rst:5
msgid "MegEngine 实现线性分类"
msgstr "MegEngine implements linear classification"

#: ../../source/getting-started/beginner/linear-classification.rst:6
msgid "本教程涉及的内容"
msgstr "What this tutorial covers"

#: ../../source/getting-started/beginner/linear-classification.rst:9
msgid "对计算机视觉领域的图像编码有一个基础认知，了解机器学习中的分类任务；"
msgstr "Have a basic understanding of image coding in the field of computer vision and understand the classification tasks in machine learning;"

#: ../../source/getting-started/beginner/linear-classification.rst:10
msgid "思考线性回归模型经过什么样的变化，可用于解决分类任务，以及该采取什么样的评估指标；"
msgstr "Think about what changes the linear regression model has undergone to solve the classification task, and what evaluation metrics should be used;"

#: ../../source/getting-started/beginner/linear-classification.rst:11
msgid "根据前面的介绍，使用 MegEngine 实现线性分类器，完成 MNIST 手写数字分类任务。"
msgstr "According to the previous introduction, use MegEngine to implement a linear classifier to complete the MNIST handwritten digit classification task."

#: ../../source/getting-started/beginner/linear-classification.rst:13
msgid "可视化操作"
msgstr "Visual operation"

#: ../../source/getting-started/beginner/linear-classification.rst:16
msgid "本教程中进行了一些数据可视化操作，如果你没有 `Matplotlib <https://matplotlib.org/stable/index.html>`_ 的使用经验，则只需关注输出结果。"
msgstr "There is some data visualization in this tutorial, if you have no experience with `Matplotlib <https://matplotlib.org/stable/index.html>`_, just focus on the output."

#: ../../source/getting-started/beginner/linear-classification.rst:20
msgid "获取原始数据集"
msgstr "Get the original dataset"

#: ../../source/getting-started/beginner/linear-classification.rst:22
msgid "在上一个教程中，我们使用了 Scikit-learn 中的接口来获取加利福尼亚住房数据， 并提到许多 Python 库和框架会封装好获取一些数据集的接口，方便用户调用。 MegEngine 也不例外，在 :mod:`.data.dataset` 模块中，可以 :ref:`megengine-dataset` 来获取原始数据集，如本次教程所需用到的 :class:`~.MNIST` 数据集。"
msgstr "In the previous tutorial, we used the interface in Scikit-learn to get California housing data, and mentioned that many Python libraries and frameworks will encapsulate the interface to get some datasets, which is convenient for users to call. MegEngine is no exception. In :mod:`.data.dataset` module, you can get :ref:`megengine-dataset` to get the original data set, such as :class:`~.MNIST` data set used in this tutorial."

#: ../../source/getting-started/beginner/linear-classification.rst:37
msgid "和上个教程中得到的数据格式不同，这里得到的是已经划分好的训练集 ``train_dataset`` 和测试集 ``test_dataset``, 且均已经封装成了 MegEngine 中可提供给 :class:`~.DataLoader` 的 :class:`~.Dataset` 类型（具体的介绍可参考 :ref:`dataset-guide` ）。 其中的每个元素是由单个样本 :math:`X_i` 和标记 :math:`y_i` 组成的元组 :math:`(X_i, y_i)` ："
msgstr "Different from the data format obtained in the previous tutorial, what is obtained here is the already divided training set ``train_dataset`` and test set ``test_dataset``, and both have been encapsulated into MegEngine and can be provided to :class:`~. :class:`~.Dataset` type of DataLoader` (for details, please refer to :ref:`dataset-guide`). where each element is a tuple :math:`(X_i, y_i)` ：consisting of a single sample :math:`X_i` and a token :math:`y_i`"

#: ../../source/getting-started/beginner/linear-classification.rst:45
msgid "为了方便进行接下来的讨论，我们这里特意地将训练数据的样本和标签拆分出来："
msgstr "In order to facilitate the following discussion, we deliberately split the samples and labels of the training data into："

#: ../../source/getting-started/beginner/linear-classification.rst:56
msgid "统计训练样本的均值和标准差信息，用于数据预处理时进行归一化："
msgstr "Statistical mean and standard deviation information of training samples, normalized to：for data preprocessing"

#: ../../source/getting-started/beginner/linear-classification.rst:64
msgid "对于 MNIST 这样的经典数据集，通常在数据集主页会提供这些统计信息， 或可以借助搜索引擎在网络上找到其它人已经统计好的数据。"
msgstr "For classic datasets such as MNIST, these statistics are usually provided on the dataset homepage, or you can find data that others have already calculated on the Internet with the help of search engines."

#: ../../source/getting-started/beginner/linear-classification.rst:68
msgid "了解数据集信息"
msgstr "Learn about dataset information"

#: ../../source/getting-started/beginner/linear-classification.rst:70
msgid "回忆一下，在上一个教程中，我们的住房样本数据（未拆分前）形状为 :math:`(20640, 8)`, 表示一共有 20640 个样本，每个样本具有 8 个属性值，也可以说是特征向量的维度为 8. 而这里训练数据的形状是 :math:`(60000, 28, 28, 1)`, 如果 60000 是训练样本的总数，那么后面的 :math:`(28, 28, 1)` 形状又代表什么呢？"
msgstr "Recall that in the last tutorial, our housing sample data (before splitting) has a shape of :math:`(20640, 8)`, which means that there are 20640 samples in total, each with 8 attribute values. It can also be said that is the dimension of the feature vector is 8. And the shape of the training data here is :math:`(60000, 28, 28, 1)`, if 60000 is the total number of training samples, then the following :math:`(28, 28, 1)` shape What does it stand for?"

#: ../../source/getting-started/beginner/linear-classification.rst:77
msgid "查询 `MNIST 官网主页 <http://yann.lecun.com/exdb/mnist/>`_ :footcite:p:`lecun2010mnist` 介绍信息可知： 手写数字数据集中包含 60,000 张训练图像和 10,000 张测试图像，每张图片是 28x28 像素的灰度图。 Google 提供了一个叫做 `Know Your Data <https://knowyourdata.withgoogle.com/>`_ 的网站，能够以可视化的方式帮助我们了解一些经典公开数据集， 包括此处用到的 `MNIST 数据集 <https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=mnist>`_ 。"
msgstr "Query the `MNIST official website homepage <http://yann.lecun.com/exdb/mnist/>`_ :footcite:p:`lecun2010mnist` for the introduction information.： The handwritten digits dataset contains 60,000 training images and 10,000 testing images, each image is a grayscale image of 28x28 pixels. Google provides a website called `Know Your Data <https://knowyourdata.withgoogle.com/>`_ that can help us understand some classic public datasets in a visual way, including the `MNIST dataset <https://knowyourdata-tfds.withgoogle.com/#tab=STATS&dataset=mnist>`_ used here."

#: ../../source/getting-started/beginner/linear-classification.rst:85
msgid "标记 :math:`y \\in \\left\\{ 0, 1, \\ldots , 9 \\right\\}` 是离散的数值， 与上一个教程中的样本标记值（房价） :math:`y \\in \\mathbb R` 不一样。"
msgstr "Marker :math:`y \\in \\left\\{ 0, 1, \\ldots , 9 \\right\\}` is a discrete numerical value, not the same as the sample marker value (house price) :math:`y \\in \\mathbb R` in the previous tutorial Same."

#: ../../source/getting-started/beginner/linear-classification.rst:88
msgid "运行下面的可视化代码，可以帮助你对数据有一个直观的理解（目前不用理解代码实现）："
msgstr "Running the following visualization code can help you have an intuitive understanding of the data (no need to understand the code implementation at present)："

#: ../../source/getting-started/beginner/linear-classification.rst:114
msgid "在不少的公开可获取的数据集中，数据集的提供者已经为使用者划分好了训练集和测试集。 但我们在上一个教程中提到了，我们至少还应提供一个验证集来避免在训练集上过拟合。 而在实际实践时，对这样的数据集会出现许多奇怪的划分情况，以我们已有的 MNIST 训练集与测试集为例："
msgstr "In many publicly available datasets, dataset providers have divided training and test sets for users. But as we mentioned in the last tutorial, we should at least also provide a validation set to avoid overfitting on the training set. In actual practice, there will be many strange divisions of such data sets. Take our existing：training set and test set as an example."

#: ../../source/getting-started/beginner/linear-classification.rst:118
msgid "如果我们将训练集中的 60000 个样本按照 5:1 的比例进一步划分成训练集和验证集。 这样的处理方式和上一个教程中一致，此时测试集负责模拟将来的生产环境中 “不可预知的” 数据，只用一次；"
msgstr "If we further divide the 60000 samples in the training set into training set and validation set according to the ratio of 5:1. This way of handling is the same as in the previous tutorial, at this time the test set is responsible for simulating the \"unpredictable\" data in the future production environment, and it is only used once;"

#: ../../source/getting-started/beginner/linear-classification.rst:121
msgid "如果我们将当前测试集中的 10000 个样本按照 1:1 的比例进一步划分成验证集和测试集，则有不同的含义。 这种做法的出发点是： “验证集” 应当是对 “测试集” 的模拟，应当尽量和测试集保持同一分布。 比如在机器学习竞赛中，提供给选手的测试集 :math:`T_a` 可能就是从原本的测试集 :math:`T` 中划分出来的一部分， 在公榜排名中给出的是在 :math:`T_a` 上的成绩，而最终的排名将在真正的测试集 :math:`T_b` 进行。"
msgstr "If we further divide the 10,000 samples in the current test set into a validation set and a test set in a ratio of 1:1, there are different implications. The starting point of this approach is that the \"validation： \" should be a simulation of the \"test set\" and should try to maintain the same distribution as the test set. For example, in the machine learning competition, the test set :math:`T_a` provided to the players may be a part of the original test set :math:`T`, and the ranking in the public list is given on the :math:`T_a` score, and the final ranking will be done on the real test set :math:`T_b`."

#: ../../source/getting-started/beginner/linear-classification.rst:126
msgid "如果我们在这里并不对训练集和测试集进行进一步划分，而是每经过一轮训练，在已有的测试集上进行验证， 即意味着这个测试集其实起到的是验证集的作用。那么真实的测试集在哪里呢？ 来自于我们的生产环境，毕竟测试集的定义本就是 “不可提前预知” 的数据，很多时候无法提前获取。"
msgstr "If we do not further divide the training set and the test set here, but verify on the existing test set after each round of training, it means that the test set actually plays the role of the verification set. So where is the real test set? It comes from our production environment. After all, the definition of the test set is \"unpredictable\" data, and it is often impossible to obtain it in advance."

#: ../../source/getting-started/beginner/linear-classification.rst:130
msgid "总而言之，数据集的划分策略更多取决于我们能够获得的数据是什么样的，是否与将来使用情景下的数据分布一致。 测试集和验证集这两个术语的意义经常被混用（或者相互指代），因此最简单的判断方法是看用途。 需要确保的是：在模型训练的过程中，及时地验证模型性能是不可缺少的过程。"
msgstr "All in all, the division strategy of the data set depends more on what kind of data we can obtain and whether it is consistent with the data distribution in future usage scenarios. The terms test set and validation set are often used interchangeably (or refer to each other), so the easiest way to judge is by purpose. What needs to be：is that in the process of model training, timely verification of model performance is an indispensable process."

#: ../../source/getting-started/beginner/linear-classification.rst:134
msgid "在本教程中，将 MNIST 测试集视作验证集，即每完成一轮参数更新后，将评估模型在测试集上的性能。"
msgstr "In this tutorial, the MNIST test set is treated as the validation set, that is, after each round of parameter update, the performance of the model on the test set will be evaluated."

#: ../../source/getting-started/beginner/linear-classification.rst:137
msgid "非结构化数据：图片"
msgstr "Unstructured data：pictures"

#: ../../source/getting-started/beginner/linear-classification.rst:139
msgid "加利福利亚住房数据集中，我们描述住房特征时可以用特征向量 :math:`\\mathrm{x} \\in \\mathbb{R}^d` 来进行表示。 整个数据集可以借助二维表格（矩阵）来进行表达和存储，我们称这样的数据叫做结构化数据（Structured data）。 而在现实生活中，人类更容易接触和理解的是非结构化数据（Unstructured data），比如视频、音频、图片等等..."
msgstr "In the California housing dataset, we can use the feature vector :math:`\\mathrm{x} \\in \\mathbb{R}^d` to describe the characteristics of housing. The entire data set can be expressed and stored with the help of a two-dimensional table (matrix). We call such data structured data. In real life, it is easier for humans to touch and understand unstructured data, such as video, audio, pictures, etc..."

#: ../../source/getting-started/beginner/linear-classification.rst:143
msgid "计算机如何理解、表达和存储这样的数据呢？本教程将以 MNIST 中的手写数字图片样本为例进行说明："
msgstr "How do computers understand, represent, and store such data? This tutorial will illustrate the example of a sample handwritten digit picture in："

#: ../../source/getting-started/beginner/linear-classification.rst:152
msgid "MNIST 数据集中的图像均为位图图像（Bitmap），也叫光栅图像、栅格图像，与矢量图的概念相对； 每张图由许多被称作像素（Pixel, 即图像元素 Pictrue element）的点组成。 我们取出一张标记为 3 的图片样本 ``X_train[idx]`` 进行观察，发现其高度（Height）和宽度（Weight）均为 28 个像素， 与单个样本的形状 :math:`(28, 28, 1)` 对应；最后一个维度表示通道（Channel）数量， 由于 MNIST 中的图片为灰度图，因此通道数为 1. 对于 RGB 颜色制式的彩图，其通道数则为 3，我们会在下一个教程中看到这样的数据。"
msgstr "The images in the MNIST dataset are all bitmap images (Bitmap), also called raster images, raster images, as opposed to the concept of vector graphics; each image consists of many points called pixels (Pixel, Pictrue element) composition. We take a picture sample marked 3 ``X_train[idx]`` for observation, and find that its height (Height) and width (Weight) are both 28 pixels, which is similar to the shape of a single sample :math:`(28, 28, 1) ` Corresponding; the last dimension represents the number of channels. Since the image in MNIST is a grayscale image, the number of channels is 1. For color images in the RGB color system, the number of channels is 3, which we will cover in the next tutorial. see data like this."

#: ../../source/getting-started/beginner/linear-classification.rst:159
msgid "现在让我们将这张灰度图进行 3 维可视化，以助于培养直觉理解："
msgstr "Now let's visualize this grayscale image in 3 dimensions to help develop an intuitive understanding of："

#: ../../source/getting-started/beginner/linear-classification.rst:173
msgid "在灰度图中，每个像素值用 0（黑色）~ 255（白色）进行表示，即一个 int8 所能表示的范围。"
msgstr "In a grayscale image, each pixel value is represented by 0 (black) ~ 255 (white), which is the range that an int8 can represent."

#: ../../source/getting-started/beginner/linear-classification.rst:175
msgid "我们通常称呼 :math:`(60000, 28, 28, 1)` 这样的数据为 NHWC 布局，在后面我们会看到更多的布局形式。"
msgstr "We usually call :math:`(60000, 28, 28, 1)` such data as NHWC layout, we will see more layout forms later."

#: ../../source/getting-started/beginner/linear-classification.rst:178
msgid "图片特征的简单处理"
msgstr "Simple processing of image features"

#: ../../source/getting-started/beginner/linear-classification.rst:180
msgid "不同类型数据之间特征的区别"
msgstr "Differences in features between different types of data"

#: ../../source/getting-started/beginner/linear-classification.rst:182
msgid "在上一个教程中，线性模型的单样本输入是一个特征向量 :math:`\\mathrm{x} \\in \\mathbb{R}^d`;"
msgstr "In the previous tutorial, the one-shot input to the linear model was a feature vector :math:`\\mathrm{x} \\in \\mathbb{R}^d`;"

#: ../../source/getting-started/beginner/linear-classification.rst:183
msgid "而此处 MNSIT 数据集中的图片 :math:`\\mathsf{I}` 的特征空间为 :math:`\\mathbb{R}^{H \\times W \\times C}`."
msgstr "And the feature space of picture :math:`\\mathsf{I}` in the MNSIT dataset here is :math:`\\mathbb{R}^{H \\times W \\times C}`."

#: ../../source/getting-started/beginner/linear-classification.rst:185
msgid "想要满足线性模型对输入形状的需求，需要对输入样本的特征进行一定的处理。 最容易简单粗暴地实现 :math:`\\mathbb{R}^{H \\times W \\times C} \\mapsto \\mathbb{R}^d` 而又不丢失各元素数值信息的做法是使用 :func:`~.flatten` 扁平化操作："
msgstr "In order to meet the requirements of the linear model for the input shape, it is necessary to perform certain processing on the characteristics of the input samples. The easiest and crudest way to implement :math:`\\mathbb{R}^{H \\times W \\times C} \\mapsto \\mathbb{R}^d` without losing the numerical information of each element is to use the :func:`~.flatten` flattening operation："

#: ../../source/getting-started/beginner/linear-classification.rst:193
msgid "原始图像"
msgstr "The original image"

#: ../../source/getting-started/beginner/linear-classification.rst:206
msgid "Flatten 处理后"
msgstr "After Flatten processing"

#: ../../source/getting-started/beginner/linear-classification.rst:218
msgid "在 MegEngine 中对应的接口为 :meth:`.Tensor.flatten` 或 :func:`.functional.flatten`. 与在上个教程中提到的 :func:`~.expand_dims` 和 :func:`~.squeeze` 操作一样， :func:`~.flatten` 操作也可以通过 :func:`~.reshape` 实现。 但考虑到代码语义，此时应当尽量使用 :func:`~.flatten` 而不是 :func:`~.reshape`."
msgstr "The corresponding interface in MegEngine is :meth:`.Tensor.flatten` or :func:`.functional.flatten`. Same as :func:`~.expand_dims` and :func:`~.squeeze` operations mentioned in the previous tutorial, :func:` The ~.flatten` operation can also be implemented with :func:`~.reshape`. But considering the code semantics, you should try to use :func:`~.flatten` instead of :func:`~.reshape`."

#: ../../source/getting-started/beginner/linear-classification.rst:223
msgid "MegEngine 中的扁平化操作可以直接用于多个样本的情况（以便支持向量化计算）："
msgstr "The flattening operation in MegEngine can be directly used in the case of multiple samples (in order to support vectorized calculations)："

#: ../../source/getting-started/beginner/linear-classification.rst:235
msgid "经过简单处理，就能够把单个样本的线性预测模型和上一个教程中的 :math:`\\hat{y} = \\boldsymbol{w} \\cdot \\boldsymbol{x}+b` 形式相互联系起来了。"
msgstr "After a simple process, it is possible to correlate the single-sample linear prediction model with the form :math:`\\hat{y} = \\boldsymbol{w} \\cdot \\boldsymbol{x}+b` from the previous tutorial."

#: ../../source/getting-started/beginner/linear-classification.rst:239
msgid "分类任务的输出形式"
msgstr "The output form of the classification task"

#: ../../source/getting-started/beginner/linear-classification.rst:241
msgid "在线性回归任务中，我们的预测输出值 :math:`\\hat{y}` 在实数域 :math:`\\mathbb R` 上，是连续的。 而对于分类任务，通常给出的标记是由离散的值组成的集合，比如这里的 :math:`y \\in \\left\\{ 0, 1, \\ldots , 9 \\right\\}`. 此时要怎么样达成模型预测输出和标记形式上的统一呢？"
msgstr "In a linear regression task, our predicted output value :math:`\\hat{y}` is continuous on the real number domain :math:`\\mathbb R`. For classification tasks, the labels are usually given as a set of discrete values, such as :math:`y \\in \\left\\{ 0, 1, \\ldots , 9 \\right\\}` here. What about achieving unity of model prediction output and labeling form?"

#: ../../source/getting-started/beginner/linear-classification.rst:245
msgid "让我们先简化问题的形式，从最简单的分类情况，即从二分类任务开始讨论。"
msgstr "Let's simplify the problem form first, starting with the simplest classification case, the binary classification task."

#: ../../source/getting-started/beginner/linear-classification.rst:248
msgid "二分类任务"
msgstr "binary classification task"

#: ../../source/getting-started/beginner/linear-classification.rst:250
msgid "假定我们的手写数字分类任务简化成标记只含有 0 和 1 的情况，即 :math:`y \\in \\left\\{ 0, 1 \\right\\}`. 其中 0 意味着这张图片是手写数字 0, 而 1 意味着这张图片不是手写数字 0. 对于离散的标记，我们可以引入非线性的链接函数 :math:`g(\\cdot)` 来将线性输出映射为类别。 比如，可以将 :math:`f(\\boldsymbol{x})=\\boldsymbol{x} \\cdot \\boldsymbol{w}+b` 的输出以 0 为阈值（Threshold） 进行划分，认为凡是计算结果大于 0 的样本，都代表这张图片是手写数字 0； 而凡是计算结果小于 0 的样本，都代表这张图片不是手写数字 0. 故可以得到这样的预测模型："
msgstr "Suppose our handwritten digit classification task is simplified to the case where the tokens only contain 0 and 1, ie :math:`y \\in \\left\\{ 0, 1 \\right\\}`. Where 0 means that the picture is a handwritten digit 0, and 1 means that the image is not a handwritten number 0. For discrete tokens, we can introduce a non-linear link function :math:`g(\\cdot)` to map the linear output to categories. For example, you can divide the output of :math:`f(\\boldsymbol{x})=\\boldsymbol{x} \\cdot \\boldsymbol{w}+b` with 0 as the threshold (Threshold), and think that any sample whose calculation result is greater than 0 represents this picture is the handwritten number 0; and any sample whose calculation result is less than 0 means that the picture is not a handwritten number 0. Therefore, such a prediction model can be："

#: ../../source/getting-started/beginner/linear-classification.rst:258
msgid "\\hat{y}=\\mathbb{I}(f(\\boldsymbol{x})-\\text { threshold })=\n"
"\\left\\{\\begin{array}{lll}\n"
"1 & \\text { if } & f(\\boldsymbol{x})>\\text { threshold } \\\\\n"
"0 & \\text { if } & f(\\boldsymbol{x})<\\text { threshold }\n"
"\\end{array}\\right."
msgstr "\\hat{y}=\\mathbb{I}(f(\\boldsymbol{x})-\\text { threshold })=\n"
"\\left\\{\\begin{array}{lll}\n"
"1 & \\text { if } & f(\\boldsymbol{x})>\\text { threshold } \\\\\n"
"0 & \\text { if } & f(\\boldsymbol{x})<\\text { threshold }\n"
"\\end{array}\\right."

#: ../../source/getting-started/beginner/linear-classification.rst:266
msgid "其中 :math:`\\mathbb I` 是指示函数（也叫示性函数），也是我们这里用到的链接函数，但它并不常用。 原因很多，对于优化问题来说，它的数学性质并不好，不适合被用于梯度下降算法。 对于线性模型的输出 -100 和 -1000, 这种分段函数将二者都决策成标记为 0 的这个类别， 并不能体现出两个样本类内的区别 —— 举例来说，尽管二者都不是 0，但输出为 -1000 的样本应该比输出为 -100 的样本更加不像 0. 前者可能是 1, 后者可能是 6."
msgstr "Where :math:`\\mathbb I` is the indicator function (also called the indicative function), which is also the link function we use here, but it is not commonly used. There are many reasons, for optimization problems, its mathematical properties are not good and it is not suitable for gradient descent algorithm. For the outputs of the linear model -100 and -1000, this piecewise function decides both to be in the class marked 0, and does not reflect the difference within the two sample classes - for example, although both are Not 0, but a sample with an output of -1000 should be less like 0 than a sample with an output of -100. The former might be 1, the latter 6."

#: ../../source/getting-started/beginner/linear-classification.rst:273
msgid "在实践中，我们更常使用的链接函数是 Sigmoid 函数 :math:`\\sigma(\\cdot)` (也叫 Logistic 函数)："
msgstr "In practice, the link function we use more often is the Sigmoid function :math:`\\sigma(\\cdot)` (also called the Logistic function)："

#: ../../source/getting-started/beginner/linear-classification.rst:275
msgid "\\sigma(x)=\\frac{1}{1+\\exp (-x)}"
msgstr "\\sigma(x)=\\frac{1}{1+\\exp (-x)}"

#: ../../source/getting-started/beginner/linear-classification.rst:289
msgid "其中 :math:`\\exp`  指以自然常数 :math:`e` 为底的指数函数："
msgstr "where :math:`\\exp` refers to the exponential function：with the natural constant :math:`e` as the base"

#: ../../source/getting-started/beginner/linear-classification.rst:295
msgid "Logistic 是比利时数学家 Pierre François Verhulst 在 1844 或 1845 年在研究人口增长的关系时命名的， 和 Sigmoid 一样指代 S 形函数：起初阶段大致是指数增长；然后随着开始变得饱和，增加变慢最后，达到成熟时增加停止。以下是 Verhulst 对命名的解释："
msgstr "Logistic was named by Belgian mathematician Pierre François Verhulst in 1844 or 1845 when he was studying the relationship between population growth. Like Sigmoid, it refers to the sigmoid function：The initial stage is roughly exponential growth; then as it begins to become saturated, the increase slows down Finally, the increase stops when maturity is reached. Here's Verhulst's explanation of the naming："

#: ../../source/getting-started/beginner/linear-classification.rst:298
msgid "Verhulst writes \"We will give the name logistic [logistique] to the curve\" (1845 p.8). Though he does not explain this choice, there is a connection with the logarithmic basis of the function. Logarithm was coined by John Napier (1550-1617) from Greek logos (ratio, proportion, reckoning) and arithmos (number). Logistic comes from the Greek logistikos (computational). In the 1700's, logarithmic and logistic were synonymous. Since computation is needed to predict the supplies an army requires, logistics has come to be also used for the movement and supply of troops."
msgstr "Verhulst writes \"We will give the name logistic [logistique] to the curve\" (1845 p.8). Though he does not explain this choice, there is a connection with the logarithmic basis of the function. Logarithm was coined by John Napier (1550- 1617) from Greek logos (ratio, proportion, reckoning) and arithmos (number). Logistic comes from the Greek logistikos (computational). In the 1700's, logarithmic and logistic were synonymous. Since computation is needed to predict the supplies an army requires, logistics has come to be also used for the movement and supply of troops."

#: ../../source/getting-started/beginner/linear-classification.rst:305
msgid "Berkson coined logit (pronouced \"low-jit\") in 1944 as a contraction of logistic unit to indicate the unit of measurement (J. Amer. Stat. Soc. 39:357-365). Georg Rasch derives logit as a contraction of logistic transform (1980, p.80). Ben Wright derives logit as a contraction of log-odds unit. Logit is also used to characterize the logistic function in the way that probit (probability unit, coined by Chester Bliss about 1934), characterizes the cumulative normal function."
msgstr "Berkson coined logit (pronouced \"low-jit\") in 1944 as a contraction of logistic unit to indicate the unit of measurement (J. Amer. Stat. Soc. 39:357-365). Georg Rasch derives logit as a contraction of logistic transform (1980, p.80). Ben Wright derives logit as a contraction of log-odds unit. Logit is also used to characterize the logistic function in the way that probit (probability unit, coined by Chester Bliss about 1934), characterizes the cumulative normal function."

#: ../../source/getting-started/beginner/linear-classification.rst:311
msgid "Sigmoid 的反函数是 Logit 函数，即 :math:`\\text{logit}(p) = \\log(\\frac{p}{1-p})`, 有 :math:`\\sigma (\\text{logit}(p))=p`. 根据 Sigmoid 能够将实数映射到概率的特性，很容易发现 Logit 能够将概率映射到实数。"
msgstr "The inverse function of Sigmoid is the Logit function, that is, :math:`\\text{logit}(p) = \\log(\\frac{p}{1-p})`, there are :math:`\\sigma (\\text{logit}(p))=p`. According to Sigmoid, real numbers can be Properties that map to probabilities, it's easy to see that Logit is able to map probabilities to real numbers."

#: ../../source/getting-started/beginner/linear-classification.rst:314
msgid "选择 Sigmoid 的另一原因是它从信息论角度具有良好的数学解释（本教程中不会具体介绍）， 可以根据最大熵（Maximum entroy）原则从伯努利分布和广义线性模型的假设下推导得出。 使用了 Sigmoid / Logistic 函数的模型也被我们称为 Logistic 回归模型（LR 模型）。 很多地方将其翻译为 “逻辑回归”，这就和将 Robustness 翻译成 “鲁棒性” 一样迷惑， 从数学性质上看更适合翻译为对数几率回归。"
msgstr "Another reason to choose Sigmoid is that it has a good mathematical explanation from an information theory perspective (not covered in detail in this tutorial), which can be derived under the assumptions of Bernoulli distribution and generalized linear models according to the principle of Maximum entroy . Models that use Sigmoid / Logistic functions are also called Logistic Regression Models (LR Models). It is translated as \"logistic regression\" in many places, which is as confusing as the translation of Robustness into \"robustness\", which is more suitable for translation as logarithmic probability regression in terms of mathematical properties."

#: ../../source/getting-started/beginner/linear-classification.rst:320
msgid "在本教程中将不对这个 Logistic 术语进行翻译。"
msgstr "This logistic term will not be translated in this tutorial."

#: ../../source/getting-started/beginner/linear-classification.rst:322
msgid "我们发现，对于二分类任务，Sigmoid 函数具有以下的优点："
msgstr "We found that for binary classification tasks, the sigmoid function has the following advantages："

#: ../../source/getting-started/beginner/linear-classification.rst:324
msgid "易于求导：:math:`\\sigma^{\\prime}(x)=\\sigma(x)(1-\\sigma(x))`"
msgstr "Easy to differentiate：:math:`\\sigma^{\\prime}(x)=\\sigma(x)(1-\\sigma(x))`"

#: ../../source/getting-started/beginner/linear-classification.rst:325
msgid "将线性计算部分的输出映射到了 :math:`(0, 1)` 范围内，且可以表示成概率。"
msgstr "The output of the linear calculation part is mapped to the range of :math:`(0, 1)` and can be expressed as a probability."

#: ../../source/getting-started/beginner/linear-classification.rst:327
msgid "我们可使用如下形式表示将样本 :math:`\\mathrm{x}` 的标签预测为某分类的概率（这里假定为 1）："
msgstr "We can express the probability of predicting the label of sample :math:`\\mathrm{x}` to be a certain class (assuming 1 here)："

#: ../../source/getting-started/beginner/linear-classification.rst:329
msgid "p(y=1 \\mid \\boldsymbol{x})=\\sigma(f(\\boldsymbol{x}))=\\frac{1}{1+\\exp (-f(\\boldsymbol{x}))}"
msgstr "p(y=1 \\mid \\boldsymbol{x})=\\sigma(f(\\boldsymbol{x}))=\\frac{1}{1+\\exp (-f(\\boldsymbol{x}))}"

#: ../../source/getting-started/beginner/linear-classification.rst:333
msgid "继续使用我们上面给出的例子：对于线性模型的输出, 经过 Sigmoid 函数的映射后， :math:`\\sigma(-1000)` 比 :math:`\\sigma(-100)` 要更加接近 0，表示前者不是 0 的概率要更高一些。 这就使得不同的样本预测结果之间的区别能够得到有效的体现。 与此同时，对于已经正确分类的样本，Sigmoid 函数不鼓励将线性模型的输出范围拉得过大，收益不明显。"
msgstr "Continue to use the example：we gave above for the output of the linear model, after the mapping of the Sigmoid function, :math:`\\sigma(-1000)` is closer to 0 than :math:`\\sigma(-100)`, indicating that the former is not 0 probability is higher. This enables the difference between the prediction results of different samples to be effectively reflected. At the same time, for samples that have been correctly classified, the sigmoid function discourages stretching the output range of the linear model too large, and the benefit is not obvious."

#: ../../source/getting-started/beginner/linear-classification.rst:340
msgid "二分类 Logistic 回归最终的输出只有一个值，表示预测为某基类的概率。 接下来我们将介绍多分类 Logistic 回归（Multinational Logistic Regression）。"
msgstr "The final output of binary logistic regression has only one value, which represents the probability of predicting a base class. Next we will introduce Multiclass Logistic Regression (Multinational Logistic Regression)."

#: ../../source/getting-started/beginner/linear-classification.rst:344
msgid "多分类任务"
msgstr "Multi-classification tasks"

#: ../../source/getting-started/beginner/linear-classification.rst:346
msgid "将分类模型的输出转换为对目标类别的概率表示，是非常常见（但不唯一）的处理思路。 Logistic 回归中最终的输出只有一个概率 :math:`p`, 代表预测为某分类的概率； 另一个分类上的概率可以直接使用 :math:`1-p` 进行表示。 推广到多分类任务，我们如何将线性模型的输出转换为在多个类别上的概率预测？"
msgstr "Converting the output of a classification model to a probabilistic representation of the target class is a very common (but not unique) approach. The final output in logistic regression has only one probability :math:`p`, which represents the probability of predicting a certain category; the probability of another category can be directly represented by :math:`1-p`. Generalizing to multi-classification tasks, how do we transform the output of a linear model into probabilistic predictions over multiple classes?"

#: ../../source/getting-started/beginner/linear-classification.rst:351
msgid "对标记进行 One-hot 编码"
msgstr "One-hot encoding of markers"

#: ../../source/getting-started/beginner/linear-classification.rst:353
msgid "细心的读者可能已经注意到了这一点，MNIST 数据集中的标记均为标量。 因此需要做一些额外的处理，使其变成多分类上的概率向量，常见的做法是使用 One-hot 编码。 例如某两张手写数字样本的对应标记为 3 和 5, 使用 One-hot 编码 （对应接口为 :func:`.functional.nn.one_hot` ）将得到如下表示："
msgstr "Astute readers may have noticed this, the tokens in the MNIST dataset are all scalars. Therefore, some additional processing is required to turn it into a probability vector on multi-classification. The common practice is to use One-hot encoding. For example, the corresponding marks of two handwritten digit samples are 3 and 5, using One-hot encoding (the corresponding interface is :func:`.functional.nn.one_hot` ) will get the following representation："

#: ../../source/getting-started/beginner/linear-classification.rst:370
msgid "经过这样的处理，每个样本的标记就变成了一个 10 分类的概率向量，可与模型预测输出进行比较。"
msgstr "After this processing, the label of each sample becomes a 10-class probability vector that can be compared with the model prediction output."

#: ../../source/getting-started/beginner/linear-classification.rst:372
msgid "回忆一下线性回归的形式，对于单个样本 :math:`\\boldsymbol x \\in \\mathbb{R}^d`, 借助于权重向量 :math:`\\boldsymbol{w} \\in \\mathbb{R}^d` 和偏置 :math:`b`, 可以得到一个线性输出 :math:`\\boldsymbol {w} \\cdot \\boldsymbol {x} + b`, 现在我们可以将这个输出看作是预测当前样本是某个分类的得分 :math:`s`, 或者说预测为该分类的可信度。 对于 :math:`c` 分类任务，我们希望得到 :math:`c` 个这样的得分，因此可以借助矩阵运算的形式实现："
msgstr "Recalling the form of linear regression, for a single sample :math:`\\boldsymbol x \\in \\mathbb{R}^d`, with the help of the weight vector :math:`\\boldsymbol{w} \\in \\mathbb{R}^d` and the bias :math:`b`, you can Get a linear output :math:`\\boldsymbol {w} \\cdot \\boldsymbol {x} + b`, now we can think of this output as the score :math:`s` predicting that the current sample is a certain category, or predicting the credibility of the category Spend. For :math:`c` classification tasks, we want to get :math:`c` such scores, so we can achieve：with the help of matrix operations"

#: ../../source/getting-started/beginner/linear-classification.rst:378
msgid "\\left[\\begin{array}{c}\n"
"-\\boldsymbol{w}_{1}^{T}- \\\\\n"
"-\\boldsymbol{w}_{2}^{T}- \\\\\n"
"\\vdots \\\\\n"
"-\\boldsymbol{w}_{c}^{T}-\n"
"\\end{array}\\right]\n"
"\\left[\\begin{array}{c}\n"
"x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{d}\n"
"\\end{array}\\right] +\n"
"\\left[\\begin{array}{c}\n"
"b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{c}\n"
"\\end{array}\\right] =\n"
"\\left[\\begin{array}{c}\n"
"\\boldsymbol{w}_{1}^{T} \\boldsymbol{x} + b_1 \\\\\n"
"\\boldsymbol{w}_{2}^{T} \\boldsymbol{x} + b_2 \\\\\n"
"\\vdots \\\\\n"
"\\boldsymbol{w}_{c}^{T} \\boldsymbol{x} + b_c\n"
"\\end{array}\\right] =\n"
"\\left[\\begin{array}{c}\n"
"s_{1} \\\\ s_{2} \\\\ \\vdots \\\\ s_{c}\n"
"\\end{array}\\right]"
msgstr "\\left[\\begin{array}{c}\n"
"-\\boldsymbol{w}_{1}^{T}- \\\\\n"
"-\\boldsymbol{w}_{2}^{T}- \\\\\n"
"\\vdots \\\\\n"
"-\\boldsymbol{w}_{c}^{T}-\n"
"\\end{array}\\right]\n"
"\\left[\\begin{array}{c}\n"
"x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{d}\n"
"\\end{array}\\right] +\n"
"\\left[\\begin{array}{c}\n"
"b_{1} \\\\ b_{2} \\ \\ \\vdots \\\\ b_{c}\n"
"\\end{array}\\right] =\n"
"\\left[\\begin{array}{c}\n"
"\\boldsymbol{w}_{1}^{T} \\boldsymbol{x} + b_1 \\\\\n"
"\\boldsymbol{w}_{2}^{T} \\boldsymbol{x} + b_2 \\\\\n"
"\\vdots \\\\\n"
"\\boldsymbol{w}_{c}^{T} \\boldsymbol{x} + b_c\n"
"\\end{array}\\right] =\n"
"\\left[\\begin{array}{c}\n"
"s_{1} \\\\ s_{2} \\\\ \\vdots \\\\ s_{c}\n"
"\\end{array}\\right]"

#: ../../source/getting-started/beginner/linear-classification.rst:402
msgid "注意：为了方便理解，这里写成了线性代数中矩阵运算的形式，实际计算时未必一定要将向量提升为矩阵。"
msgstr "Note that：is written here in the form of matrix operations in linear algebra for ease of understanding. In actual calculations, it is not necessary to upgrade vectors to matrices."

#: ../../source/getting-started/beginner/linear-classification.rst:404
msgid "我们简单验证一下输出的得分是否是一个 10 维向量（利用上一个教程中 矩阵@向量 运算）："
msgstr "Let's simply verify that the output score is a 10-dimensional vector (using the matrix@vector operation in the previous tutorial)："

#: ../../source/getting-started/beginner/linear-classification.rst:419
msgid "我们可以直观理解成，一个 :math:`d` 维特征向量，经过一个权重矩阵变换加偏置后能够得到一个 :math:`c` 维得分向量。"
msgstr "We can intuitively understand that a :math:`d` dimensional feature vector can get a :math:`c` dimensional score vector after a weight matrix transformation and bias."

#: ../../source/getting-started/beginner/linear-classification.rst:421
msgid "Sigmoid 函数能够将单个输出从实数域 :math:`\\mathbb R` 映射到概率区间 :math:`(0,1)` ， 对于得分向量。如果每个类得分都应用 Sigmoid 函数，的确会得到多个概率值，但问题在于这样得到的概率值之和不为 1. 需要寻找其它做法。"
msgstr "The sigmoid function is able to map a single output from the real number domain :math:`\\mathbb R` to the probability interval :math:`(0,1)` , for a score vector. If you apply the sigmoid function for each class score, you do get multiple probability values, but the problem is that the probability values obtained in this way do not sum to 1. Need to find other ways."

#: ../../source/getting-started/beginner/linear-classification.rst:425
msgid "最容易想到的做法是学习 One-hot 表示，将最大值处 （对应接口 :func:`.functional.argmax` ） 的概率设置成 1, 其它位置统统设置成 0. 如 :math:`(10, -10, 30, 20)` 变成 :math:`(0, 0, 1, 0)`. :math:`\\operatorname{Argmax}` 这种做法属于硬分类，和之前在二分类任务中看到的示性函数 :math:`\\mathbb I(\\cdot )` 存在着同样的问题：数学性质比较差；无法体现类内、类间样本的区别... 等等。"
msgstr "The easiest way to think of it is to learn the One-hot representation, set the probability at the maximum value (corresponding to interface :func:`.functional.argmax`) to 1, and all other positions to 0. For example, :math:`(10, -10, 30 , 20)` becomes :math:`(0, 0, 1, 0)`. :math:`\\operatorname{Argmax}` This approach belongs to hard classification, and the indicative function :math:seen earlier in the binary classification task `\\mathbb I (\\cdot )` has the same problem：that the mathematical properties are relatively poor; the difference between samples within a class, between classes cannot be reflected... and so on."

#: ../../source/getting-started/beginner/linear-classification.rst:430
msgid "在多分类任务中，比较常用的是 :math:`\\operatorname{Softmax}` 函数（对应接口 :func:`.functional.nn.softmax` ）："
msgstr "In multi-classification tasks, the more commonly used function is :math:`\\operatorname{Softmax}` (corresponding to interface :func:`.functional.nn.softmax` )："

#: ../../source/getting-started/beginner/linear-classification.rst:432
msgid "p(y=c \\mid \\boldsymbol{x})\n"
"=\\operatorname{Softmax}\\left(s_{c}\\right)\n"
"=\\frac{\\exp s_{c}}{\\sum_{i} \\exp s_{i}}"
msgstr "p(y=c \\mid \\boldsymbol{x})\n"
"=\\operatorname{Softmax}\\left(s_{c}\\right)\n"
"=\\frac{\\exp s_{c}}{\\sum_{i} \\exp s_{i}}"

#: ../../source/getting-started/beginner/linear-classification.rst:438
msgid "可以理解成我们对多个类上的得分经过指数归一化后得到了目标分类概率值，我们通常称为 Logits;"
msgstr "It can be understood that we get the target classification probability value after exponential normalization of the scores on multiple classes, which we usually call Logits;"

#: ../../source/getting-started/beginner/linear-classification.rst:439
msgid "这种指数形式的归一化具有一些优点：相较均值归一化，具有“马太效应”，我们认为较大的原始值在归一化后得到的概率值也应该更大； 顾名思义，相较于硬分类 :math:`\\operatorname{Argmax}` 要软（Soft）一些，可以体现类内、类间样本预测值的区别； 有利于找到 Top-k 个分类候选项，即前 :math:`k` 大概率的分类，有利于用作评估模型性能。"
msgstr "This exponential form of normalization has some advantages：compared to the mean normalization, it has a \"Matthew effect\", we think that the larger original value should also have a larger probability value after normalization; Compared with hard classification :math:`\\operatorname{Argmax}`, it is softer, which can reflect the difference between the predicted values of samples within and between classes; it is beneficial to find the Top-k classification candidates, that is, the first :math:`k` have high probability Classification, useful for evaluating model performance."

#: ../../source/getting-started/beginner/linear-classification.rst:447
msgid "到这里，我们已经能够让我们的模型基于输入样本，输出在目标类别上进行预测的概率向量了。"
msgstr "At this point, we've been able to get our model to output a probability vector of predictions on the target class based on the input samples."

#: ../../source/getting-started/beginner/linear-classification.rst:450
msgid "分类任务的优化目标"
msgstr "Optimization Objectives for Classification Tasks"

#: ../../source/getting-started/beginner/linear-classification.rst:452
msgid "我们已经得到了模型在多分类上预测的概率向量 :math:`\\hat{\\boldsymbol{y}}`, 也对真实标记通过使用 One-hot 编码得到了概率向量 :math:`\\boldsymbol{y}`. 二者各表示一种概率分布。我们的优化目标是，让预测值和真实标记尽可能地接近，需设计合适的损失函数。"
msgstr "We have obtained the probability vector :math:`\\hat{\\boldsymbol{y}}` predicted by the model on multi-classification, and also obtained the probability vector :math:`\\boldsymbol{y}` by using One-hot encoding for the real mark. a probability distribution. Our optimization goal is to make the predicted values as close as possible to the ground truth, and we need to design a suitable loss function."

#: ../../source/getting-started/beginner/linear-classification.rst:456
msgid "信息论中的相对熵与交叉熵"
msgstr "Relative Entropy and Cross Entropy in Information Theory"

#: ../../source/getting-started/beginner/linear-classification.rst:458
msgid "在信息论中，如果对同一个随机变量 :math:`x` 有两个单独的概率分布 :math:`p(x)` 和 :math:`q(x)`, 可以用相对熵（KL 散度）来表示两个分布之间的差异："
msgstr "In information theory, if there are two separate probability distributions :math:`p(x)` and :math:`q(x)` for the same random variable :math:`x`, the relative entropy (KL divergence) can be used to represent the two Difference Between Distributions："

#: ../../source/getting-started/beginner/linear-classification.rst:461
msgid "\\begin{aligned}\n"
"\\mathrm{KL}(p \\| q) &=-\\int p(x) \\ln q(x) d x-\\left(-\\int p(x) \\ln p(x) d x\\right) \\\\\n"
"&=H(p, q)-H(p)\n"
"\\end{aligned}"
msgstr "\\begin{aligned}\n"
"\\mathrm{KL}(p \\| q) &=-\\int p(x) \\ln q(x) d x-\\left(-\\int p(x) \\ln p(x) dx\\right ) \\\\\n"
"&=H(p, q)-H(p)\n"
"\\end{aligned}"

#: ../../source/getting-started/beginner/linear-classification.rst:468
msgid "相对熵的特点是，两个概率分布完全相同时，其值为零。二者分布之间的差异越大，相对熵值越大。"
msgstr "The characteristic of relative entropy is that when two probability distributions are exactly the same, its value is zero. The greater the difference between the two distributions, the greater the relative entropy value."

#: ../../source/getting-started/beginner/linear-classification.rst:470
msgid "这与我们希望 “设计一个损失函数，可用来评估当前预测得到的概率分布 :math:`q(x)` 与真实标记的概率分布 :math:`p(x)` 之间差异” 的目的不谋而合。 且由于训练过程中样本标记不会变化，其概率分布 :math:`p(x)` 是确定的常数， 则上式中的 :math:`H(p)` 值不会随着训练样本的变化而改变，也不会影响梯度计算，所以可省略。"
msgstr "This coincides with our goal of \"designing a loss function that can be used to evaluate the difference between the probability distribution :math:`q(x)` of the current prediction and the probability distribution :math:`p(x)` of the true labels\". And since the sample label will not change during the training process, its probability distribution :math:`p(x)` is a definite constant, then the :math:`H(p)` value in the above formula will not change with the change of the training sample, It will not affect the gradient calculation, so it can be omitted."

#: ../../source/getting-started/beginner/linear-classification.rst:475
msgid "剩下的 :math:`H(p, q)` 部分则被定义为我们常用的交叉熵（Cross Entropy, CE）， 用我们现在的例子中的离散概率值来表示则为："
msgstr "The remaining :math:`H(p, q)` part is defined as our commonly used cross entropy (Cross Entropy, CE), which is：in terms of discrete probability values in our current example"

#: ../../source/getting-started/beginner/linear-classification.rst:478
msgid "\\ell_{\\mathrm{CE}}=H(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})=-\\sum_{i=1}^{c} y_{i} \\ln \\hat{y}_{i}"
msgstr "\\ell_{\\mathrm{CE}}=H(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})=-\\sum_{i=1}^{c} y_{i} \\ln \\hat{y}_{i}"

#: ../../source/getting-started/beginner/linear-classification.rst:482
msgid "这即是分类任务经常使用的损失函数，对应 MegEngine 中的 :func:`.functional.nn.cross_entropy` 接口。"
msgstr "This is the loss function often used in classification tasks, corresponding to the :func:`.functional.nn.cross_entropy` interface in MegEngine."

#: ../../source/getting-started/beginner/linear-classification.rst:486
msgid "MegEngine 中的 :func:`~.cross_entropy` 接口是考虑到实际使用情景而设计的："
msgstr "The :func:`~.cross_entropy` interface in MegEngine is designed with the actual usage scenario in："

#: ../../source/getting-started/beginner/linear-classification.rst:488
msgid "默认会对标记值进行 One-hot 编码，不需要用户手动地提前处理好；"
msgstr "One-hot encoding is performed on the tag value by default, which does not require the user to manually process it in advance;"

#: ../../source/getting-started/beginner/linear-classification.rst:489
msgid "默认会先进行 Softmax 计算转换成类别概率值（即 ``with_logits=True`` ）。"
msgstr "By default, the Softmax calculation is first converted into a class probability value (ie ``with_logits=True``)."

#: ../../source/getting-started/beginner/linear-classification.rst:491
msgid "因此在写模型代码时，会发现不需要用到 :func:`~.one_hot` 和 :func:`~.softmax` 接口。"
msgstr "Therefore, when writing model code, you will find that :func:`~.one_hot` and :func:`~.softmax` interfaces do not need to be used."

#: ../../source/getting-started/beginner/linear-classification.rst:493
msgid "作为演示，下面展示三种预测值与真实值计算得到交叉熵损失的情况："
msgstr "As a demonstration, the following shows three cases where the predicted value and the actual value are calculated to obtain the cross：loss."

#: ../../source/getting-started/beginner/linear-classification.rst:516
msgid "分类任务的评估指标"
msgstr "Evaluation Metrics for Classification Tasks"

#: ../../source/getting-started/beginner/linear-classification.rst:518
msgid "在回归问题中常用的评估指标有 MAE, MSE 等等，但它们并不适用于分类任务。"
msgstr "Commonly used evaluation metrics in regression problems are MAE, MSE, etc., but they are not suitable for classification tasks."

#: ../../source/getting-started/beginner/linear-classification.rst:520
msgid "在本教程中，我们将引入准确率（Accuracy）作为手写数字图像分类的评估指标。 其定义如下：被分类正确的样本数占所有样本数的比例。 例如 10000 个样本中，如果有 6000 个样本被正确分类， 则我们说当前分类器的准确率为 0.6, 换算成百分比也即 60%."
msgstr "In this tutorial, we will introduce Accuracy as an evaluation metric for handwritten digit image classification. Its definition is as follows:：is the proportion of correctly classified samples to all samples. For example, out of 10,000 samples, if 6,000 samples are correctly classified, then we say that the accuracy of the current classifier is 0.6, which is 60% when converted into a percentage."

#: ../../source/getting-started/beginner/linear-classification.rst:525
msgid "对于模型预测得到的多分类上的多个输出，我们通常用 :func:`~.argmax` 接口将最大的位置处所对应的标记作为预测分类，与真实的标记进行比较 （如果是批样本的向量化实现，通常需要指定 ``axis`` 参数）："
msgstr "For the multiple outputs on the multi-class predicted by the model, we usually use the :func:`~.argmax` interface to use the 0 `~.argmax` interface to take the label corresponding to the largest position as the predicted classification, and compare it with the real label (if it is a vectorized implementation of batch samples , usually need to specify the ``axis`` parameter)："

#: ../../source/getting-started/beginner/linear-classification.rst:538
msgid "对于在多个样本上得到的输出，我们可以借助 ``==`` 运算进行判断，以 5 个样本上预测结果为例："
msgstr "For the output obtained on multiple samples, we can use the ``==`` operation to judge, taking the prediction results on 5 samples as an example："

#: ../../source/getting-started/beginner/linear-classification.rst:545
msgid "这时借助 :func:`~.functional.sum` 接口，就能得到正确分类的样本数："
msgstr "At this time, with the help of :func:`~.functional.sum` interface, you can get the number of correctly classified samples："

#: ../../source/getting-started/beginner/linear-classification.rst:550
msgid "在我们训练线性分类器的过程中，每经过一轮训练， 都可以检测一下当前分类器在训练样本和测试样本上的分类准确率，作为模型性能的评估手段。 如果经过多轮训练，训练集上的准确率不断上升，而测试集上的准确率开始下降，则表明可能过拟合了。"
msgstr "In the process of training the linear classifier, after each round of training, the classification accuracy of the current classifier on the training samples and test samples can be detected as a means of evaluating the model performance. If after many rounds of training, the accuracy on the training set keeps rising and the accuracy on the test set starts to drop, it may be an indication of overfitting."

#: ../../source/getting-started/beginner/linear-classification.rst:556
msgid "准确率不是分类任务唯一的评估指标，我们会在后面的教程中接触到更多评估指标。"
msgstr "Accuracy is not the only evaluation metric for classification tasks, and we will touch on more evaluation metrics in later tutorials."

#: ../../source/getting-started/beginner/linear-classification.rst:557
msgid "MegEngine 中还提供了 :func:`~.topk_accuracy` 来计算 Top-k 准确率， 而本教程中为了演示计算过程（对应 ``k=1`` 的情况），没有直接使用这个接口。"
msgstr "MegEngine also provides :func:`~.topk_accuracy` to calculate Top-k accuracy, but in this tutorial, in order to demonstrate the calculation process (corresponding to the case of ``k=1``), this interface is not used directly."

#: ../../source/getting-started/beginner/linear-classification.rst:561
msgid "练习：线性分类器"
msgstr "Exercise：Linear Classifier"

#: ../../source/getting-started/beginner/linear-classification.rst:563
msgid "结合前面提到的所有概念，现在我们使用 MegEngine 对线性分类器进行完整的实现："
msgstr "Combining all the concepts mentioned above, now we use：for a complete implementation of the linear classifier."

#: ../../source/getting-started/beginner/linear-classification.rst:565
msgid "可对照着上一个教程的流程进行相应环节代码的改动，体会一下整个逻辑；"
msgstr "You can change the code of the corresponding link according to the process of the previous tutorial, and experience the whole logic;"

#: ../../source/getting-started/beginner/linear-classification.rst:566
msgid "再次注意，在本教程中将 MNIST 提供的测试集当做验证集进行使用（虽然名为 ``test`` ）；"
msgstr "Note again that in this tutorial the test set provided by MNIST is used as the validation set (albeit named ``test``);"

#: ../../source/getting-started/beginner/linear-classification.rst:567
msgid "对批块的数据，记得使用向量化实现来代替在单个样本上的 for 循环实现。"
msgstr "For batches of data, remember to use a vectorized implementation instead of a for loop implementation on a single sample."

#: ../../source/getting-started/beginner/linear-classification.rst:571
msgid "经过 5 轮训练，通常会得到一个正确率在 92% 左右的线性分类器。"
msgstr "After 5 rounds of training, a linear classifier with an accuracy rate of about 92% is usually obtained."

#: ../../source/getting-started/beginner/linear-classification.rst:575
msgid "本教程的对应源码： :docs:`examples/beginner/linear-classification.py`"
msgstr "The corresponding source code for this tutorial： :docs:`examples/beginner/linear-classification.py`"

#: ../../source/getting-started/beginner/linear-classification.rst:578
msgid "总结：数据的流动"
msgstr "Summarize the flow of：data"

#: ../../source/getting-started/beginner/linear-classification.rst:580
msgid "到目前为止，我们用线性模型完成了直线拟合、房价预测和手写数字识别任务，是时候进行一次对比了。"
msgstr "So far, we've done line fitting, house price prediction, and handwritten digit recognition tasks with linear models, so it's time for a comparison."

#: ../../source/getting-started/beginner/linear-classification.rst:585
msgid "任务名"
msgstr "task name"

#: ../../source/getting-started/beginner/linear-classification.rst:586
msgid "前向计算（单样本）"
msgstr "Forward computation (single sample)"

#: ../../source/getting-started/beginner/linear-classification.rst:587
msgid "损失函数"
msgstr "loss function"

#: ../../source/getting-started/beginner/linear-classification.rst:588
msgid "评估指标"
msgstr "Evaluation Metrics"

#: ../../source/getting-started/beginner/linear-classification.rst:590
msgid "拟合直线"
msgstr "Fit straight line"

#: ../../source/getting-started/beginner/linear-classification.rst:591
msgid ":math:`x \\in \\mathbb{R} \\stackrel{w,b}{\\mapsto} y \\in \\mathbb{R}`"
msgstr ":math:`x \\in \\mathbb{R} \\stackrel{w,b}{\\mapsto} y \\in \\mathbb{R}`"

#: ../../source/getting-started/beginner/linear-classification.rst:592
#: ../../source/getting-started/beginner/linear-classification.rst:597
msgid "MSE"
msgstr "MSE"

#: ../../source/getting-started/beginner/linear-classification.rst:593
msgid "观察拟合度"
msgstr "observed fit"

#: ../../source/getting-started/beginner/linear-classification.rst:595
msgid "房价预测"
msgstr "house price forecast"

#: ../../source/getting-started/beginner/linear-classification.rst:596
msgid ":math:`\\boldsymbol{x} \\in \\mathbb{R}^{d} \\stackrel{\\boldsymbol{w},b}{\\mapsto} y \\in \\mathbb{R}`"
msgstr ":math:`\\boldsymbol{x} \\in \\mathbb{R}^{d} \\stackrel{\\boldsymbol{w},b}{\\mapsto} y \\in \\mathbb{R}`"

#: ../../source/getting-started/beginner/linear-classification.rst:598
msgid "MAE"
msgstr "MAE"

#: ../../source/getting-started/beginner/linear-classification.rst:600
msgid "手写数字识别"
msgstr "Handwritten Digit Recognition"

#: ../../source/getting-started/beginner/linear-classification.rst:601
msgid ":math:`\\boldsymbol{x} \\in \\mathbb{R}^{d} \\stackrel{W,\\boldsymbol{b}}{\\mapsto} \\boldsymbol{y} \\in \\mathbb{R}^{c}`"
msgstr ":math:`\\boldsymbol{x} \\in \\mathbb{R}^{d} \\stackrel{W,\\boldsymbol{b}}{\\mapsto} \\boldsymbol{y} \\in \\mathbb{R}^{c}`"

#: ../../source/getting-started/beginner/linear-classification.rst:602
msgid "CE"
msgstr "CE"

#: ../../source/getting-started/beginner/linear-classification.rst:603
msgid "Accuracy"
msgstr "Accuracy"

#: ../../source/getting-started/beginner/linear-classification.rst:606
msgid "注：默认图片信息已经展平为向量， 即 :math:`\\mathsf{I} \\in \\mathbb{R}^{H \\times W \\times C} \\stackrel{\\operatorname{flatten}}{\\mapsto} \\boldsymbol{x} \\in \\mathbb{R}^{d}`"
msgstr "Note：The default image information has been flattened into a vector, ie :math:`\\mathsf{I} \\in \\mathbb{R}^{H \\times W \\times C} \\stackrel{\\operatorname{flatten}}{\\mapsto} \\boldsymbol{x} \\in \\mathbb{R}^{d}"

#: ../../source/getting-started/beginner/linear-classification.rst:610
msgid "在用 MegEngine 进行实现时，我们需要关注单个样本数据在整个计算图中的流动， 尤其需要关注 Tensor 形状的变化（本质上这些都是线性代数中常见的变换）， 这其实就是我们的机器学习模型进行设计的过程，除了线性模型本身， 整个机器学习流程还需要设计合适的损失函数，选定科学的评估指标。"
msgstr "When implementing with MegEngine, we need to pay attention to the flow of single sample data in the entire calculation graph, especially the change of Tensor shape (essentially these are common transformations in linear algebra), which is actually our machine learning model In the design process, in addition to the linear model itself, the entire machine learning process also needs to design an appropriate loss function and select scientific evaluation indicators."

#: ../../source/getting-started/beginner/linear-classification.rst:615
msgid "在其它方面，几个任务的做法都是一致的，比如对于批大小的数据使用向量化实现来加速， 采取的都是梯度下降算法来对模型中的参数进行迭代优化，都需要及时评估模型性能... 希望你对这些步骤已经有了更加深刻的了解。"
msgstr "In other respects, the practices of several tasks are the same. For example, for batch-sized data, the vectorized implementation is used to speed up, and the gradient descent algorithm is used to iteratively optimize the parameters in the model, and it is necessary to evaluate the model performance in time. ...hopefully you already have a better understanding of these steps."

#: ../../source/getting-started/beginner/linear-classification.rst:619
msgid "看来线性模型就能搞定一切？"
msgstr "It seems that linear models can do everything?"

#: ../../source/getting-started/beginner/linear-classification.rst:621
msgid "答案是否定的。由于这几个教程中数据集都过于简单， 因此即使使用最简单的线性模型来训练，最终也能取得比较不错的效果。 在下一个教程中，我们将选用更加复杂的数据集（相较于 MNIST 只复杂了一些些）， 同样是分类任务，看线性模型是否依旧那么强大。"
msgstr "the answer is negative. Since the datasets in these tutorials are too simple, even using the simplest linear model for training can achieve relatively good results in the end. In the next tutorial, we will use a more complex dataset (compared to MNIST only slightly more complex), the same classification task, to see if the linear model is still as powerful."

#: ../../source/getting-started/beginner/linear-classification.rst:627
msgid "拓展材料"
msgstr "Expansion material"

#: ../../source/getting-started/beginner/linear-classification.rst:631
msgid "请尝试去了解一下 K 近邻（K nearest neighbor, KNN）算法与支撑向量机（Support vector machine, SVM）算法， 并尝试用 MegEngine 进行具体实现，一个比较好的参考材料是 CS231n 课程。 比如用 SVM 完成本教程中的手写数字分类任务，你会发现模型的预测输出不一定需要表示为概率； 用 KNN 完成手写数字分类，你更会惊讶地发现它不需要创建模型，也不需要进行训练！ 那么代价是什么呢？进阶地，你可以对比不同的机器学习算法，看看彼此之间的优劣和适用情景。"
msgstr "Please try to understand the K nearest neighbor (KNN) algorithm and the Support vector machine (SVM) algorithm, and try to use MegEngine for specific implementation. A good reference material is the CS231n course. For example, using SVM to complete the handwritten digit classification task in this tutorial, you will find that the predicted output of the model does not necessarily need to be expressed as a probability; using KNN to complete the handwritten digit classification, you will be surprised to find that it does not need to create a model, and does not need to Training! So what's the price? More advanced, you can compare different machine learning algorithms to see how well they work against each other and where they work."

#: ../../source/getting-started/beginner/linear-classification.rst:640
msgid "对于二分类 :math:`y \\in \\{0, 1\\}` 的情况，使用 Softmax 将退化成 Sigmoid. 请尝试证明。"
msgstr "For the case of binary classification :math:`y \\in \\{0, 1\\}`, using Softmax will degenerate to Sigmoid. Please try to prove."

#: ../../source/getting-started/beginner/linear-classification.rst:643
msgid "参考文献"
msgstr "references"

#: ../../source/getting-started/beginner/linear-classification.rst:648
msgid "Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2010."
msgstr ""

#~ msgid ""
#~ "Yann LeCun, Corinna Cortes, and CJ "
#~ "Burges. Mnist handwritten digit database. "
#~ "ATT Labs [Online]. Available: "
#~ "http://yann.lecun.com/exdb/mnist, 2010."
#~ msgstr ""
#~ "Yann LeCun, Corinna Cortes, and CJ "
#~ "Burges. Mnist handwritten digit database. "
#~ "ATT Labs [Online]. Available: "
#~ "http://yann.lecun.com/exdb/mnist, 2010."

