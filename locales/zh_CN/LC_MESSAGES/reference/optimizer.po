
msgid ""
msgstr ""
"Project-Id-Version:  megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-05-12 09:02+0800\n"
"PO-Revision-Date: 2021-04-29 10:33+0000\n"
"Last-Translator: \n"
"Language: zh_Hans_CN\n"
"Language-Team: Chinese Simplified\n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/reference/optimizer.rst:6
msgid "优化器（Optimizer）"
msgstr "优化器（Optimizer）"

#: ../../source/reference/optimizer.rst:13:<autosummary>:1
msgid ":obj:`Optimizer <megengine.optimizer.Optimizer>`"
msgstr ":obj:`Optimizer <megengine.optimizer.Optimizer>`"

#: ../../source/reference/optimizer.rst:13:<autosummary>:1
msgid "Base class for all optimizers."
msgstr "所有优化器的基类。"

#: ../../source/reference/optimizer.rst:15
msgid "常见优化器"
msgstr "常见优化器"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
msgid ":obj:`SGD <megengine.optimizer.SGD>`"
msgstr ":obj:`SGD <megengine.optimizer.SGD>`"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
msgid "Implements stochastic gradient descent."
msgstr "实现随机梯度下降。"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
#, fuzzy
msgid ":obj:`AdamW <megengine.optimizer.AdamW>`"
msgstr ":obj:`Adam <megengine.optimizer.Adam>`"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
#, fuzzy
msgid ""
"Implements AdamW algorithm proposed in `\"Decoupled Weight Decay "
"Regularization\" <https://arxiv.org/abs/1711.05101>`_."
msgstr ""
"实现 `\"Adam: A Method for Stochastic Optimization\" "
"<https://arxiv.org/abs/1412.6980>`_  中提出的Adam算法。"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
msgid ":obj:`Adam <megengine.optimizer.Adam>`"
msgstr ":obj:`Adam <megengine.optimizer.Adam>`"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
msgid ""
"Implements Adam algorithm proposed in `\"Adam: A Method for Stochastic "
"Optimization\" <https://arxiv.org/abs/1412.6980>`_."
msgstr ""
"实现 `\"Adam: A Method for Stochastic Optimization\" "
"<https://arxiv.org/abs/1412.6980>`_  中提出的Adam算法。"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
msgid ":obj:`Adagrad <megengine.optimizer.Adagrad>`"
msgstr ":obj:`Adagrad <megengine.optimizer.Adagrad>`"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
msgid "Implements Adagrad algorithm."
msgstr "实现Adagrad算法。"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
msgid ":obj:`Adadelta <megengine.optimizer.Adadelta>`"
msgstr ":obj:`Adadelta <megengine.optimizer.Adadelta>`"

#: ../../source/reference/optimizer.rst:26:<autosummary>:1
msgid "Implements Adadelta algorithm."
msgstr "实现Adadelta算法。"

#: ../../source/reference/optimizer.rst:28
msgid "学习率调整"
msgstr "学习率调整"

#: ../../source/reference/optimizer.rst:36:<autosummary>:1
msgid ":obj:`LRScheduler <megengine.optimizer.LRScheduler>`"
msgstr ":obj:`LRScheduler <megengine.optimizer.LRScheduler>`"

#: ../../source/reference/optimizer.rst:36:<autosummary>:1
msgid "Base class for all learning rate based schedulers."
msgstr "所有学习率调度器的基类。"

#: ../../source/reference/optimizer.rst:36:<autosummary>:1
msgid ":obj:`MultiStepLR <megengine.optimizer.MultiStepLR>`"
msgstr ":obj:`MultiStepLR <megengine.optimizer.MultiStepLR>`"

#: ../../source/reference/optimizer.rst:36:<autosummary>:1
msgid "Decays the learning rate of each parameter group by gamma once the"
msgstr "以gamma为倍率阶梯式衰减各参数组的学习率"

#~ msgid ":obj:`Optimizer.step <megengine.optimizer.Optimizer.step>`"
#~ msgstr ":obj:`Optimizer.step <megengine.optimizer.Optimizer.step>`"

#~ msgid "Performs a single optimization step."
#~ msgstr "执行单一优化步骤。"

#~ msgid ":obj:`Optimizer.clear_grad <megengine.optimizer.Optimizer.clear_grad>`"
#~ msgstr ":obj:`Optimizer.clear_grad <megengine.optimizer.Optimizer.clear_grad>`"

#~ msgid "Set the grad attribute to None for all parameters."
#~ msgstr "把所有参数的梯度属性设置为 None。"

#~ msgid ""
#~ ":obj:`Optimizer.add_param_group "
#~ "<megengine.optimizer.Optimizer.add_param_group>`"
#~ msgstr ""
#~ ":obj:`Optimizer.add_param_group "
#~ "<megengine.optimizer.Optimizer.add_param_group>`"

#~ msgid ""
#~ "Add a param group to ``param_groups``"
#~ " of the :class:`~megengine.optim.optimizer.Optimizer`."
#~ msgstr ""
#~ "向 :class:`~megengine.optim.optimizer.Optimizer` 的 "
#~ "``param_groups`` 中添加一组参数。"

#~ msgid ":obj:`Optimizer.state_dict <megengine.optimizer.Optimizer.state_dict>`"
#~ msgstr ":obj:`Optimizer.state_dict <megengine.optimizer.Optimizer.state_dict>`"

#~ msgid "Export the optimizer state."
#~ msgstr "导出优化器状态。"

#~ msgid ""
#~ ":obj:`Optimizer.load_state_dict "
#~ "<megengine.optimizer.Optimizer.load_state_dict>`"
#~ msgstr ""
#~ ":obj:`Optimizer.load_state_dict "
#~ "<megengine.optimizer.Optimizer.load_state_dict>`"

#~ msgid "Loads the optimizer state."
#~ msgstr "加载优化器状态。"

