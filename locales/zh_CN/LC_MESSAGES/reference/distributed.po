msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-09-21 10:50+0000\n"
"PO-Revision-Date: 2023-09-27 08:40\n"
"Last-Translator: \n"
"Language: zh_CN\n"
"Language-Team: Chinese Simplified\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-CN\n"
"X-Crowdin-File: /dev/locales/zh_CN/LC_MESSAGES/reference/distributed.po\n"
"X-Crowdin-File-ID: 9785\n"

#: ../../source/reference/distributed.rst:6
msgid "megengine.distributed"
msgstr "megengine.distributed"

#: ../../source/reference/distributed.rst:16:<autosummary>:1
msgid ":py:obj:`backend <megengine.distributed.backend>`"
msgstr ":py:obj:`backend <megengine.distributed.backend>`"

#: ../../source/reference/distributed.rst:16:<autosummary>:1
msgid "Get or set backend of collective communication."
msgstr "获取或设置集合通信后端。"

#: ../../source/reference/distributed.rst:18
msgid "分组（Group）"
msgstr "分组（Group）"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`Server <megengine.distributed.Server>`"
msgstr ":py:obj:`Server <megengine.distributed.Server>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Distributed Server for distributed training."
msgstr "用于分布式训练的分布式服务器。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`Group <megengine.distributed.Group>`"
msgstr ":py:obj:`Group <megengine.distributed.Group>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Include ranked nodes running collective communication (See :mod:`~.functional.distributed`)."
msgstr "包含运行集群通信的排名节点 (看 :mod:`~.functional.distributed`)."

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`init_process_group <megengine.distributed.init_process_group>`"
msgstr ":py:obj:`init_process_group <megengine.distributed.init_process_group>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Initialize the distributed process group and specify the device used in the current process"
msgstr "初始化分布式进程组，并且指定在当前进程中使用的设备。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`new_group <megengine.distributed.new_group>`"
msgstr ":py:obj:`new_group <megengine.distributed.new_group>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Build a subgroup containing certain ranks."
msgstr "构造一个包含特定序号的子通信组。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`group_barrier <megengine.distributed.group_barrier>`"
msgstr ":py:obj:`group_barrier <megengine.distributed.group_barrier>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Block until all ranks in the group reach this barrier."
msgstr "阻止调用，直到组中的所有进程达到这个障碍点。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`override_backend <megengine.distributed.override_backend>`"
msgstr ":py:obj:`override_backend <megengine.distributed.override_backend>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Override distributed backend"
msgstr "重定义分布式后端"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`is_distributed <megengine.distributed.is_distributed>`"
msgstr ":py:obj:`is_distributed <megengine.distributed.is_distributed>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Return True if the distributed process group has been initialized."
msgstr "如果分布式进程组已完成初始化则返回True。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`get_backend <megengine.distributed.get_backend>`"
msgstr ":py:obj:`get_backend <megengine.distributed.get_backend>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Get the backend str."
msgstr "获取字符串形式表示的后端。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`get_client <megengine.distributed.get_client>`"
msgstr ":py:obj:`get_client <megengine.distributed.get_client>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Get client of python XML RPC server."
msgstr "获取 python XML RPC 服务器的客户端。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`get_mm_server_addr <megengine.distributed.get_mm_server_addr>`"
msgstr ":py:obj:`get_mm_server_addr <megengine.distributed.get_mm_server_addr>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Get master_ip and port of C++ mm_server."
msgstr "获取 C++ mm_server 的主机IP和端口。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`get_py_server_addr <megengine.distributed.get_py_server_addr>`"
msgstr ":py:obj:`get_py_server_addr <megengine.distributed.get_py_server_addr>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Get master_ip and port of python XML RPC server."
msgstr "获取 python XML RPC 服务器的主机IP和端口。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`get_rank <megengine.distributed.get_rank>`"
msgstr ":py:obj:`get_rank <megengine.distributed.get_rank>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Get the rank of the current process."
msgstr "返回当前进程的 Rank（进程序号）。"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid ":py:obj:`get_world_size <megengine.distributed.get_world_size>`"
msgstr ":py:obj:`get_world_size <megengine.distributed.get_world_size>`"

#: ../../source/reference/distributed.rst:37:<autosummary>:1
msgid "Get the total number of processes participating in the job."
msgstr "获取的参与任务的进程总数。"

#: ../../source/reference/distributed.rst:39
msgid "运行器（Launcher）"
msgstr "运行器（Launcher）"

#: ../../source/reference/distributed.rst:45:<autosummary>:1
msgid ":py:obj:`launcher <megengine.distributed.launcher>`"
msgstr ":py:obj:`launcher <megengine.distributed.launcher>`"

#: ../../source/reference/distributed.rst:45:<autosummary>:1
msgid "Decorator for launching multiple processes in single-machine/multi-machine multi-gpu training."
msgstr "用于在单机/多机中多gpu训练时启动多个进程的装饰器。"

#: ../../source/reference/distributed.rst:47
msgid "辅助功能（Helper）"
msgstr "辅助功能（Helper）"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid ":py:obj:`bcast_list_ <megengine.distributed.bcast_list_>`"
msgstr ":py:obj:`bcast_list_ <megengine.distributed.bcast_list_>`"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid "Broadcast tensors between given group."
msgstr "在指定通信组间广播张量列表。"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid ":py:obj:`synchronized <megengine.distributed.synchronized>`"
msgstr ":py:obj:`synchronized <megengine.distributed.synchronized>`"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid "Decorator."
msgstr "装饰器。"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid ":py:obj:`make_allreduce_cb <megengine.distributed.make_allreduce_cb>`"
msgstr ":py:obj:`make_allreduce_cb <megengine.distributed.make_allreduce_cb>`"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid "alias of :py:class:`megengine.distributed.helper.AllreduceCallback`"
msgstr "alias of :py:class:`megengine.distributed.helper.AllreduceCallback`"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid ":py:obj:`helper.AllreduceCallback <megengine.distributed.helper.AllreduceCallback>`"
msgstr ":py:obj:`helper.AllreduceCallback <megengine.distributed.helper.AllreduceCallback>`"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid "Allreduce Callback with tensor fusion optimization."
msgstr "具有张量融合优化的 Allreduce 回调函数。"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid ":py:obj:`helper.param_pack_split <megengine.distributed.helper.param_pack_split>`"
msgstr ":py:obj:`helper.param_pack_split <megengine.distributed.helper.param_pack_split>`"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid "Returns split tensor to list of tensors as offsets and shapes described, only used for ``parampack``."
msgstr "按照 ``offsets`` 和 ``shapes`` 的描述拆分输入 Tensor，并返回拆分后的 Tensor 列表，仅用于 ``parampack``。"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid ":py:obj:`helper.param_pack_concat <megengine.distributed.helper.param_pack_concat>`"
msgstr ":py:obj:`helper.param_pack_concat <megengine.distributed.helper.param_pack_concat>`"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid "Returns concated tensor, only used for ``parampack``."
msgstr "返回拼接后的 Tensor，仅用于 ParamPack。"

#: ../../source/reference/distributed.rst:58:<autosummary>:1
msgid ":py:obj:`helper.pack_allreduce_split <megengine.distributed.helper.pack_allreduce_split>`"
msgstr ":py:obj:`helper.pack_allreduce_split <megengine.distributed.helper.pack_allreduce_split>`"

#~ msgid "分布式训练（Distributed）"
#~ msgstr "分布式训练（Distributed）"

#~ msgid ":obj:`backend <megengine.distributed.backend>`"
#~ msgstr ""

#~ msgid ":obj:`Group <megengine.distributed.Group>`"
#~ msgstr ""

#~ msgid ":obj:`init_process_group <megengine.distributed.init_process_group>`"
#~ msgstr ""

#~ msgid ":obj:`new_group <megengine.distributed.new_group>`"
#~ msgstr ""

#~ msgid ":obj:`is_distributed <megengine.distributed.is_distributed>`"
#~ msgstr ""

#~ msgid ":obj:`get_backend <megengine.distributed.get_backend>`"
#~ msgstr ""

#~ msgid ":obj:`get_client <megengine.distributed.get_client>`"
#~ msgstr ""

#~ msgid ":obj:`get_mm_server_addr <megengine.distributed.get_mm_server_addr>`"
#~ msgstr ""

#~ msgid ":obj:`get_py_server_addr <megengine.distributed.get_py_server_addr>`"
#~ msgstr ""

#~ msgid ":obj:`get_rank <megengine.distributed.get_rank>`"
#~ msgstr ""

#~ msgid ":obj:`get_world_size <megengine.distributed.get_world_size>`"
#~ msgstr ""

#~ msgid ":obj:`group_barrier <megengine.distributed.group_barrier>`"
#~ msgstr ""

#~ msgid ":obj:`launcher <megengine.distributed.launcher>`"
#~ msgstr ""

#~ msgid ":obj:`Client <megengine.distributed.Client>`"
#~ msgstr ""

#~ msgid ":obj:`Server <megengine.distributed.Server>`"
#~ msgstr ""

#~ msgid ":obj:`bcast_list_ <megengine.distributed.bcast_list_>`"
#~ msgstr ""

#~ msgid ":obj:`synchronized <megengine.distributed.synchronized>`"
#~ msgstr ""

#~ msgid ":obj:`make_allreduce_cb <megengine.distributed.make_allreduce_cb>`"
#~ msgstr ""

#~ msgid "alias of :class:`megengine.distributed.helper.AllreduceCallback`"
#~ msgstr ""

#~ msgid ""
#~ ":obj:`helper.AllreduceCallback "
#~ "<megengine.distributed.helper.AllreduceCallback>`"
#~ msgstr ""

#~ msgid "客户端与服务端(C/S)"
#~ msgstr ""

#~ msgid ":py:obj:`Client <megengine.distributed.Client>`"
#~ msgstr ""

#~ msgid "Distributed Client for distributed training."
#~ msgstr "分布式训练的分布式客户端。"

