
msgid ""
msgstr ""
"Project-Id-Version:  megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-14 16:12+0800\n"
"PO-Revision-Date: 2021-12-28 12:21+0000\n"
"Last-Translator: \n"
"Language: zh_CN\n"
"Language-Team: Chinese Simplified\n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/reference/api/megengine.optimizer.Adagrad.rst:5
msgid "Adagrad"
msgstr ""

#: megengine.optimizer.adagrad.Adagrad:1 of
msgid "Implements Adagrad algorithm."
msgstr "实现Adagrad算法。"

#: megengine.optimizer.adagrad.Adagrad:3 of
msgid ""
"It has been proposed in `\"Adaptive Subgradient Methods for Online "
"Learning and Stochastic Optimization\" "
"<http://jmlr.org/papers/v12/duchi11a.html>`_."
msgstr ""
"这已经在 `\"Adaptive Subgradient Methods for Online Learning and Stochastic "
"Optimization\" <http://jmlr.org/papers/v12/duchi11a.html>` _ 中被提出。"

#: megengine.optimizer.adagrad.Adagrad of
msgid "参数"
msgstr ""

#: megengine.optimizer.adagrad.Adagrad:7 of
msgid "iterable of parameters to optimize or dicts defining parameter groups."
msgstr "可迭代对象，可以是一组待优化的参数，或定义几组参数的dict类型。"

#: megengine.optimizer.adagrad.Adagrad:10 of
msgid ""
"coefficient that scales delta before it is applied to the parameters. "
"Default: 1e-2"
msgstr "在将delta应用于参数之前缩放比例系数。默认: 1e-2"

#: megengine.optimizer.adagrad.Adagrad:13 of
msgid "learning rate decay. Default: 0"
msgstr "学习率衰减的乘数因子。默认: 0"

#: megengine.optimizer.adagrad.Adagrad:15 of
msgid ""
"term added to the denominator to improve numerical stability. Default: "
"1e-10"
msgstr "加到分母上以提高数值稳定性的值。默认: 1e-10"

#: megengine.optimizer.adagrad.Adagrad:18 of
msgid "weight decay (L2 penalty). Default: 0"
msgstr "权重衰减(L2惩罚)。默认：0"

#~ msgid "基类：:class:`megengine.optimizer.optimizer.Optimizer`"
#~ msgstr "基类：:class:`megengine.optimizer.optimizer.Optimizer`"

#~ msgid ""
#~ ":obj:`__init__ <megengine.optimizer.Adagrad.__init__>`\\ "
#~ "\\(params\\[\\, lr\\, lr\\_decay\\, eps\\, "
#~ "...\\]\\)"
#~ msgstr ""
#~ ":obj:`__init__ <megengine.optimizer.Adagrad.__init__>`\\ "
#~ "\\(params\\[\\, lr\\, lr\\_decay\\, eps\\, "
#~ "...\\]\\)"

#~ msgid "Initialize self."
#~ msgstr "初始化方法。"

#~ msgid "megengine.optimizer.Adagrad"
#~ msgstr "megengine.optimizer.Adagrad"

#~ msgid "Methods"
#~ msgstr "方法"

#~ msgid ""
#~ ":obj:`add_param_group "
#~ "<megengine.optimizer.Adagrad.add_param_group>`\\ "
#~ "\\(param\\_group\\)"
#~ msgstr ""
#~ ":obj:`add_param_group "
#~ "<megengine.optimizer.Adagrad.add_param_group>`\\ "
#~ "\\(param\\_group\\)"

#~ msgid ""
#~ "Add a param group to ``param_groups``"
#~ " of the :class:`~megengine.optim.optimizer.Optimizer`."
#~ msgstr ""
#~ "向 :class:`~megengine.optim.optimizer.Optimizer` 的 "
#~ "``param_groups`` 中添加一组参数。"

#~ msgid ":obj:`backward <megengine.optimizer.Adagrad.backward>`\\ \\(loss\\)"
#~ msgstr ":obj:`backward <megengine.optimizer.Adagrad.backward>`\\ \\(loss\\)"

#~ msgid ":obj:`bcast_param <megengine.optimizer.Adagrad.bcast_param>`\\ \\(\\)"
#~ msgstr ":obj:`bcast_param <megengine.optimizer.Adagrad.bcast_param>`\\ \\(\\)"

#~ msgid ":obj:`clear_grad <megengine.optimizer.Adagrad.clear_grad>`\\ \\(\\)"
#~ msgstr ":obj:`clear_grad <megengine.optimizer.Adagrad.clear_grad>`\\ \\(\\)"

#~ msgid "Set the grad attribute to None for all parameters."
#~ msgstr "把所有参数的梯度属性设置为 None。"

#~ msgid ""
#~ ":obj:`load_state_dict "
#~ "<megengine.optimizer.Adagrad.load_state_dict>`\\ \\(state\\)"
#~ msgstr ""
#~ ":obj:`load_state_dict "
#~ "<megengine.optimizer.Adagrad.load_state_dict>`\\ \\(state\\)"

#~ msgid "Loads the optimizer state."
#~ msgstr "加载优化器状态。"

#~ msgid ""
#~ ":obj:`state_dict <megengine.optimizer.Adagrad.state_dict>`\\ "
#~ "\\(\\[keep\\_var\\]\\)"
#~ msgstr ""
#~ ":obj:`state_dict <megengine.optimizer.Adagrad.state_dict>`\\ "
#~ "\\(\\[keep\\_var\\]\\)"

#~ msgid "Export the optimizer state."
#~ msgstr "导出优化器状态。"

#~ msgid ":obj:`step <megengine.optimizer.Adagrad.step>`\\ \\(\\)"
#~ msgstr ":obj:`step <megengine.optimizer.Adagrad.step>`\\ \\(\\)"

#~ msgid "Performs a single optimization step."
#~ msgstr "执行单一优化步骤。"

#~ msgid ":obj:`zero_grad <megengine.optimizer.Adagrad.zero_grad>`\\ \\(\\)"
#~ msgstr ":obj:`zero_grad <megengine.optimizer.Adagrad.zero_grad>`\\ \\(\\)"

