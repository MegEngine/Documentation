
msgid ""
msgstr ""
"Project-Id-Version:  megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-11-08 21:51+0800\n"
"PO-Revision-Date: 2021-08-17 23:16+0000\n"
"Last-Translator: \n"
"Language: zh_Hans_CN\n"
"Language-Team: Chinese Simplified\n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/reference/api/megengine.core.tensor.megbrain_graph.optimize_for_inference.rst:2
msgid "megengine.core.tensor.megbrain\\_graph.optimize\\_for\\_inference"
msgstr ""

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:1 of
msgid "Applies optimize_for_inference pass for computing graph."
msgstr ""

#: megengine.core.tensor.megbrain_graph.optimize_for_inference of
msgid "参数"
msgstr ""

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:3 of
msgid "list of output vars in the computing graph"
msgstr ""

#: megengine.core.tensor.megbrain_graph.optimize_for_inference of
msgid "Keyword Arguments"
msgstr "关键字参数"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:5 of
msgid ""
"whether to use float16 for I/O between oprs and use float32 as internal "
"computation precision. Note the output var would be changed to float16."
msgstr "是否使用float16作为算子间I/O的数据精度，同时float32作为内部计算的数据精度。注意输出变量的类型也随之更改为float16。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:8 of
msgid "whether to use float16 for both I/O and computation precision."
msgstr "是否使用float16同时作为算子间I/O和内部计算的数据精度。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:10 of
msgid "whether to use NHWCD4 data layout. This is faster on some OpenCL backend."
msgstr "是否使用NHWCD4数据格式。在某些OpenCL设备上，会提高计算速度。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:12 of
msgid "whether to use NCHW88 data layout, currently used in X86 AVX backend."
msgstr "是否使用NCHW88数据格式。当前用于X86 AVX后端。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:14 of
msgid "whether to use NCHW44 data layout, currently used in arm backend."
msgstr "是否使用NCHW44数据格式。当前用于arm后端。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:16 of
msgid ""
"whether to use NCHW44_dot data layout, currently used in armv8.2+dotprod "
"backend."
msgstr "是否使用NCHW4_dot数据格式。当前用于armv8.2+dotprod后端。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:18 of
msgid ""
"whether to use NCHW4 data layout, currently used in nvidia backend(based "
"on cudnn)."
msgstr "是否使用NCHW4数据格式。当前用于nvidia后端（基于cudnn）。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:20 of
msgid ""
"whether to use NCHW32 data layout, currently used in nvidia backend with "
"tensorcore(based on cudnn)."
msgstr "是否使用NCHW32数据格式。当前与tensorcore用于nvidia后端（基于cudnn）。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:22 of
msgid ""
"whether to use CHWN4 data layout, currently used in nvidia backend with "
"tensorcore."
msgstr "是否使用CHWN4数据格式。当前与tensorcore用于nvidia后端。"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:24 of
msgid ""
"whether to use NCHW64 data layout, used for fast int4 support on Nvidia "
"GPU."
msgstr ""

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:26 of
msgid "whether to fuse conv+bias+nonlinearty into one opr."
msgstr ""

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:28 of
#, fuzzy
msgid ""
"whether to fuse conv_bias with z input for inference on nvidia "
"backend(this optimization pass will result in mismatch of the precision "
"of output of training and inference)"
msgstr "推理阶段是否在nvidia后端对输入z融合 CONV + bias 成一个算子（这个优化会导致训练和推理的输出精度不一致）"

#~ msgid "param dest_vars"
#~ msgstr ""

#~ msgid "enable_io16xc32 --"
#~ msgstr "enable_io16xc32 --"

#~ msgid "enable_ioc16 --"
#~ msgstr "enable_ioc16 --"

#~ msgid "enable_hwcd4 --"
#~ msgstr "enable_hwcd4 --"

#~ msgid "enable_nchw88 --"
#~ msgstr "enable_nchw88 --"

#~ msgid "enable_nchw44 --"
#~ msgstr "enable_nchw44 --"

#~ msgid "enable_nchw44_dot --"
#~ msgstr "enable_nchw44_dot --"

#~ msgid "enable_nchw4 --"
#~ msgstr "enable_nchw4 --"

#~ msgid "enable_nchw32 --"
#~ msgstr "enable_nchw32 --"

#~ msgid "enable_chwn4 --"
#~ msgstr "enable_chwn4 --"

#~ msgid ""
#~ "enable_fuse_conv_bias_nonlinearity: whether to fuse"
#~ " conv+bias+nonlinearty"
#~ msgstr "enable_fuse_conv_bias_nonlinearity：是否融合 conv bias nonlinearty 算子"

#~ msgid "into one opr."
#~ msgstr "成一个算子。"

#~ msgid "enable_fuse_conv_bias_with_z: whether to fuse conv_bias with z"
#~ msgstr "enable_fuse_conv_bias_with_z："

