msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-08-04 16:51+0800\n"
"PO-Revision-Date: 2023-09-21 06:27\n"
"Last-Translator: \n"
"Language-Team: Chinese Simplified\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-CN\n"
"X-Crowdin-File: /dev/locales/zh_CN/LC_MESSAGES/reference/api/megengine.module.LSTM.po\n"
"X-Crowdin-File-ID: 9511\n"
"Language: zh_CN\n"

#: ../../source/reference/api/megengine.module.LSTM.rst:5
msgid "LSTM"
msgstr "LSTM"

#: megengine.module.rnn.LSTM:1 of
msgid "Applies a multi-layer long short-term memory LSTM to an input sequence."
msgstr "将多层 LSTM 应用于输入序列。"

#: megengine.module.rnn.LSTM:5 of
msgid "For each element in the input sequence, each layer computes the following function:"
msgstr "对于输入序列中的每个元素，每层都计算以下函数："

#: megengine.module.rnn.LSTM:8 of
msgid "\\begin{array}{ll} \\\\\n"
"    i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n"
"    f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n"
"    g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n"
"    o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n"
"    c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n"
"    h_t = o_t \\odot \\tanh(c_t) \\\\\n"
"\\end{array}\n\n"
msgstr "\\begin{array}{ll} \\\\\n"
"    i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n"
"    f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n"
"    g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n"
"    o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n"
"    c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n"
"    h_t = o_t \\odot \\tanh(c_t) \\\\\n"
"\\end{array}\n\n"

#: megengine.module.rnn.LSTM:18 of
msgid "where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell state at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{t-1}` is the hidden state of the layer at time `t-1` or the initial hidden state at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell, and output gates, respectively. :math:`\\sigma` is the sigmoid function, and :math:`\\odot` is the Hadamard product."
msgstr "其中 :math:`h_t` 是时间 `t` 的 hidden state, :math:`c_t` 是时间 `t` 的 cell state, :math:`x_t` 是时间 `t` 的输入， :math: `h_{t-1}` 是时间 `t-1` 的层的 hidden state 或时间 `0` 的初始 hidden state, :math:`i_t`, :math:`f_t`, :math:`g_t`, :math:`o_t` 分别是输入、遗忘、单元和输出门。 :math:`\\sigma` 是sigmoid函数， :math:`\\odot` 是 Hadamard 积。"

#: megengine.module.rnn.LSTM:25 of
msgid "In a multilayer LSTM, the input :math:`x^{(l)}_t` of the :math:`l` -th layer (:math:`l >= 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by dropout :math:`\\delta^{(l-1)}_t` where each :math:`\\delta^{(l-1)}_t` is a Bernoulli random variable which is :math:`0` with probability :attr:`dropout`."
msgstr "在多层LSTM中， :math:`l` 层（:math:`l >= 2`）的输入 :math:`x^{(l)}_t` 是前一层的 hidden state :math:`h^{(l-1)}_t` 乘以 dropout :math:`delta^{(l-1)}_t`，其中每个 :math:`delta^{(l-1)}_t` 是一个伯努利随机变量，其概率 :attr:`dropout` 为 :math:`0`."

#: megengine.module.rnn.LSTM:30 of
msgid "If ``proj_size > 0`` is specified, LSTM with projections will be used. This changes the LSTM cell in the following way. First, the dimension of :math:`h_t` will be changed from ``hidden_size`` to ``proj_size`` (dimensions of :math:`W_{hi}` will be changed accordingly). Second, the output hidden state of each layer will be multiplied by a learnable projection matrix: :math:`h_t = W_{hr}h_t`. Note that as a consequence of this, the output of LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact dimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128."
msgstr "如果指定 ``proj_size > 0``，将使用带投影的LSTM。这将以下列方式改变LSTM单元。首先， :math:`h_t` 的维度将从 ``hidden_size`` 改为 ``proj_size``（ :math:`W_{hi}`的维度将相应改变）。第二，每层的输出 hidden state 将被乘以一个可学习的投影矩阵： :math:`h_t = W_{hr}h_t`。请注意，这样做的结果是，LSTM网络的输出也将是不同的形状。所有变量的确切维度见下面的输入/输出部分。你可以在 https://arxiv.org/abs/1402.1128 中找到更多细节。"

#: megengine.module.rnn.LSTM of
msgid "参数"
msgstr "参数"

#: megengine.module.rnn.LSTM:38 of
msgid "The number of expected features in the input `x`"
msgstr "输入 `x ` 中的预期特征的数量"

#: megengine.module.rnn.LSTM:39 of
msgid "The number of features in the hidden state `h`"
msgstr "Hidden state `h` 中特征的数量"

#: megengine.module.rnn.LSTM:40 of
msgid "Number of recurrent layers. E.g., setting ``num_layers=2`` would mean stacking two LSTMs together to form a `stacked LSTM`, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1"
msgstr "Recurrent 层的数量。例如，设置 ``num_layers=2`` 意味着将两个LSTM堆叠在一起，形成一个 `stacked LSTM`，第二个LSTM接收第一个LSTM的输出并计算最终结果。默认值：1"

#: megengine.module.rnn.LSTM:44 of
msgid "If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`. Default: ``True``"
msgstr "如果 ``False``，那么该层不使用偏置权重 `b_ih` 和 `b_hh`。默认值: ``True``"

#: megengine.module.rnn.LSTM:46 of
msgid "If ``True``, then the input and output tensors are provided as `(batch, seq, feature)` instead of `(seq, batch, feature)`. Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details.  Default: ``False``"
msgstr "如果 ``True``，那么输入和输出 tesnor 被提供为 `(batch, seq, feature)`，而不是 `(seq, batch, feature)`。注意，这不适用于 hidden state 或 cell state. 详见下面的输入/输出部分。 默认值: ``False``"

#: megengine.module.rnn.LSTM:50 of
msgid "If non-zero, introduces a `Dropout` layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to :attr:`dropout`. Default: 0"
msgstr "如果非零，在每个LSTM层的输出上引入一个 ``dropout`` 层，除了最后一层，dropout 概率等于 :attr:`dropout`。默认值：0"

#: megengine.module.rnn.LSTM:53 of
msgid "If ``True``, becomes a bidirectional LSTM. Default: ``False``"
msgstr "如果是 ``True``，则成为一个双向LSTM。默认值: ``False``"

#: megengine.module.rnn.LSTM:54 of
msgid "If ``> 0``, will use LSTM with projections of corresponding size. Default: 0"
msgstr "如果 ``> 0``，将为LSTM设置相应大小的投影。默认值：0"

#: megengine.module.rnn.LSTM:79 of
msgid "Inputs: input, (h_0, c_0)"
msgstr "Inputs: input, (h_0, c_0)"

#: megengine.module.rnn.LSTM:57 of
msgid "**input**: tensor of shape :math:`(L, N, H_{in})` when ``batch_first=False`` or :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of the input sequence.  The input can also be a packed variable length sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence` or :func:`torch.nn.utils.rnn.pack_sequence` for details."
msgstr "**input**：当 ``batch_first=False`` 时，形状为 :math:`(L, N, H_{in})` 的tensor；当 ``batch_first=True`` 时，形状为:math:`(N, L, H_{in})` 的tensor，该tensor包含输入序列的特征。 输入也可以是一个打包的可变长度序列。详见 :func:`torch.nn.utils.rnn.pack_padded_sequence` 或 :func:`torch.nn.utils.rnn.pack_sequence`."

#: megengine.module.rnn.LSTM:62 of
msgid "**h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the initial hidden state for each element in the batch. Defaults to zeros if (h_0, c_0) is not provided."
msgstr "**h_0**：包含 batch 中每个元素的初始 hidden state 的形状为 :math:`(D *\\text{num\\_layers}, N, H_{out})` 的 tensor. 如果没有提供（h_0, c_0），则默认为零。"

#: megengine.module.rnn.LSTM:65 of
msgid "**c_0**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the initial cell state for each element in the batch. Defaults to zeros if (h_0, c_0) is not provided."
msgstr "**c_0**：包含 batch 中每个元素的初始 cell state 的形状为 :math:`(D *\\text{num\\_layers}, N, H_{cell})` 的 tensor. 如果没有提供（h_0, c_0），则默认为零。"

#: megengine.module.rnn.LSTM:69 of
msgid "where:"
msgstr "其中："

#: megengine.module.rnn.LSTM:71 of
msgid "\\begin{aligned}\n"
"    N ={} & \\text{batch size} \\\\\n"
"    L ={} & \\text{sequence length} \\\\\n"
"    D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n"
"    H_{in} ={} & \\text{input\\_size} \\\\\n"
"    H_{cell} ={} & \\text{hidden\\_size} \\\\\n"
"    H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\\n"
"\\end{aligned}\n\n"
msgstr "\\begin{aligned}\n"
"    N ={} & \\text{batch size} \\\\\n"
"    L ={} & \\text{sequence length} \\\\\n"
"    D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n"
"    H_{in} ={} & \\text{input\\_size} \\\\\n"
"    H_{cell} ={} & \\text{hidden\\_size} \\\\\n"
"    H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\\n"
"\\end{aligned}\n\n"

#: megengine.module.rnn.LSTM:90 of
msgid "Outputs: output, (h_n, c_n)"
msgstr "Outputs: output, (h_n, c_n)"

#: megengine.module.rnn.LSTM:82 of
msgid "**output**: tensor of shape :math:`(L, N, D * H_{out})` when ``batch_first=False`` or :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features `(h_t)` from the last layer of the LSTM, for each `t`. If a :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output will also be a packed sequence."
msgstr "**output**：当 ``batch_first=False`` 时，形状为 :math:`(L, N, D * H_{out})` 的tensor；当 ``batch_first=True`` 时，形状为 :math:`(N, L, D * H_{out})` 的tensor，该tensor包含LSTM最后一层的关于每个 `t` 的输出特征 `(h_t)`。如果一个 :class:`torch.nn.utils.rnn.PackedSequence` 被作为输入，输出也将是一个打包序列。"

#: megengine.module.rnn.LSTM:87 of
msgid "**h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the final hidden state for each element in the batch."
msgstr "**h_n**：形状为 :math:`(D *\\text{num\\_layers}, N, H_{out})`，包含 batch 中每个元素的最终 hidden state 的 tensor."

#: megengine.module.rnn.LSTM:89 of
msgid "**c_n**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the final cell state for each element in the batch."
msgstr "**c_n**：形状为 :math:`(D *\\text{num\\_layers}, N, H_{cell})`，包含 batch 中每个元素的最终 cell state 的 tensor."

#: megengine.module.rnn.LSTM:93 of
msgid "实际案例"
msgstr "实际案例"

#: megengine.module.rnn.LSTM:107 of
msgid "Outputs:"
msgstr "输出："

