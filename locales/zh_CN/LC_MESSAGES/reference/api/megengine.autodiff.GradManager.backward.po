msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-06-03 10:50+0800\n"
"PO-Revision-Date: 2021-07-08 05:04\n"
"Last-Translator: \n"
"Language-Team: Chinese Simplified\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-CN\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/zh_CN/LC_MESSAGES/reference/api/megengine.autodiff.GradManager.backward.po\n"
"X-Crowdin-File-ID: 6586\n"
"Language: zh_CN\n"

#: ../../source/reference/api/megengine.autodiff.GradManager.backward.rst:2
msgid "megengine.autodiff.GradManager.backward"
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward:1 of
msgid "Compute gradients (or vector-Jacobian product) for all attached tensors, accumulate to corresponding .grad attribute, and release resources along the way."
msgstr "对所有 attached Tensor 计算梯度 (或向量-雅可比积)，并将其积累到对应 Tensor 的 .grad 属性中，在过程中释放资源。"

#: megengine.autodiff.grad_manager.GradManager.backward:4 of
msgid ":meth:`backward` computes the vector-Jacobian product :math:`dx_j = \\sum_{i} dy_i J_{ij}` where :math:`J_{ij} = ∂y_i/∂x_j` is the Jacobian matrix between vector variables :math:`y` and :math:`x`, with all vectors involved represented as a list of tensors, in the sense of direct sums (or flatten-and-concatenate). :math:`y` and :math:`dy` are passed as the first and second parameter respectively, whereas :math:`x` is directly taken from the list of all attached tensors. The result :math:`dx` is also not returned. Instead, it is directly accumulated into the .grad attribute of matching attached tensors (a.k.a. :math:`x`). This can be done unambiguously since :math:`dx` as a list of tensors has the same structure as :math:`x`."
msgstr ":meth:`backward` 计算向量-雅可比积 :math:`dx_j = \\sum_{i} dy_i J_{ij}` ，其中 :math:`J_{ij} = ∂y_i/∂x_j` 是向量变量 :math:`y` 和 :math:`x` 的雅可比矩阵，涉及的所有向量都在向量空间直和 (或展平并拼接) 的意义下表示为 Tensor 的列表。:math:`y` and :math:`dy` 分别是第一和第二个参数，:math:`x` 则由所有被 :meth:`attach` 的 Tensor 的列表形成。计算的结果 :math:`dx` 不作为返回值，而是直接积累到对应 Tensor (即 :math:`x`) 的 .grad 属性上。这个对应可以无歧义地完成，因为 :math:`dx` 作为 Tensor 的列表和 :math:`x` 有着相同的结构。"

#: megengine.autodiff.grad_manager.GradManager.backward:14 of
msgid "If :math:`y` is a scalar and :math:`dy` is chosen to be 1, the vector-Jacobian product yield gradient of :math:`y` with repect to :math:`x` as a special case. In that case, you will be able to omit the :math:`dy` parameter and :meth:`backward` will automatically use 1 for it and compute the gradient."
msgstr "如果 :math:`y` 是一个标量而 :math:`dy` 被取为 1，那么向量-雅可比积的结果恰好就是 :math:`y` 对 :math:`x` 的梯度。在这种情况下，您可以省略 :math:`dy` 参数，:meth:`backward` 会自动使用 1 作为它的值并计算梯度。"

#: megengine.autodiff.grad_manager.GradManager.backward:19 of
msgid ":meth:`backward` consumes all resources held by this GradManager and releases them in the process of this call. When the call successfully finishes, the GradManager will be put back to an inactive state."
msgstr ":meth:`backward` 会随计算的进行释放 GradManager 持有的所有资源。在函数调用成功返回后，GradManager 会回到非活跃的状态。"

#: megengine.autodiff.grad_manager.GradManager.backward of
msgid "参数"
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward:23 of
msgid "tensor or list of tensors"
msgstr "Tensor 或 Tensor 的列表"

#: megengine.autodiff.grad_manager.GradManager.backward:24 of
msgid "tensor or list of tensors. Defaults to 1 if y is scalar"
msgstr "Tensor 或 Tensor 的列表。如果 y 是标量，有默认值 1"

