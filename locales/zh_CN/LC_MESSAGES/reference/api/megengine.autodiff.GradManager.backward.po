# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2021, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.4.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-06-03 10:50+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/reference/api/megengine.autodiff.GradManager.backward.rst:2
msgid "megengine.autodiff.GradManager.backward"
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward:1 of
msgid ""
"Compute gradients (or vector-Jacobian product) for all attached tensors, "
"accumulate to corresponding .grad attribute, and release resources along "
"the way."
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward:4 of
msgid ""
":meth:`backward` computes the vector-Jacobian product :math:`dx_j = "
"\\sum_{i} dy_i J_{ij}` where :math:`J_{ij} = ∂y_i/∂x_j` is the Jacobian "
"matrix between vector variables :math:`y` and :math:`x`, with all vectors"
" involved represented as a list of tensors, in the sense of direct sums "
"(or flatten-and-concatenate). :math:`y` and :math:`dy` are passed as the "
"first and second parameter respectively, whereas :math:`x` is directly "
"taken from the list of all attached tensors. The result :math:`dx` is "
"also not returned. Instead, it is directly accumulated into the .grad "
"attribute of matching attached tensors (a.k.a. :math:`x`). This can be "
"done unambiguously since :math:`dx` as a list of tensors has the same "
"structure as :math:`x`."
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward:14 of
msgid ""
"If :math:`y` is a scalar and :math:`dy` is chosen to be 1, the vector-"
"Jacobian product yield gradient of :math:`y` with repect to :math:`x` as "
"a special case. In that case, you will be able to omit the :math:`dy` "
"parameter and :meth:`backward` will automatically use 1 for it and "
"compute the gradient."
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward:19 of
msgid ""
":meth:`backward` consumes all resources held by this GradManager and "
"releases them in the process of this call. When the call successfully "
"finishes, the GradManager will be put back to an inactive state."
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward of
msgid "参数"
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward:23 of
msgid "tensor or list of tensors"
msgstr ""

#: megengine.autodiff.grad_manager.GradManager.backward:24 of
msgid "tensor or list of tensors. Defaults to 1 if y is scalar"
msgstr ""

