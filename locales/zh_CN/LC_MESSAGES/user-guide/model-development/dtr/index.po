msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-08 20:45+0800\n"
"PO-Revision-Date: 2021-12-08 19:02\n"
"Last-Translator: \n"
"Language: zh_CN\n"
"Language-Team: Chinese Simplified\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-CN\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/model-development/dtr/index.po\n"
"X-Crowdin-File-ID: 8179\n"

#: ../../source/user-guide/model-development/dtr/index.rst:5
msgid "使用 DTR 进行显存优化"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:7
msgid "MegEngine 通过引入 `DTR <https://arxiv.org/pdf/2006.09616.pdf>`_ :footcite:p:`kirisame2021dynamic` 技术来进行动态图下的显存优化，同时也支持在静态图下开启。"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:10
msgid "Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic tensor rematerialization. In International Conference on Learning Representations. 2021. URL: https://openreview.net/forum?id=Vfs_2RnOD0H."
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:12
msgid "DTR 使用与配置方式"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:14
msgid "在训练代码之前添加一行代码，即可启用动态图的 DTR 显存优化："
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:18
msgid "用户现在可以直接开启 DTR 优化，不再需要设置一个显存阈值  :py:data:`~.dtr.eviction_threshold` 作为触发条件。 MegEngine 默认会在当前空闲的显存无法满足一次申请时尝试进行优化，根据 DTR 策略找出最优的 Tensor 并释放其显存，直到该次显存申请成功。"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:22
msgid "而在 1.4 版本中，必须提前设置好显存阈值，才能开启 DTR 显存优化："
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:27
msgid "显存阈值的设置技巧"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:30
msgid "一般情况下，显存阈值设得越小，显存峰值就越低，训练耗时也会越大；显存阈值设得越大，显存峰值就越高，训练耗时也会越小。 但值得注意的是，当显存阈值接近显卡容量时，容易引发碎片问题。因为 DTR 是根据活跃的显存大小来执行释放操作的，释放掉的 Tensor 在显卡上的物理地址很可能不连续。 例如：释放了两个物理位置不相邻的 100MB 的 Tensor, 仍然无法满足一次 200MB 显存的申请。此时就会自动触发碎片整理操作，对性能造成巨大影响。"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:34
msgid "结合分布式训练"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:37
msgid "在分布式情景下，我们通常会使用 :class:`~.distributed.launcher` 将一个函数包装成一个多进程运行的函数， 此时如果想要开启 DTR 显存优化，需要在被包装的函数中定义 DTR 的参数："
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:47
msgid "如果你还不清楚相关概念，可参考 :ref:`distributed-guide` 页面了解细节。"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:51
msgid "还有一些其它的接口如 :py:data:`~.dtr.evictee_minimum_size`, :py:data:`~.dtr.enable_sqrt_sampling` ... 可以对 DTR 策略进行自定义，更多配置说明请参考 :py:mod:`~.dtr` 模块的 API 文档页面。"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:55
msgid "在静态图下开启 DTR"
msgstr ""

#: ../../source/user-guide/model-development/dtr/index.rst:57
msgid "用户在编译静态图时使用 :class:`~.jit.DTRConfig` 设置 :class:`~.jit.trace` 的参数 ``dtr_config``, 就可以打开 DTR 优化："
msgstr ""

#~ msgid "参数设置"
#~ msgstr ""

#~ msgid "动态图 Sublinear 显存优化（DTR）"
#~ msgstr "Dynamic graphics Sublinear video memory optimization (DTR)"

#~ msgid ""
#~ "MegEngine 通过引入 `Dynamic Tensor "
#~ "Rematerialization <https://arxiv.org/pdf/2006.09616.pdf>`_ "
#~ "（简称 DTR）技术，进一步工程化地解决了动态图显存优化的问题，从而享受到大 Batchsize "
#~ "训练带来的收益。"
#~ msgstr ""
#~ "MegEngine by introducing Dynamic Tensor "
#~ "Rematerialization ` "
#~ "<https://arxiv.org/pdf/2006.09616.pdf>` _ (referred "
#~ "to as DTR) technology, further "
#~ "engineering solution to the dynamic "
#~ "graph memory optimization problems in "
#~ "order to enjoy the benefits brought "
#~ "about by large Batchsize training."

#~ msgid "单卡训练"
#~ msgstr "Single card training"

#~ msgid "使用方式十分简单，在训练代码之前添加两行代码："
#~ msgstr "Very simple to use, add two lines of code in the code before training："

#~ msgid "即可启用动态图的 Sublinear 显存优化。"
#~ msgstr ""
#~ "Then you can enable Sublinear video "
#~ "memory optimization for dynamic graphics."

#~ msgid "更多设置"
#~ msgstr "More settings"

#~ msgid "参数介绍"
#~ msgstr "Parameter introduction"

#~ msgid "参数名"
#~ msgstr "parameter name"

#~ msgid "实际含义"
#~ msgstr "Actual meaning"

#~ msgid "``eviction_threshold``"
#~ msgstr "``eviction_threshold``"

#~ msgid ""
#~ "显存阈值（单位：字节）。当被使用的显存总和超过该阈值后，显存优化会生效，根据 DTR 的策略找出最优的 "
#~ "tensor 并释放其显存，直到被使用的显存总和低于该阈值。默认值：0"
#~ msgstr ""
#~ "Video memory threshold (unit:：bytes). When "
#~ "the total used video memory exceeds "
#~ "the threshold, the video memory "
#~ "optimization will take effect. According "
#~ "to the DTR strategy, the optimal "
#~ "tensor will be found and its video"
#~ " memory will be released until the"
#~ " total used video memory is lower "
#~ "than the threshold. Default value：0"

#~ msgid "``evictee_minimum_size``"
#~ msgstr "``evictee_minimum_size''"

#~ msgid ""
#~ "被释放显存的 tensor 的大小下限（单位：字节）。只有当 tensor "
#~ "的大小不小于该下限时，才有可能被 DTR 策略选中释放其显存。默认值：1048576"
#~ msgstr ""
#~ "The lower limit of the size of "
#~ "the tensor to be released (in：bytes)."
#~ " Only when the size of the "
#~ "tensor is not less than the lower"
#~ " limit, can it be selected by "
#~ "the DTR strategy to release its "
#~ "video memory. Default value：1048576"

#~ msgid "``enable_sqrt_sampling``"
#~ msgstr "``enable_sqrt_sampling''"

#~ msgid ""
#~ "是否开启根号采样。设当前候选 tensor 集合的大小为 :math:`n`，开启该设置后，每次需要释放"
#~ " tensor 时只会遍历 :math:`\\sqrt{n}` 个候选 "
#~ "tensor。默认值：False"
#~ msgstr ""
#~ "Whether to enable root sampling. "
#~ "Provided the size of the current "
#~ "candidate set tensor :math:`n` within "
#~ "the, after turning on the set, "
#~ "each traversed only required when "
#~ "released tensor :math:` \\ sqrt{n}`candidate"
#~ " tensor. Default value：False"

#~ msgid "设置方法请参考 :py:mod:`~.dtr`"
#~ msgstr "Please refer to the setting method: py:mod:`~.dtr`"

#~ msgid ""
#~ "``eviction_threshold`` 表示开始释放 tensor "
#~ "的显存阈值。当被使用的显存大小超过该阈值时，动态图显存优化会生效， 根据 DTR 的策略找出最优的"
#~ " tensor 并释放其显存，直到活跃的显存大小不超过该阈值。因此实际运行时的活跃显存峰值比该阈值高一些属于正常现象。"
#~ msgstr ""
#~ "``eviction_threshold`` indicates the threshold "
#~ "of the video memory to start "
#~ "releasing the tensor. When the used "
#~ "video memory size exceeds the threshold,"
#~ " the dynamic graph video memory "
#~ "optimization will take effect. According "
#~ "to the DTR strategy, the optimal "
#~ "tensor is found and its video "
#~ "memory is released until the active "
#~ "video memory size does not exceed "
#~ "the threshold. Therefore, it is normal"
#~ " that the peak value of active "
#~ "video memory during actual operation is"
#~ " higher than this threshold."

#~ msgid "一般情况下，显存阈值设得越小，显存峰值就越低，训练耗时也会越大；显存阈值设得越大，显存峰值就越高，训练耗时也会越小。"
#~ msgstr ""
#~ "In general, the smaller the video "
#~ "memory threshold is set, the lower "
#~ "the video memory peak, and the "
#~ "longer the training time; the larger "
#~ "the video memory threshold, the higher"
#~ " the video memory peak and the "
#~ "less training time will be."

#~ msgid "下图是 ResNet50（batch size=200）在2080Ti（显存：11GB）上设定不同显存阈值后的性能表现。"
#~ msgstr ""
#~ "The following figure shows the "
#~ "performance of ResNet50 (batch size=200) "
#~ "after setting different video memory "
#~ "thresholds："

#~ msgid "性能表现"
#~ msgstr "Performance"

#~ msgid "如上图（左）所示，"
#~ msgstr "As shown in the picture above (left),"

#~ msgid "当显存阈值从 2 增长到 7 的时候，训练耗时是越来越低的，因为随着显存阈值升高，释放掉的 tensor 数量变少，重计算的开销降低；"
#~ msgstr ""
#~ "When the video memory threshold "
#~ "increases from 2 to 7, the "
#~ "training time becomes lower and lower,"
#~ " because as the video memory "
#~ "threshold increases, the number of "
#~ "released tensors decreases and the cost"
#~ " of recalculation decreases;"

#~ msgid "当显存阈值增长到 8 和 9 的时候，可供申请的空闲显存总和已经不多，并且地址大概率不连续，导致需要不断地进行碎片整理，造成训练耗时显著增长，"
#~ msgstr ""
#~ "When the video memory threshold "
#~ "increases to 8 and 9, the total"
#~ " amount of free video memory "
#~ "available for application is not much,"
#~ " and the address is likely to "
#~ "be discontinuous, resulting in the need"
#~ " for continuous defragmentation, resulting "
#~ "in a significant increase in training"
#~ " time."

#~ msgid "当显存阈值增长到 10 之后，空闲的显存甚至无法支持一次 kernel 的计算，导致 OOM."
#~ msgstr ""
#~ "When the video memory threshold "
#~ "increases to 10, the idle video "
#~ "memory cannot even support a kernel "
#~ "calculation, resulting in OOM."

#~ msgid "显存峰值"
#~ msgstr "Video memory peak"

#~ msgid "如上图（右）所示，可以看出显存阈值和显存峰值之间有很大的差距。"
#~ msgstr ""
#~ "As shown in the figure above "
#~ "(right), it can be seen that there"
#~ " is a big gap between the video"
#~ " memory threshold and the video "
#~ "memory peak."

#~ msgid "当显存阈值在 2 到 5 之间时，显存峰值都在 8 左右；"
#~ msgstr ""
#~ "When the video memory threshold is "
#~ "between 2 and 5, the peak video"
#~ " memory is around 8;"

#~ msgid "当显存阈值在 6 到 9 之间时，显存峰值更是逼近显存总容量。"
#~ msgstr ""
#~ "When the video memory threshold is "
#~ "between 6 and 9, the peak video"
#~ " memory is closer to the total "
#~ "video memory capacity."

#~ msgid ""
#~ "前者的原因是，DTR 只能保证在任意时刻，被使用的显存总和在显存阈值附近，但是这些被使用的显存的地址不一定连续。 "
#~ "被释放掉的空闲块会被 MegEngine 收集起来，当最大的空闲块大小也满足不了一次申请时, "
#~ "MegEngine 会从 CUDA 申请一段新的显存， "
#~ "虽然被使用的显存总量在显存阈值附近，但是显存峰值上升了； 后者的原因是显存容量总共只有 "
#~ "11G，如果最大的空闲块大小也无法满足申请时只能靠碎片整理来满足申请，峰值不会变得更大。"
#~ msgstr ""
#~ "The reason for the former is that"
#~ " DTR can only guarantee that the "
#~ "sum of the used video memory is"
#~ " near the video memory threshold at"
#~ " any time, but the addresses of "
#~ "these used video memory are not "
#~ "necessarily continuous. The free blocks "
#~ "that are released will be collected "
#~ "by MegEngine. When the maximum free "
#~ "block size is not enough for one"
#~ " request, MegEngine will apply for a"
#~ " new segment of video memory from "
#~ "CUDA. Although the total amount of "
#~ "video memory used is near the "
#~ "video memory threshold, the peak video"
#~ " memory is The reason for the "
#~ "latter is that the total video "
#~ "memory capacity is only 11G. If "
#~ "the maximum free block size cannot "
#~ "meet the application, the application "
#~ "can only be met by defragmentation, "
#~ "and the peak value will not become"
#~ " larger."

#~ msgid "所以从 ``nvidia-smi`` 上看到的显存峰值会显著高于显存阈值。"
#~ msgstr ""
#~ "Therefore, the peak video memory seen"
#~ " from ``nvidia-smi'' will be "
#~ "significantly higher than the video "
#~ "memory threshold."

#~ msgid "综上所述，在实际训练过程中，显存阈值需要用户根据模型和显卡的具体情况设定。"
#~ msgstr ""
#~ "In summary, in the actual training "
#~ "process, the video memory threshold "
#~ "needs to be set by the user "
#~ "according to the specific conditions of"
#~ " the model and graphics card."

#~ msgid "FAQ"
#~ msgstr "FAQ"

#~ msgid ""
#~ "Q：为什么 ``eviction_threshold=2GB`` 的时候训练耗时远高于 "
#~ "``eviction_threshold=3GB`` 的训练耗时？"
#~ msgstr ""
#~ "Q：Why `` eviction_threshold = 2GB`` time"
#~ " consuming training is much higher "
#~ "than the training takes `` "
#~ "eviction_threshold = 3GB`` of?"

#~ msgid ""
#~ "A：因为在该模型中，不可被释放的 tensor（例如：参数、执行当前算子需要用到的输入 tensor "
#~ "和产生的输出 tensor 等等）的大小之和一直保持在 2GB 以上，所以几乎所有的 "
#~ "tensor 都会在不被用到的时刻立即被释放，所以会产生非常可观的重计算时间开销。"
#~ msgstr ""
#~ "A：Because in this model, the total "
#~ "size of tensors that cannot be "
#~ "released (for example,：parameter, the input"
#~ " tensor needed to execute the current"
#~ " operator and the output tensor, "
#~ "etc.) has been kept above 2GB, so"
#~ " almost all The tensor will be "
#~ "released immediately when it is not "
#~ "used, so it will incur considerable "
#~ "recalculation time overhead."

#~ msgid ""
#~ "Q：为什么 ``eviction_threshold=2GB`` 的时候显存峰值高于 "
#~ "``eviction_threshold=3GB`` 的显存峰值？"
#~ msgstr ""
#~ "Q：Why `` eviction_threshold = 2GB`` "
#~ "memory when the peak is higher "
#~ "than the peak memory `` "
#~ "eviction_threshold = 3GB`` of?"

#~ msgid ""
#~ "A：原因同上，由于 ``eviction_threshold=2GB`` 时重计算次数远多于 "
#~ "``eviction_threshold=3GB`` ，需要频繁地申请和释放显存， "
#~ "一旦某次空闲块大小不能满足申请，显存峰值就会增加，所以 ``eviction_threshold=2GB`` "
#~ "时显存峰值大概率更高。"
#~ msgstr ""
#~ "A：reason is the same as above, "
#~ "because ``eviction_threshold=2GB`` time "
#~ "recalculation times are far more than"
#~ " ``eviction_threshold=3GB``, it is necessary "
#~ "to frequently apply for and release "
#~ "the video memory, once a certain "
#~ "free block size can not meet the"
#~ " application, the peak video memory "
#~ "will be Increased, so "
#~ "``eviction_threshold=2GB'' has a higher "
#~ "probability of peak video memory."

#~ msgid "Q：用不同的 ``eviction_threshold`` 训练模型时的显存峰值可以估算吗？"
#~ msgstr ""
#~ "Q：memory peaks at different use `` "
#~ "eviction_threshold`` training model can "
#~ "estimate it?"

#~ msgid "A：很难。这取决于 DTR 策略释放和重计算了哪些 tensor，以及具体到某次显存申请时空闲块大小能否满足要求，这些都会影响最终的显存峰值。"
#~ msgstr ""
#~ "A：is difficult. This depends on which"
#~ " tensors are released and recalculated "
#~ "by the DTR strategy, and whether "
#~ "the free block size can meet the"
#~ " requirements during a certain video "
#~ "memory application, which will affect "
#~ "the final video memory peak."

