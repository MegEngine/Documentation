# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.8\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-04-19 16:51+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:4
msgid "Load and run 设置选项列表及说明"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:9
msgid "基本设置选项"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:13
msgid "以下选项用于 Load and run 运行时的一些基本设置，如推理运行次数，输入设置等"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:16
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:122
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:176
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:274
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:381
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:484
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:559
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:603
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:647
msgid "选项列表"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:22
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:128
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:182
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:280
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:329
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:387
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:490
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:565
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:609
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:653
msgid "设置选项"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:23
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:129
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:183
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:281
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:330
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:388
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:491
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:566
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:610
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:654
msgid "功能"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:24
msgid "``--iter <iter_number>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:25
msgid "设置模型测速的迭代次数(默认值为 ``10`` )"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:26
msgid "``--warmup-iter <warmup_number>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:27
msgid "模型测速前warm up的次数（默认值为 ``1`` ）"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:28
msgid "``--thread <thread_number>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:29
msgid "验证当前程序是否支持thread，可以提供多个线程跑多个模型的验证（默认值为 ``1`` ）"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:30
msgid "``--input \"<data_0_name:data_0>;...;<data_n_name:data_n>\"``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:31
msgid "设置输入数据"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:37
msgid ""
"``--thread`` 选项主要是为了验证 Load and run 是否支持多个线程跑多个模型，不会有计算加速。 "
"如果需要使用多线程来加速推理，可以参考 :ref:`device-options` 中 ``multithread`` 相关的设置"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:42
msgid "``--input`` 目前支持以下几种输入格式："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:48
msgid "格式名称"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:49
msgid "格式定义"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:50
msgid "格式说明"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:51
msgid "字符串输入"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:52
msgid "“<data_name>:[[data00,data01,...],[data10,data11,...],...]]”"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:53
msgid "这一方式只适用于简单的验证性测试时使用"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:54
msgid "json文件"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:55
msgid "如表后所示"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:56
msgid "与 string 方式相差不多，raw 部分数据需要表示为一个字符串列表"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:57
msgid "numpy 数据"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:58
msgid "使用python numpy 保存的数据"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:59
msgid "比较常用"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:60
msgid "ppm 以及 pgm 格式图像"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:61
msgid "`ppm 以及 pgm <https://en.wikipedia.org/wiki/Netpbm#File_formats>`__"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:62
msgid "数据格式比较原始，相当于 json 格式的二进制版"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:64
msgid "如下所示为 json 格式数据的一个简单示例："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:79
msgid "json数据输入时需要保证与上述示例格式一致，输入数据名称要与模型中的输入相同"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:81
msgid "numpy 数据生成："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:90
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:153
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:242
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:343
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:441
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:518
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:579
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:624
#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:672
msgid "使用示例"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:112
msgid "fast-run 相关设置"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:114
msgid "fast-run 的设置主要用于在存在多种算法实现的算子中选出其中在当前情况下性能最好的算法。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:118
msgid ""
"使用 fast-run 相关配置前需要保证 fast-run 部分代码的可用性， MegEngine 使用宏 "
"``MGB_ENABLE_FASTRUN`` 来控制。编译时加上选项 ``-DMGB_ENABLE_FASTRUN=1`` 即可。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:119
msgid "MegEngine 默认会开启 ``MGB_ENABLE_FASTRUN``。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:130
msgid "``--full-run``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:131
msgid "profile 各算子所有的算法（包括 naive 的算法），选择其中性能最好的算法进行推理。对应的 Strategy 为：``PROFILE``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:132
msgid "``--fast-run``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:133
msgid "profile 优化的算法，选择其中性能最好的算法进行推理。对应的 Strategy 为：``PROFILE+OPTIMIZED``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:134
msgid "``--fast-run-algo-policy <cache_file>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:135
msgid "设置给定文件中的缓存算法作为推理时用到的算法，或者将推理时选择到的算法缓存到给定文件中。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:136
msgid "``--reproducible``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:137
msgid "在可复现的算法集合中选择算法，用到的算法可以保证前后两次推理结果的一致性。对应的 Strategy 为：``REPRODUCIBLE``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:138
msgid "``--fast-run-shared-batch-size <size>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:139
msgid ""
"使用统一给定的 batch size 来选择相应算法，忽略模型的 batch size 变化， 该选项设置算法 negativate "
"属性为：``USABLE_DEPEND_ON_SHAPE``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:140
msgid "``--binary-equal-between-batch``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:141
msgid ""
"在精度对 batch 不敏感的算法中选择。 该选项设置算法 negativate "
"属性为：``ACCURACY_DEPEND_ON_BATCH``，同时会设置 Strategy 为：``REPRODUCIBLE``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:145
msgid "Megengine 算法选择默认的 Strategy 为：``HEURISTIC``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:146
msgid "所谓精度对 batch 敏感，是指在多 batch 的情况下，即使各 batch 的输入内容完全一致，其对应的输出也完全不同。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:150
msgid "有些特殊的算子可能没有除 naive 以外的算法，此时运行终止，报相关错误信息"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:173
msgid "IO相关设置"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:184
msgid "``--input \"data_name:data_file|data_string;...;...\"``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:185
msgid ""
"输入用户自定义数据，支持的数据格式：json 文件，ppm pgm 图像，npy 数据，自定义数据字符串， 参考 :ref:`basic-"
"options` 。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:186
msgid "``--io-dump``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:187
msgid "以文本的形式 dump 计算图中算子的输入输出"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:188
msgid "``--io-dump-stdout|--io-dump-stderr``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:189
msgid "与 ``--io-dump`` 功能相同，只是将相关文本输出到标准输出或标准错误中。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:190
msgid "``--bin-io-dump <dir_name>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:191
msgid "以二进制的形式 dump算子的 IO 信息，输出二进制文件到 dir_name 的文件夹中,文件名称为各个算子的输出 tensor 的内部 id"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:192
msgid "``--bin-out-dump <dir_name>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:193
msgid "以二进制的形式 dump算子的输出信息，输出与 ``--bin-io-dump`` 类似。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:194
msgid "``--copy-to-host``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:195
msgid ""
"将 device 上的输出 copy 到 host 上，默认情况下不会进行输出 d2h 的 copy 操作。该设置选项用来设置输出 tensor "
"从 device 到 host 的拷贝，用于测速实际应用中真正用到的运行时间。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:199
msgid "文本形式输出信息如下所示："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:209
msgid "主要包括变量 tensorid 变量 tensor 节点所在 opr,变量依赖节点 tesnor id，以及变量 tensor 相关值等"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:212
msgid ""
"二进制输出文件格式定义（参考 `dump_tensor "
"<https://github.com/MegEngine/MegEngine/blob/master/src/core/impl/utils/debug.cpp#L447>`__）为："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:227
msgid ""
"参照该格式可以解析算子的相关信息。具体解析的实现细节可以参考 MegEngine 提供的 binary io 比较工具 "
"`megengine.tools.compare_binary_iodump "
"<https://github.com/MegEngine/MegEngine/blob/master/imperative/python/megengine/tools/compare_binary_iodump.py>`__"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:229
msgid "``--bin-out-dump`` 输出文件的名称格式定义："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:236
msgid ""
"细节参考 `outdumper "
"<https://github.com/MegEngine/MegEngine/blob/master/Lite/load_and_run/src/helpers/outdumper.cpp>`__"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:263
msgid ""
"text 形式只是显示了部分信息，比如 Tensor 的前几个输出结果，整个 Tensor 的平均值、标准差之类， "
"如果需要具体到哪个值错误，通常用 binary 的方式进行验证"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:271
msgid "layout 优化相关设置"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:282
msgid "``--enable-nchw4``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:283
msgid "使用 ``{N, C/4, H, W, 4}`` layout 格式的优化，``GPU int8`` 模型有加速。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:284
msgid "``--enable-chwn4``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:285
msgid ""
"使用 ``{C/4, H, W, N, 4}`` 的 layout 格式的优化，``NVIDIA tensorcore int8`` "
"模型有加速。（该格式为 Megengine 定义）"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:286
msgid "``--enable-nchw44``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:287
msgid "使用 ``{N/4, C/4, H, W, 4, 4}`` 的 layout 格式的优化，``Arm CPU float32`` 模型有加速。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:288
msgid "``--enable-nchw88``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:289
msgid ""
"使用 ``{N/8, C/8, H, W, 8, 8}`` 的 layout 格式的优化，``x86 CPU（支持 "
"avx256）flloat32`` 模型有加速。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:290
msgid "``--enable-nchw32``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:291
msgid ""
"使用 ``{N, C/32, H, W, 32}`` 的 layout 格式的优化，``NVIDIA tensorcore int8`` "
"模型有加速。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:292
msgid "``--enable-nchw64``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:293
msgid ""
"使用 ``{N, C/64, H, W, 64}`` 的 layout 格式的优化，``NVIDIA tensorcore`` `fast "
"int4 <https://developer.nvidia.com/blog/int4-for-ai-inference/>`__ 模型有加速。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:294
msgid "``--enable-nhwcd4``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:295
msgid "使用 ``{N, H, W, (C+3)/4, 4} `` 的 layout 格式的优化，移动平台 ``GPU float16`` 模型有加速。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:296
msgid "``--enable-nchw44-dot``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:297
msgid ""
"使用 ``{N/4, C/4, H, W, 4, 4}`` 的 layout 格式的优化，``Arm CPU arch>=8.2`` "
"量化模型有加速。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:300
msgid ""
"各种 layout 的细节可以参考 `layout_manager "
"<https://github.com/MegEngine/MegEngine/blob/master/src/gopt/include/megbrain/gopt/reformat_manager.h>`__"
" 。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:305
msgid ""
"对于 ``--enable-nchw32`` 使用时需要开启 ``--enable-fuse-conv-bias-nonlinearity``, "
"可以选择性开启 ``--enable-fuse-conv-bias-with-z``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:306
msgid "选项可以在 dump 时开启，参考 :ref:`dump` 的推理优化设置选项。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:307
msgid ""
"使用 ``--enable-nchw44-dot`` 编译选项需要加上 ``-march=armv8.2-a+fp16+dotprod``， "
"Megengine 提供的编译脚本会自动进行环境检测开启这一选项"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:312
msgid "全局 layout 优化"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:316
msgid ""
"上述单一的 layout 转换实现简单，只能在 **固定平台以及特定算子** 上有明显的加速，另外 layout 转换的开销使得 **局部最优的 "
"layout 转换不一定是全局上最优的**。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:318
msgid ""
"基于上述两个问题, MegEngine 引入了全局 layout 优化的机制，该机制通过统一的 layout 管理，针对不同后端 "
"**profile 不同的 layout 转换性能，全局规划，自动选择最合适的 layout 转换** ，得到全局最优的 layout "
"转换路径，从而实现推理加速。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:320
msgid ""
"全局 layout 优化可以直接将计算图中的 layout 优化融合到计算密集的算子中，并将其中冗余的 layout "
"转换消除，可以直接得到优化后的模型计算图，从而可以直接获取优化后的模型，减少了部署时额外的优化设置。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:323
msgid "Load and run 为全局 layout 优化提供了如下两个设置接口"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:331
msgid "``--layout-transform <backend_type>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:332
msgid ""
"使用给定 backend 的全局 layout 优化 pass ,支持的 backend 包括： ``cpu`` ，``cuda`` "
"等。该选项用来设置全局 layout 优化的后端类型，并启用这一优化选项。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:333
msgid "``--layout-transform-dump <model_path_after_layout_transform>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:334
msgid "将进行全局 layout 优化之后的模型重新进行 dump，得到layout优化之后的模型。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:338
msgid ""
"``--layout-transform-dump`` 选项使用时需要与全局 layout 优化的设置 ``--layout-"
"transform`` 同时使用。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:376
msgid "算子融合以及其他优化"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:378
msgid "这些优化选项主要包括前处理以及可融合的算子优化，预热优化，存储优化以及计算 kern record 优化，通过这些设置期望减少推理的运行时间."
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:389
msgid "``--enable-fuse-preprocess``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:390
msgid ""
"允许前处理融合，如融合 astype + pad_channel + dimshuffle 等算子。实现细节参考 "
"`fuse_nchw4_int8_preprocess "
"<https://github.com/MegEngine/MegEngine/blob/master/src/gopt/impl/fuse_nchw4_int8_preprocess.cpp>`__"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:391
msgid "``--weight-preprocess``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:392
msgid ""
"允许 weight 前处理，此时，会返回执行前的 kern,用于前处理，因此可能会占用较多内存（常用于 winograd/im2col 等 "
"conv 算法的 fast-run 中）"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:393
msgid "``--enable-fuse-conv-bias-nonlinearity``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:394
msgid "允许convolution, bias add, relu oprs 的 fuse，三者融合成一个 ConvBiasForward opr."
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:395
msgid "``--enable-fuse-conv-bias-with-z``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:396
msgid "允许 ConvBias, z(binary elemwise) oprs 的 fuse，将二者融合为 ConvBiasForward op"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:397
msgid "``--const-shape``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:398
msgid "将所有 SharedDeviceTensor 和 Host2DeviceCopy的tensor shape 设置为不可变的。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:399
msgid "``--fake-first``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:400
msgid "允许下次执行时，仅执行非计算任务，如内存分配，队列初始化等。常用来减少预热时间，且在执行完后会置为 false"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:401
msgid "``--no-sanity-check``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:402
msgid "不在首次执行时进行变量合理性检查，此时需要用户保证其变量合理。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:403
msgid "``--disable-mem-opt``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:404
msgid "不允许计算序列的内存优化，主要禁止静态内存的再使用以及内存规划。用于测试在原生的内存分配策略下的推理性能"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:405
msgid "``--workspace-limit <size>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:406
msgid "设置 workspace 的上限，设备存储有限是，需要限制workspace上限来保证推理正确进行"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:407
msgid "``--record-comp-seq | --record-comp-seq2``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:408
msgid "第一次执行的时候, 记录整个计算过程中会调用的 kern，在移动端 GPU 上有很大提升。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:409
msgid "``--enbale-jit``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:410
msgid ""
"JIT 开关，打开时可以在允许运行时的计算图编译，设置　JIT level 为1，即仅对 elemwise 类的算子起作用，level 为 2 "
"时，会进一步包含 reduce 的 opr"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:414
msgid "record 设置有两级，``--record-comp-seq`` 为常用，设置开启时会记录整个计算过程中会调用的 kern。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:416
msgid "``--record-comp-seq2`` 除了记录计算 kern 之外，会析构掉存储在graph上的一些信息。起到节省内存的作用"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:420
msgid "两个使用时都有一定的限制，如下："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:422
msgid "level1限制条件："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:424
msgid "所有变量静态分配内存且 tensor shape 必须保持不变，执行时被 record 的 kern 才不会失效 。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:426
msgid "数据同步只会在运行结束后进行，否则同步以及同步结果无法保证正确性（同步过程中可能存在无法记录的非计算逻辑）。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:428
msgid "计算图中只有一种计算设备的抽象在执行，也就意味，record 只在单一的固定设备上生效。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:430
msgid "level2限制条件："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:432
msgid "预热 fake_next_exec 以及变量合理性检查 var_sanity_check_first_run 需要关掉"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:434
msgid "计算图编译之前，变量 shape 需要设置合适，如 ``--const-shape`` 设置在可变的 Tensor 上时会导致record 失败"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:436
msgid "参考 :ref:`record_optimize`"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:477
msgid "设备相关设置选项"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:481
msgid "Load and run 可以指定推理用到的后端设备，设备被抽象为 CompNode, 通过制定 CompNode 的映射信息来指定对应推理后端。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:492
msgid "``--cuda``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:493
msgid "设置 CompNode 为 cuda 上的 CompNode"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:494
msgid "``--cpu``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:495
msgid "设置 CompNode 为 cpu 上的 CompNode"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:496
msgid "``--cpu-default``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:497
msgid "将所有任务分派到 caller 线程上, 对于低端 CPU 设备，能够减少同步所需时间提高推理性能"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:498
msgid "``--multithread <thread_number>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:499
msgid "设置 CompNode 为 multithread 上的 CompNode，多线程推理加速"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:500
msgid "``--multithread-default <thread_number>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:501
msgid "将任务分派到线程池的各线程上，caller 线程为主线程。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:502
msgid "``--multithread <thread_number> --multi-thread-core-ids <id0,id1,...>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:503
msgid "设置 multithread 绑核，对应 cpu id 由 id0,id1等给出。（常用于 ARM 设备上的绑核操作，验证不同核上的推理性能）"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:504
msgid "``--rocm``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:505
msgid ""
"设置为 ROCm 平台上执行，暂时只支持非 MegEngine Lite 的模型，设备主要支持 AMD GPU（支持 ROCm），编译时开启 "
"编译选项：``-DMGE_WITH_ROCM=1``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:506
msgid "``--rocm-enable-miopen-search``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:507
msgid ""
"使用 `MIOpen <https://github.com/ROCmSoftwarePlatform/MIOpen>`__ 相关算法自动 "
"tuning"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:508
msgid "``--tensorrt```"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:509
msgid "使用 tesorRT作为后端进行推理"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:510
msgid "``--tensorrt-cache <cache_path>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:511
msgid "使用 tensorRT engine 来预生成 ICudaEngine，缓存到给定文件中"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:515
msgid "tensorRT 编译时需开启 ``-DMGB_ENABLE_TENSOR_RT=1`` , MegEngine 的脚本是默认开启该选项的"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:552
msgid "插件相关设置选项"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:556
msgid ""
"这些选项主要用于对 MegEngine 中的 `plugin "
"<https://github.com/MegEngine/MegEngine/tree/master/src/plugin/include/megbrain/plugin>`__"
" 进行设置"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:567
msgid "``--check-dispatch``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:568
msgid "检查 cpu dispatch 情况,当算子在 cpu 上没有调用 dispatch 时，会输出警告到标准输出上"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:569
msgid "``--range <range_number>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:570
msgid "检查计算图中所有数字的绝对值是否在给定范围内。超出范围会抛出异常信息"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:571
msgid "``--check-var-value switch_interval:init_var_idx``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:572
msgid "检查计算图中计算序列的第 init_var_idx++ 个变量节点，在执行 switch_interval 次后变量的值。需要进行多次迭代才能使用。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:573
msgid "``--profile <profile_cache>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:574
msgid ""
"记录计算图中各算子的运行信息，将其以 json 文件的格式保存。编译时开启编译选项： ``-DMGB_ENABLE_JSON=1`` "
"，Megengine 提供的脚本默认开启。json 数据的分析参考 :ref:`lar-profile-model`"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:575
msgid "``--profile-host <porfile_cache>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:576
msgid "只记录在 host 上运行的算子信息，以快速得到相关性能情况。（device 上的 profile 可能十分缓慢）"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:600
msgid "debug用到的一些设置选项"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:611
msgid "``--mode-info``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:612
msgid "以表格形式展示模型的输入输出信息。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:613
msgid "``--verbose``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:614
msgid ""
"设置 MegEngine 以及 MegEngine Lite 的 log 级别为 debug 级别，用于展示更多运行时信息（ "
"debug，warning 以及 error )"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:615
msgid "``--disable-assert-throw``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:616
msgid "不在计算图执行时进行 assert 操作，常用于性能调优（前提是运行结果默认正确）。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:617
msgid "``--get-static-mem-info <dir_name>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:618
msgid ""
"获取计算图以及运行显存信息的 json 文件用于显存和性能可视化，参考 :ref:`lar-debug` "
"。编译时开启编译选项：``-DMGB_ENABLE_JSON=1``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:619
msgid "``--wait-gdb``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:620
msgid "输出当前进程 PID 给 gdb 工具 attach 用。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:644
msgid "外部定义的Copr加载选项"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:655
msgid "``--c-opr-lib <dynamic_lib_path>``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:656
msgid "将第三方的算子库封装为 MegEngine 可以识别接口，传入 MegEngine 进行调用。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:657
msgid "``--c-opr-lib-with-param``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:658
msgid "使用外部的参数来运行相关的 opr,主要是包括算子执行时需要的输入输出信息以及算子执行设备的信息。"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:662
msgid "外部算子库要进行封装时，需要提供四个主要的 C API 供 MegEngine 接入："
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:664
msgid "库入口：``MGB_C_OPR_INIT_FUNC``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:665
msgid "内存分配：``copr_param_device_ptr_malloc``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:666
msgid "内存在 host 与 device 上的迁移：``copr_param_device_ptr_h2d``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:667
msgid "内存释放：``copr_param_device_ptr_free``"
msgstr ""

#: ../../source/user-guide/deployment/lite/load_and_run/options-list.rst:669
msgid "其中后三个 API 为可选实现，库入口 API 必须实现"
msgstr ""

