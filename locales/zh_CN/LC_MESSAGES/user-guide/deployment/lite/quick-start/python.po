# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2021, The MegEngine Open Source Team
# This file is distributed under the same license as the MegEngine package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MegEngine 1.7\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-14 16:12+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:5
msgid "MegEngine Lite Python 部署模型快速上手"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:9
msgid ""
"Lite 的 Python 包是和 MegEngine 本体绑定在一起的，所以只需要 :ref:`安装 MegEngine 包 <install-"
"with-pip>` 即可使用。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:10
msgid "相较于 C++ 推理，该方法可以直接加载导出的模型并执行推理，无需经历复杂的编译环节。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:14
msgid "但是这种方式目前只支持 Windows/MacOS/Linux x86 和 CUDA 版本，不支持 Android Arm."
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:16
msgid ""
"本文将从获取一个训练好的 ``shufflenet_v2`` 模型出发，讲解如何使用 MegEngine Lite 的 Python "
"接口将其部署到 Linux x86 中环境下运行。 主要分为以下小节："
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:19
msgid ":ref:`lite-model-dump-python`"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:20
msgid ":ref:`lite-infer-code-python`"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:25
msgid "导出已经训练好的模型"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:27
msgid "请参考 :ref:`get-model`。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:32
msgid "编写并执行 Inference 代码"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:34
msgid ""
"首先创建一个 ``inference.py``, 在这个文件中将直接调用 MegEngine Lite 的 Python 接口运行 "
"``shufflenet_v2.mge`` 模型， 注意输入数据 ``input_tensor`` 是随机生成的，不用在乎计算结果。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:69
msgid "上面代码主要完成了几个步骤，包括："
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:71
msgid "创建默认配置的 Network；"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:72
msgid "载入模型，MegEngine Lite 将读取并解析模型文件，并创建计算图；"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:73
msgid "通过输入 Tensor 的名字获取模型的输入 Tensor, 并设置随机数作为输入数据；"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:74
msgid "执行 Inference 逻辑；"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:75
msgid "获取模型输出 Tensor, 并处理输出数据。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:77
msgid "这样这个调用 MegEngine Lite 的 Python 接口的 demo 就完成了。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:80
msgid "使用 CUDA 进行推理"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:81
#: ../../source/user-guide/deployment/lite/quick-start/python.rst:122
msgid ""
"下面将通过 CUDA 运行 shufflenet.mge 来展示如何使用 TensorBatchCollector 来攒 "
"batch，攒好之后传递到 network 的输入 tensor 中进行推理。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:118
msgid "上面示例主要演示在 CUDA 设备上进行 Inference 的接口调用过程。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:121
msgid "使用 TensorBatchCollector 辅助完成 CUDA 推理"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:175
msgid "上面示例主要做了以下事情："
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:177
msgid ""
"通过 :ref:`lite_config` 和 :ref:`lite_io` 来创建一个运行在 CUDA 上的 Network，并配置该 "
"Network 中输入名字为 \"data\" 的 Tensor 在 CUDA 上，这样用户可以直接将 CUDA device 上的内存 "
"share 给它。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:178
msgid "通过该 Network 加载 shufflenet 模型，并获取名字为 \"data\" 的输入 Tensor，以及它的 layout 信息。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:179
msgid ""
"通过输入 tensor 的 layout 信息和 batch 信息，将创建一个在 CUDA 上的 "
"TensorBatchCollector，并循环攒了 4 个 batch。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:180
msgid ""
"然后将 TensorBatchCollector 中的 tensor 和 Network 的输入 tensor 通过 "
"share_memory_with 进行内存 share。"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:181
msgid "执行推理，获取输出数据"
msgstr ""

#: ../../source/user-guide/deployment/lite/quick-start/python.rst:185
msgid ""
"上面通过 share_memory_with 进行内存共享，将不会产生多余的数据 copy，其中 TensorBatchCollector "
"的使用请参考 :ref:`lite_utils_api`。"
msgstr ""

#~ msgid "创建默认配置的 Network （第 10 行）；"
#~ msgstr ""

#~ msgid "载入模型，MegEngine Lite 将读取并解析模型文件，并创建计算图（第 11 行）；"
#~ msgstr ""

#~ msgid "通过输入 Tensor 的名字获取模型的输入 Tensor, 并设置随机数作为输入数据（第 13~16 行）"
#~ msgstr ""

#~ msgid "执行 Inference 逻辑（第 19~20 行）；"
#~ msgstr ""

#~ msgid "获取模型输出 Tensor, 并处理输出数据（第 22~426 行）。"
#~ msgstr ""

