msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-28 09:14+0000\n"
"PO-Revision-Date: 2021-12-28 12:23\n"
"Last-Translator: \n"
"Language: zh_CN\n"
"Language-Team: Chinese Simplified\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-CN\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/deployment/index.po\n"
"X-Crowdin-File-ID: 2862\n"

#: ../../source/user-guide/deployment/index.rst:5
msgid "模型部署总览与流程建议"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:7
msgid "当使用 MegEngine 完成模型的训练过程后，为了让模型可以实现它的价值，我们需要对模型进行“部署”， 即在特定的硬件设备和系统环境限制下，使用模型进行推理。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:10
msgid "根据最终部署设备的不同，我们可能将会经历不同的部署路线："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:16
msgid "计算硬件"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:17
msgid "举例"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:18
msgid "适用场景"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:20
msgid "有 Python 环境的设备"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:21
msgid "GPU 服务器"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:22
msgid "希望尽量简单，不在意 Python 性能上的限制"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:23
msgid "C / C++ 环境的设备"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:24
msgid "任何设备，尤其是嵌入式芯片、TEE 环境等"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:25
msgid "希望性能尽量高、资源占用低，能接受编译 C++ 库的复杂性"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:26
msgid "NPU"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:27
msgid "Atlas / RockChip / 寒武纪等芯片"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:28
msgid "需要利用 NPU 的算力，接受稍复杂的转换步骤"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:30
msgid "在下面这张流程图中，可以了解到不同部署路线中的几个基本步骤："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:48
msgid "为了更好的选择模型部署，需要了解到以下几点："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:50
msgid "最推荐的路线为训练代码 -> ``.tm`` 文件 -> ``.mge`` 文件 -> Lite 执行；"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:51
msgid "如果你的团队中存在研究员 / 工程人员的分工，建议以 ``.tm`` 文件做为分界面 —— 研究员负责交付 ``.tm`` 模型（永久存档），工程人员负责后续的部署流程；"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:53
msgid "如果你独立负责完整的训练到部署过程，且不在意长期存档模型。 为了快捷，可以直接从训练代码生成 ``.mge`` 文件（即上述虚线），结果是等价的。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:58
msgid ":ref:`tracedmodule-quick-start`"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:59
msgid ":ref:`lite-quick-start-python`"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:60
msgid ":ref:`lite-quick-start-cpp`"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:61
msgid "`mgeconvert <https://github.com/megengine/mgeconvert>`_ : 适用于 MegEngine 的各种转换器。"
msgstr ""

#~ msgid "将模型部署到 C++ 环境"
#~ msgstr "Deploy the model to the C++ environment"

#~ msgid ""
#~ "MegEngine 的一大核心优势是 “训练推理一体化”，其中 “训练” 是在 "
#~ "Python 环境中进行的， 而 “推理” 则特指在 C++ "
#~ "环境下使用训练完成的模型进行推理。 而将模型迁移到无需依赖 Python "
#~ "的环境中，使其能正常进行推理计算，被称为 **部署** 。 "
#~ "部署的目的是简化除了模型推理所必需的一切其它依赖，使推理计算的耗时变得尽可能少， "
#~ "比如手机人脸识别场景下会需求毫秒级的优化，而这必须依赖于 C++ 环境才能实现。"
#~ msgstr ""
#~ "One of the core advantages of "
#~ "MegEngine is \"integration of training "
#~ "and inference\", where \"training\" is "
#~ "carried out in a Python environment, "
#~ "and \"inference\" specifically refers to "
#~ "using the trained model for inference"
#~ " in a C++ environment. Migrating the"
#~ " model to an environment that does"
#~ " not need to rely on Python so"
#~ " that it can perform inference "
#~ "calculations normally is called "
#~ "**deployment**. The purpose of deployment "
#~ "is to simplify all other dependencies"
#~ " except model reasoning, so that the"
#~ " time-consuming calculation of reasoning"
#~ " becomes as little as possible. For"
#~ " example, in the mobile phone face"
#~ " recognition scenario, millisecond-level "
#~ "optimization is required, which must "
#~ "rely on the C++ environment ."

#~ msgid "本文将从一个训练好的异或（XOR）网络模型出发，讲解如何将其部署到 CPU（X86）环境下运行。"
#~ msgstr ""
#~ "This article will start from a "
#~ "trained exclusive OR (XOR) network model"
#~ " and explain how to deploy it "
#~ "to run in a CPU (X86) environment."

#~ msgid "将模型序列化并导出"
#~ msgstr "Serialize and export the model"

#~ msgid "首先我们需要 :ref:`dump` , 用到的模型定义与序列化代码在 :src:`sdk/xor-deploy/xornet.py` ."
#~ msgstr ""
#~ "First we need :ref:`dump`, the model "
#~ "definition and serialization code used "
#~ "are in :src:`sdk/xor-deploy/xornet.py`."

#~ msgid "编写 C++ 程序读取模型"
#~ msgstr "Write a C++ program to read the model"

#~ msgid "接下来我们需要编写一个 C++ 程序，来实现我们期望在部署平台上完成的功能。"
#~ msgstr ""
#~ "Next we need to write a C++ "
#~ "program to implement the functions we"
#~ " expect to be completed on the "
#~ "deployment platform."

#~ msgid ""
#~ "继续以上面导出的异或网络模型为例子，我们实现一个最简单的功能—— 给定两个浮点数，输出对其做异或操作后结果为 0"
#~ " 的概率（以及为 1 的概率）。"
#~ msgstr ""
#~ "Continuing to take the XOR network "
#~ "model derived above as an example, "
#~ "we implement the simplest function-given"
#~ " two floating-point numbers, output "
#~ "the probability that the result will "
#~ "be 0 (and the probability of being"
#~ " 1) after XORing them."

#~ msgid ""
#~ "在此之前，为了能够正常使用 MegEngine 底层 C++ 接口， 需要先按照"
#~ " MegeEngine 中提供的编译脚本( :src:`scripts/` ) "
#~ "从源码编译得到 MegEngine 的相关库, "
#~ "通过这些脚本可以交叉编译安卓（ARMv7，ARMv8，ARMv8.2）版本、Linux "
#~ "版本（ARMv7，ARMv8，ARMv8.2）以及 iOS 相关库， 也可以本机编译 "
#~ "Windows/Linux/MacOS 相关库文件。"
#~ msgstr ""
#~ "Prior to this, in order to be "
#~ "able to use the underlying C++ "
#~ "interface of MegEngine normally, you "
#~ "need to :src:`scripts/` ). Through these"
#~ " scripts, you can cross compile "
#~ "Android (ARMv7, ARMv8, ARMv8.2) version, "
#~ "Linux version (ARMv7, ARMv8, ARMv8.2) "
#~ "and iOS related libraries, you can "
#~ "also compile Windows/Linux/MacOS related "
#~ "library files natively."

#~ msgid "实现上述异或计算的示例 C++ 代码如下（引自 ``xor-deploy.cpp`` ）："
#~ msgstr ""
#~ "The sample C++ code for realizing "
#~ "the above XOR calculation is as "
#~ "follows (quoted from ``xor-deploy.cpp``)："

#~ msgid "简单解释一下代码的意思："
#~ msgstr "Briefly explain the meaning of the code："

#~ msgid "我们首先通过 ``GraphLoader`` 将模型加载进来，"
#~ msgstr "We first load the model through ``GraphLoader``,"

#~ msgid "接着通过 ``tensor_map`` 和上节指定的输入名称 ``data`` ，找到模型的输入指针，"
#~ msgstr ""
#~ "Then find the input pointer of the"
#~ " model through ``tensor_map`` and the "
#~ "input name ``data`` specified in the "
#~ "previous section,"

#~ msgid "再将运行时提供的输入 ``x`` 和 ``y`` 赋值给输入指针，"
#~ msgstr ""
#~ "Then assign the input ``x`` and "
#~ "``y`` provided at runtime to the "
#~ "input pointer,"

#~ msgid "然后我们使用 ``network.graph->compile`` 将模型编译成一个函数接口，并调用执行，"
#~ msgstr ""
#~ "Then we use ``network.graph->compile`` to "
#~ "compile the model into a functional "
#~ "interface, and call execution,"

#~ msgid "最后将得到的结果 ``predict`` 进行输出，该输出的两个值即为异或结果为 0 的概率以及为 1 的概率。"
#~ msgstr ""
#~ "Finally, output the result ``predict``, "
#~ "and the two values of the output"
#~ " are the probability that the XOR "
#~ "result is 0 and the probability "
#~ "that it is 1."

#~ msgid ""
#~ "另外可以配置上面加载模型时候的 ``config`` 来优化 Inference 计算效率，"
#~ " 为了加速一般在 ARM 上面配置 ``enable_nchw44_layout()`` "
#~ ", 在x86 CPU 上面配置 ``enable_nchw88_layout()`` "
#~ "，具体的配置方法参考 :ref:`load-and-run` ."
#~ msgstr ""
#~ "In addition, you can configure the "
#~ "``config'' when loading the model above"
#~ " to optimize the calculation efficiency "
#~ "of Inference. In order to speed up"
#~ " the general configuration of "
#~ "``enable_nchw44_layout()'' on the ARM, and "
#~ "``enable_nchw88_layout()'' on the x86 CPU, "
#~ "the specific configuration Method reference"
#~ " :ref:`load-and-run`."

#~ msgid "编译与执行"
#~ msgstr "Compile and execute"

#~ msgid ""
#~ "为了更完整地实现 “训练推理一体化”，我们还需要支持同一个 C++ "
#~ "程序能够交叉编译到不同平台上执行，而不需要修改代码。 之所以能够实现不同平台一套代码，是由于底层依赖的算子库（内部称作 "
#~ "MegDNN）实现了对不同平台接口的封装， 在编译时会自动根据指定的目标平台选择兼容的接口。"
#~ msgstr ""
#~ "In order to realize the \"integration"
#~ " of training and reasoning\" more "
#~ "completely, we also need to support "
#~ "the same C++ program to be "
#~ "cross-compiled for execution on different"
#~ " platforms without modifying the code. "
#~ "The reason why a set of codes "
#~ "for different platforms can be realized"
#~ " is because the underlying operator "
#~ "library (called MegDNN internally) realizes"
#~ " the encapsulation of the interfaces "
#~ "of different platforms, and the "
#~ "compatible interface is automatically selected"
#~ " according to the specified target "
#~ "platform during compilation."

#~ msgid ""
#~ "目前发布的版本我们开放了对 CPU（X86、X64、ARMv7、ARMv8、ARMv8.2） 和 "
#~ "GPU（CUDA）平台的 float 和量化 int8 的支持。"
#~ msgstr ""
#~ "In the currently released version, we"
#~ " have opened up support for float "
#~ "and quantized int8 on CPU (X86, "
#~ "X64, ARMv7, ARMv8, ARMv8.2) and GPU "
#~ "(CUDA) platforms."

#~ msgid "我们在这里以 CPU（X86）平台为例，首先直接使用 gcc 或者 clang （用 ``$CXX`` 指代）进行编译即可："
#~ msgstr ""
#~ "Here we take the CPU (X86) "
#~ "platform as an example. First, directly"
#~ " use gcc or clang ($CXX``) to "
#~ "compile："

#~ msgid ""
#~ "上面的 ``$MGE_INSTALL_PATH`` 指代了编译安装时通过 "
#~ "``CMAKE_INSTALL_PREFIX`` 指定的安装路径。 编译完成之后，通过以下命令执行即可："
#~ msgstr ""
#~ "The ``$MGE_INSTALL_PATH`` above refers to "
#~ "the installation path specified by "
#~ "``CMAKE_INSTALL_PREFIX`` during compilation and "
#~ "installation. After the compilation is "
#~ "complete, execute the following command "
#~ "to："

#~ msgid ""
#~ "这里将 ``$MGE_INSTALL_PATH/lib`` 加进 ``LD_LIBRARY_PATH``"
#~ " 环境变量，确保 MegEngine 库可以被编译器找到。 上面命令对应的输出如下："
#~ msgstr ""
#~ "Here, add ``$MGE_INSTALL_PATH/lib`` to the "
#~ "``LD_LIBRARY_PATH`` environment variable to "
#~ "ensure that the MegEngine library can"
#~ " be found by the compiler. The "
#~ "output corresponding to the above "
#~ "command is as follows："

#~ msgid ""
#~ "至此我们便完成了从 Python 模型到 C++ 可执行文件的部署流程， "
#~ "如果需要快速的运行模型以及测试模型性能，请参考 :ref:`load-and-run` ."
#~ msgstr ""
#~ "So far we have completed the "
#~ "deployment process from the Python model"
#~ " to the C++ executable file. If "
#~ "you need to run the model quickly"
#~ " and test the performance of the "
#~ "model, please refer to :ref:`load-"
#~ "and-run`."

