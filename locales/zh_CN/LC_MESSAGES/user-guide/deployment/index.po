
msgid ""
msgstr ""
"Project-Id-Version:  megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-28 09:14+0000\n"
"PO-Revision-Date: 2021-06-03 10:19+0000\n"
"Last-Translator: \n"
"Language: zh_CN\n"
"Language-Team: Chinese Simplified\n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/user-guide/deployment/index.rst:5
msgid "模型部署总览与流程建议"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:7
msgid ""
"当使用 MegEngine 完成模型的训练过程后，为了让模型可以实现它的价值，我们需要对模型进行“部署”， "
"即在特定的硬件设备和系统环境限制下，使用模型进行推理。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:10
msgid "根据最终部署设备的不同，我们可能将会经历不同的部署路线："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:16
msgid "计算硬件"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:17
msgid "举例"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:18
msgid "适用场景"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:20
msgid "有 Python 环境的设备"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:21
msgid "GPU 服务器"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:22
msgid "希望尽量简单，不在意 Python 性能上的限制"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:23
msgid "C / C++ 环境的设备"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:24
msgid "任何设备，尤其是嵌入式芯片、TEE 环境等"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:25
msgid "希望性能尽量高、资源占用低，能接受编译 C++ 库的复杂性"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:26
msgid "NPU"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:27
msgid "Atlas / RockChip / 寒武纪等芯片"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:28
msgid "需要利用 NPU 的算力，接受稍复杂的转换步骤"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:30
msgid "在下面这张流程图中，可以了解到不同部署路线中的几个基本步骤："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:48
msgid "为了更好的选择模型部署，需要了解到以下几点："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:50
msgid "最推荐的路线为训练代码 -> ``.tm`` 文件 -> ``.mge`` 文件 -> Lite 执行；"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:51
msgid ""
"如果你的团队中存在研究员 / 工程人员的分工，建议以 ``.tm`` 文件做为分界面 —— 研究员负责交付 ``.tm`` "
"模型（永久存档），工程人员负责后续的部署流程；"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:53
msgid "如果你独立负责完整的训练到部署过程，且不在意长期存档模型。 为了快捷，可以直接从训练代码生成 ``.mge`` 文件（即上述虚线），结果是等价的。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:58
msgid ":ref:`tracedmodule-quick-start`"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:59
msgid ":ref:`lite-quick-start-python`"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:60
msgid ":ref:`lite-quick-start-cpp`"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:61
msgid ""
"`mgeconvert <https://github.com/megengine/mgeconvert>`_ : 适用于 MegEngine "
"的各种转换器。"
msgstr ""

#~ msgid "将模型部署到 C++ 环境"
#~ msgstr ""

#~ msgid ""
#~ "MegEngine 的一大核心优势是 “训练推理一体化”，其中 “训练” 是在 "
#~ "Python 环境中进行的， 而 “推理” 则特指在 C++ "
#~ "环境下使用训练完成的模型进行推理。 而将模型迁移到无需依赖 Python "
#~ "的环境中，使其能正常进行推理计算，被称为 **部署** 。 "
#~ "部署的目的是简化除了模型推理所必需的一切其它依赖，使推理计算的耗时变得尽可能少， "
#~ "比如手机人脸识别场景下会需求毫秒级的优化，而这必须依赖于 C++ 环境才能实现。"
#~ msgstr ""

#~ msgid "本文将从一个训练好的异或（XOR）网络模型出发，讲解如何将其部署到 CPU（X86）环境下运行。"
#~ msgstr ""

#~ msgid "将模型序列化并导出"
#~ msgstr ""

#~ msgid "首先我们需要 :ref:`dump` , 用到的模型定义与序列化代码在 :src:`sdk/xor-deploy/xornet.py` ."
#~ msgstr ""

#~ msgid "编写 C++ 程序读取模型"
#~ msgstr ""

#~ msgid "接下来我们需要编写一个 C++ 程序，来实现我们期望在部署平台上完成的功能。"
#~ msgstr ""

#~ msgid ""
#~ "继续以上面导出的异或网络模型为例子，我们实现一个最简单的功能—— 给定两个浮点数，输出对其做异或操作后结果为 0"
#~ " 的概率（以及为 1 的概率）。"
#~ msgstr ""

#~ msgid ""
#~ "在此之前，为了能够正常使用 MegEngine 底层 C++ 接口， 需要先按照"
#~ " MegeEngine 中提供的编译脚本( :src:`scripts/` ) "
#~ "从源码编译得到 MegEngine 的相关库, "
#~ "通过这些脚本可以交叉编译安卓（ARMv7，ARMv8，ARMv8.2）版本、Linux "
#~ "版本（ARMv7，ARMv8，ARMv8.2）以及 iOS 相关库， 也可以本机编译 "
#~ "Windows/Linux/MacOS 相关库文件。"
#~ msgstr ""

#~ msgid "实现上述异或计算的示例 C++ 代码如下（引自 ``xor-deploy.cpp`` ）："
#~ msgstr ""

#~ msgid "简单解释一下代码的意思："
#~ msgstr ""

#~ msgid "我们首先通过 ``GraphLoader`` 将模型加载进来，"
#~ msgstr ""

#~ msgid "接着通过 ``tensor_map`` 和上节指定的输入名称 ``data`` ，找到模型的输入指针，"
#~ msgstr ""

#~ msgid "再将运行时提供的输入 ``x`` 和 ``y`` 赋值给输入指针，"
#~ msgstr ""

#~ msgid "然后我们使用 ``network.graph->compile`` 将模型编译成一个函数接口，并调用执行，"
#~ msgstr ""

#~ msgid "最后将得到的结果 ``predict`` 进行输出，该输出的两个值即为异或结果为 0 的概率以及为 1 的概率。"
#~ msgstr ""

#~ msgid ""
#~ "另外可以配置上面加载模型时候的 ``config`` 来优化 Inference 计算效率，"
#~ " 为了加速一般在 ARM 上面配置 ``enable_nchw44_layout()`` "
#~ ", 在x86 CPU 上面配置 ``enable_nchw88_layout()`` "
#~ "，具体的配置方法参考 :ref:`load-and-run` ."
#~ msgstr ""

#~ msgid "编译与执行"
#~ msgstr ""

#~ msgid ""
#~ "为了更完整地实现 “训练推理一体化”，我们还需要支持同一个 C++ "
#~ "程序能够交叉编译到不同平台上执行，而不需要修改代码。 之所以能够实现不同平台一套代码，是由于底层依赖的算子库（内部称作 "
#~ "MegDNN）实现了对不同平台接口的封装， 在编译时会自动根据指定的目标平台选择兼容的接口。"
#~ msgstr ""

#~ msgid ""
#~ "目前发布的版本我们开放了对 CPU（X86、X64、ARMv7、ARMv8、ARMv8.2） 和 "
#~ "GPU（CUDA）平台的 float 和量化 int8 的支持。"
#~ msgstr ""

#~ msgid "我们在这里以 CPU（X86）平台为例，首先直接使用 gcc 或者 clang （用 ``$CXX`` 指代）进行编译即可："
#~ msgstr ""

#~ msgid ""
#~ "上面的 ``$MGE_INSTALL_PATH`` 指代了编译安装时通过 "
#~ "``CMAKE_INSTALL_PREFIX`` 指定的安装路径。 编译完成之后，通过以下命令执行即可："
#~ msgstr ""

#~ msgid ""
#~ "这里将 ``$MGE_INSTALL_PATH/lib`` 加进 ``LD_LIBRARY_PATH``"
#~ " 环境变量，确保 MegEngine 库可以被编译器找到。 上面命令对应的输出如下："
#~ msgstr ""

#~ msgid ""
#~ "至此我们便完成了从 Python 模型到 C++ 可执行文件的部署流程， "
#~ "如果需要快速的运行模型以及测试模型性能，请参考 :ref:`load-and-run` ."
#~ msgstr ""

