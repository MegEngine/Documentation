msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-15 19:44+0800\n"
"PO-Revision-Date: 2021-06-03 10:19\n"
"Last-Translator: \n"
"Language-Team: Chinese Simplified\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-CN\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/deployment/index.po\n"
"X-Crowdin-File-ID: 2862\n"
"Language: zh_CN\n"

#: ../../source/user-guide/deployment/index.rst:5
msgid "将模型部署到 C++ 环境"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:7
msgid "MegEngine 的一大核心优势是 “训练推理一体化”，其中 “训练” 是在 Python 环境中进行的， 而 “推理” 则特指在 C++ 环境下使用训练完成的模型进行推理。 而将模型迁移到无需依赖 Python 的环境中，使其能正常进行推理计算，被称为 **部署** 。 部署的目的是简化除了模型推理所必需的一切其它依赖，使推理计算的耗时变得尽可能少， 比如手机人脸识别场景下会需求毫秒级的优化，而这必须依赖于 C++ 环境才能实现。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:13
msgid "本文将从一个训练好的异或（XOR）网络模型出发，讲解如何将其部署到 CPU（X86）环境下运行。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:16
msgid "将模型序列化并导出"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:18
msgid "首先我们需要 :ref:`dump` , 用到的模型定义与序列化代码在 :src:`sdk/xor-deploy/xornet.py` ."
msgstr ""

#: ../../source/user-guide/deployment/index.rst:21
msgid "编写 C++ 程序读取模型"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:23
msgid "接下来我们需要编写一个 C++ 程序，来实现我们期望在部署平台上完成的功能。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:25
msgid "继续以上面导出的异或网络模型为例子，我们实现一个最简单的功能—— 给定两个浮点数，输出对其做异或操作后结果为 0 的概率（以及为 1 的概率）。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:28
msgid "在此之前，为了能够正常使用 MegEngine 底层 C++ 接口， 需要先按照 MegeEngine 中提供的编译脚本( :src:`scripts/` ) 从源码编译得到 MegEngine 的相关库, 通过这些脚本可以交叉编译安卓（ARMv7，ARMv8，ARMv8.2）版本、Linux 版本（ARMv7，ARMv8，ARMv8.2）以及 iOS 相关库， 也可以本机编译 Windows/Linux/MacOS 相关库文件。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:33
msgid "实现上述异或计算的示例 C++ 代码如下（引自 ``xor-deploy.cpp`` ）："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:78
msgid "简单解释一下代码的意思："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:80
msgid "我们首先通过 ``GraphLoader`` 将模型加载进来，"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:81
msgid "接着通过 ``tensor_map`` 和上节指定的输入名称 ``data`` ，找到模型的输入指针，"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:82
msgid "再将运行时提供的输入 ``x`` 和 ``y`` 赋值给输入指针，"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:83
msgid "然后我们使用 ``network.graph->compile`` 将模型编译成一个函数接口，并调用执行，"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:84
msgid "最后将得到的结果 ``predict`` 进行输出，该输出的两个值即为异或结果为 0 的概率以及为 1 的概率。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:86
msgid "另外可以配置上面加载模型时候的 ``config`` 来优化 Inference 计算效率， 为了加速一般在 ARM 上面配置 ``enable_nchw44_layout()`` , 在x86 CPU 上面配置 ``enable_nchw88_layout()`` ，具体的配置方法参考 :ref:`load-and-run` ."
msgstr ""

#: ../../source/user-guide/deployment/index.rst:91
msgid "编译与执行"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:93
msgid "为了更完整地实现 “训练推理一体化”，我们还需要支持同一个 C++ 程序能够交叉编译到不同平台上执行，而不需要修改代码。 之所以能够实现不同平台一套代码，是由于底层依赖的算子库（内部称作 MegDNN）实现了对不同平台接口的封装， 在编译时会自动根据指定的目标平台选择兼容的接口。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:99
msgid "目前发布的版本我们开放了对 CPU（X86、X64、ARMv7、ARMv8、ARMv8.2） 和 GPU（CUDA）平台的 float 和量化 int8 的支持。"
msgstr ""

#: ../../source/user-guide/deployment/index.rst:102
msgid "我们在这里以 CPU（X86）平台为例，首先直接使用 gcc 或者 clang （用 ``$CXX`` 指代）进行编译即可："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:108
msgid "上面的 ``$MGE_INSTALL_PATH`` 指代了编译安装时通过 ``CMAKE_INSTALL_PREFIX`` 指定的安装路径。 编译完成之后，通过以下命令执行即可："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:115
msgid "这里将 ``$MGE_INSTALL_PATH/lib`` 加进 ``LD_LIBRARY_PATH`` 环境变量，确保 MegEngine 库可以被编译器找到。 上面命令对应的输出如下："
msgstr ""

#: ../../source/user-guide/deployment/index.rst:122
msgid "至此我们便完成了从 Python 模型到 C++ 可执行文件的部署流程， 如果需要快速的运行模型以及测试模型性能，请参考 :ref:`load-and-run` ."
msgstr ""

