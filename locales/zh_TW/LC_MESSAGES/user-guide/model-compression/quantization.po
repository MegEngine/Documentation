msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-23 21:44+0800\n"
"PO-Revision-Date: 2021-04-23 16:44\n"
"Last-Translator: \n"
"Language: zh_TW\n"
"Language-Team: Chinese Traditional\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-TW\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/en/LC_MESSAGES/user-guide/model-compression/quantization.po\n"
"X-Crowdin-File-ID: 2860\n"

#: ../../source/user-guide/model-compression/quantization.rst:5
msgid "量化"
msgstr "crwdns45796:0crwdne45796:0"

#: ../../source/user-guide/model-compression/quantization.rst:7
msgid "量化指的是将浮点数模型（一般是 32 位浮点数）的权重或激活值用位数更少的数值类型 （比如 8 位整数、16 位浮点数）来近似表示的过程。 量化后的模型会占用更小的存储空间，还能够利用许多硬件平台上的专属算子进行提速。 比如在 MegEngine 中使用 8 位整数来进行量化，相比默认的 32 位浮点数， 模型大小可以减少为 1/4，而运行在特定的设备上其计算速度也能提升为 2-4 倍。"
msgstr "crwdns45798:0crwdne45798:0"

#: ../../source/user-guide/model-compression/quantization.rst:13
msgid "量化的目的是为了追求极致的推理计算速度，为此舍弃了数值表示的精度，直觉上会带来较大的模型掉点， 但是在使用一系列精细的量化处理之后，其掉点可以变得微乎其微，并能支持正常的部署使用。 而且近年来随着专用神经网络加速芯片的兴起，低比特非浮点的运算方式越来越普及， 因此如何把一个 GPU 上训练的浮点数模型转化为低比特的量化模型，就成为了工业界非常关心的话题。"
msgstr "crwdns45800:0crwdne45800:0"

#: ../../source/user-guide/model-compression/quantization.rst:18
msgid "一般来说，得到量化模型的转换过程按代价从低到高可以分为以下 4 种："
msgstr "crwdns45802:0crwdne45802:0"

#: ../../source/user-guide/model-compression/quantization.rst:23
msgid "Type1 和 Type2 由于是在模型浮点模型训练之后介入，无需大量训练数据， 故而转换代价更低，被称为后量化（Post Quantization）；"
msgstr "crwdns45804:0crwdne45804:0"

#: ../../source/user-guide/model-compression/quantization.rst:25
msgid "Type3 和 Type4 则需要在浮点模型训练时就插入一些假量化（FakeQuantize）算子， 模拟计算过程中数值截断后精度降低的情形，故而称为量化感知训练（Quantization Aware Training, QAT）。"
msgstr "crwdns45806:0crwdne45806:0"

#: ../../source/user-guide/model-compression/quantization.rst:28
msgid "本文主要介绍 Type2 和 Type3 在 MegEngine 中的完整流程。 事实上，除了 Type2 无需进行假量化，两者的整体流程完全一致。"
msgstr "crwdns45808:0crwdne45808:0"

#: ../../source/user-guide/model-compression/quantization.rst:32
msgid "整体流程"
msgstr "crwdns45810:0crwdne45810:0"

#: ../../source/user-guide/model-compression/quantization.rst:34
msgid "以 Type3 为例，一般以一个训练完毕的浮点模型为起点，称为 Float 模型。 包含假量化算子的用浮点操作来模拟量化过程的新模型，我们称之为 Quantized-Float 模型，或者 QFloat 模型。 可以直接在终端设备上运行的模型，称之为 Quantized 模型，简称 Q 模型。"
msgstr "crwdns45812:0crwdne45812:0"

#: ../../source/user-guide/model-compression/quantization.rst:38
msgid "而三者的精度一般是 ``Float > QFloat > Q`` ，故而一般量化算法也就分为两步："
msgstr "crwdns45814:0crwdne45814:0"

#: ../../source/user-guide/model-compression/quantization.rst:40
msgid "拉近 QFloat 和 Q，这样训练阶段的精度可以作为最终 Q 精度的代理指标，这一阶段偏工程；"
msgstr "crwdns45816:0crwdne45816:0"

#: ../../source/user-guide/model-compression/quantization.rst:41
msgid "拔高 QFloat 逼近 Float，这样就可以将量化模型性能尽可能恢复到 Float 的精度，这一阶段偏算法。"
msgstr "crwdns45818:0crwdne45818:0"

#: ../../source/user-guide/model-compression/quantization.rst:43
msgid "典型的三种模型在三个阶段的精度变化如下："
msgstr "crwdns45820:0crwdne45820:0"

#: ../../source/user-guide/model-compression/quantization.rst:48
msgid "对应到具体的 MegEngine 接口中，三阶段如下："
msgstr "crwdns45822:0crwdne45822:0"

#: ../../source/user-guide/model-compression/quantization.rst:50
msgid "基于 :class:`~.module.Module` 搭建网络模型，并按照正常的浮点模型方式进行训练；"
msgstr "crwdns45824:0:class:crwdne45824:0"

#: ../../source/user-guide/model-compression/quantization.rst:51
msgid "使用 :func:`~.quantization.quantize_qat` 将浮点模型转换为 QFloat 模型， 其中可被量化的关键 Module 会被转换为 :class:`~.module.qat.QATModule` ， 并基于量化配置 :class:`~.quantization.QConfig` 设置好假量化算子和数值统计方式；"
msgstr "crwdns45826:0:func:crwdnd45826:0:class:crwdnd45826:0:class:crwdne45826:0"

#: ../../source/user-guide/model-compression/quantization.rst:54
msgid "使用 :func:`~.quantization.quantize` 将 QFloat 模型转换为 Q 模型， 对应的 QATModule 则会被转换为 :class:`~.module.quantized.QuantizedModule` ， 此时网络无法再进行训练，网络中的算子都会转换为低比特计算方式，即可用于部署了。"
msgstr "crwdns45828:0:func:crwdnd45828:0:class:crwdne45828:0"

#: ../../source/user-guide/model-compression/quantization.rst:58
msgid "该流程是 Type3 对应 QAT 的步骤，Type2 对应的后量化则需使用不同 QConfig， 且需使用 evaluation 模式运行 QFloat 模型，而非训练模式。更多细节可以继续阅读下一节详细的接口介绍。"
msgstr "crwdns45830:0crwdne45830:0"

#: ../../source/user-guide/model-compression/quantization.rst:62
msgid "接口介绍"
msgstr "crwdns45832:0crwdne45832:0"

#: ../../source/user-guide/model-compression/quantization.rst:64
msgid "在 MegEngine 中，最上层的接口是配置如何量化的 :class:`~.quantization.QConfig` 和模型转换模块里的 :func:`~.quantization.quantize_qat` 与 :func:`~.quantization.quantize` 。"
msgstr "crwdns45834:0:class:crwdnd45834:0:func:crwdnd45834:0:func:crwdne45834:0"

#: ../../source/user-guide/model-compression/quantization.rst:68
msgid "QConfig"
msgstr "crwdns45836:0crwdne45836:0"

#: ../../source/user-guide/model-compression/quantization.rst:70
msgid "QConfig 包括了 :class:`~.quantization.Observer` 和 :class:`~.quantization.FakeQuantize` 两部分。 我们知道，对模型转换为低比特量化模型一般分为两步： 一是统计待量化模型中参数和 activation 的数值范围（scale）和零点（zero_point）， 二是根据 scale 和 zero_point 将模型转换成指定的数值类型。而为了统计这两个值，我们需要使用 Observer."
msgstr "crwdns45838:0:class:crwdnd45838:0:class:crwdne45838:0"

#: ../../source/user-guide/model-compression/quantization.rst:75
msgid "Observer 继承自 :class:`~.module.Module` ，也会参与网络的前向传播， 但是其 forward 的返回值就是输入，所以不会影响网络的反向梯度传播。 其作用就是在前向时拿到输入的值，并统计其数值范围，并通过 :meth:`~.quantization.Observer.get_qparams` 来获取。 所以在搭建网络时把需要统计数值范围的的 Tensor 作为 Observer 的输入即可。"
msgstr "crwdns45840:0:class:crwdnd45840:0:meth:crwdne45840:0"

#: ../../source/user-guide/model-compression/quantization.rst:92
msgid "另外如果只观察而不模拟量化会导致模型掉点，于是我们需要有 FakeQuantize 来根据 Observer 观察到的数值范围模拟量化时的截断，使得参数在训练时就能提前“适应“这种操作。 FakeQuantize 在前向时会根据传入的 scale 和 zero_point 对输入 Tensor 做模拟量化的操作， 即先做一遍数值转换再转换后的值还原成原类型，如下所示："
msgstr "crwdns45842:0crwdne45842:0"

#: ../../source/user-guide/model-compression/quantization.rst:112
msgid "目前 MegEngine 支持对 weight/activation 两部分的量化，如下所示："
msgstr "crwdns45844:0crwdne45844:0"

#: ../../source/user-guide/model-compression/quantization.rst:123
msgid "这里使用了两种 Observer 来统计信息，而 FakeQuantize 使用了默认的算子。"
msgstr "crwdns45846:0crwdne45846:0"

#: ../../source/user-guide/model-compression/quantization.rst:125
msgid "如果是后量化，或者说 Calibration，由于无需进行 FakeQuantize，故而其 fake_quant 属性为 None 即可："
msgstr "crwdns45848:0crwdne45848:0"

#: ../../source/user-guide/model-compression/quantization.rst:136
msgid "除了使用在 :class:`~.quantization.Qconfig` 里提供的预设 QConfig， 也可以根据需要灵活选择 Observer 和 FakeQuantize  实现自己的 QConfig。目前提供的 Observer 包括："
msgstr "crwdns47798:0:class:crwdne47798:0"

#: ../../source/user-guide/model-compression/quantization.rst:139
msgid ":class:`~.quantization.MinMaxObserver` ， 使用最简单的算法统计 min/max，对见到的每批数据取 min/max 跟当前存的值比较并替换， 基于 min/max 得到 scale 和 zero_point；"
msgstr "crwdns47800:0:class:crwdne47800:0"

#: ../../source/user-guide/model-compression/quantization.rst:142
msgid ":class:`~.quantization.ExponentialMovingAverageObserver` ， 引入动量的概念，对每批数据的 min/max 与现有 min/max 的加权和跟现有值比较；"
msgstr "crwdns47802:0:class:crwdne47802:0"

#: ../../source/user-guide/model-compression/quantization.rst:144
msgid ":class:`~.quantization.HistogramObserver` ， 更加复杂的基于直方图分布的 min/max 统计算法，且在 forward 时持续更新该分布， 并根据该分布计算得到 scale 和 zero_point。"
msgstr "crwdns47804:0:class:crwdne47804:0"

#: ../../source/user-guide/model-compression/quantization.rst:148
msgid "对于 FakeQuantize，目前还提供了 :class:`~.quantization.TQT` 算子， 另外还可以继承 ``_FakeQuant`` 基类实现自定义的假量化算子。"
msgstr "crwdns47806:0:class:crwdne47806:0"

#: ../../source/user-guide/model-compression/quantization.rst:151
msgid "在实际使用过程中，可能需要在训练时让 Observer 统计并更新参数，但是在推理时则停止更新。 Observer 和 FakeQuantize 都支持 :meth:`~.quantization.Observer.enable` 和 :meth:`~.quantization.Observer.disable` 功能， 且 Observer 会在 :meth:`~module.Module.train` 和 :meth:`~module.Module.eval` 时自动分别调用 enable/disable。"
msgstr "crwdns47808:0:meth:crwdnd47808:0:meth:crwdnd47808:0:meth:crwdnd47808:0:meth:crwdne47808:0"

#: ../../source/user-guide/model-compression/quantization.rst:157
msgid "所以一般在 Calibration 时，会先执行 ``net.eval()`` 保证网络的参数不被更新， 然后再执行 :``enable_observer(net)`` 来手动开启 Observer 的统计修改功能。"
msgstr "crwdns45862:0crwdne45862:0"

#: ../../source/user-guide/model-compression/quantization.rst:161
msgid "模型转换模块与相关基类"
msgstr "crwdns45864:0crwdne45864:0"

#: ../../source/user-guide/model-compression/quantization.rst:163
msgid "QConfig 提供了一系列如何对模型做量化的接口，而要使用这些接口， 需要网络的 Module 能够在 forward 时给参数、activation 加上 Observer 和进行 FakeQuantize. 转换模块的作用就是将模型中的普通 Module 替换为支持这一系列操作的 :class:`~.module.qat.QATModule` ， 并能支持进一步替换成无法训练、专用于部署的 :class:`~.module.quantized.QuantizedModule` 。"
msgstr "crwdns45866:0:class:crwdnd45866:0:class:crwdne45866:0"

#: ../../source/user-guide/model-compression/quantization.rst:168
msgid "基于三种基类实现的 Module 是一一对应的关系，通过转换接口可以依次替换为不同实现的同名 Module。 同时考虑到量化与算子融合（Fuse）的高度关联，我们提供了一系列预先融合好的 Module， 比如 :class:`~.module.ConvRelu2d` 、 :class:`~.module.ConvBn2d` 和 :class:`~.module.ConvBnRelu2d` 等。 除此之外还提供专用于量化的 :class:`~.module.QuantStub` 、 :class:`~.module.DequantStub` 等辅助模块。"
msgstr "crwdns45868:0:class:crwdnd45868:0:class:crwdnd45868:0:class:crwdnd45868:0:class:crwdnd45868:0:class:crwdne45868:0"

#: ../../source/user-guide/model-compression/quantization.rst:173
msgid "转换的原理很简单，就是将父 Module 中可被量化（Quantable）的子 Module 替换为对应的新 Module. 但是有一些 Quantable Module 还包含 Quantable 子 Module，比如 ConvBn 就包含一个 Conv2d 和一个 BatchNorm2d， 转换过程并不会对这些子 Module 进一步转换，原因是父 Module 被替换之后， 其 forward 计算过程已经完全不同了，不会再依赖于这些子 Module。"
msgstr "crwdns45870:0crwdne45870:0"

#: ../../source/user-guide/model-compression/quantization.rst:180
msgid "如果需要使一部分 Module 及其子 Module 保留 Float 状态，不进行转换， 可以使用 :meth:`~.module.Module.disable_quantize` 来处理。"
msgstr "crwdns45872:0:meth:crwdne45872:0"

#: ../../source/user-guide/model-compression/quantization.rst:183
msgid "如果网络结构中涉及一些二元及以上的 ElementWise 操作符，比如加法乘法等， 由于多个输入各自的 scale 并不一致，必须使用量化专用的算子，并指定好输出的 scale. 实际使用中只需要把这些操作替换为 :class:`~.module.Elemwise` 即可， 比如 ``self.add_relu = Elemwise(\"FUSE_ADD_RELU\")``"
msgstr "crwdns45874:0:class:crwdne45874:0"

#: ../../source/user-guide/model-compression/quantization.rst:188
msgid "另外由于转换过程修改了原网络结构，模型保存与加载无法直接适用于转换后的网络， 读取新网络保存的参数时，需要先调用转换接口得到转换后的网络，才能用 load_state_dict 将参数进行加载。"
msgstr "crwdns45876:0crwdne45876:0"

#: ../../source/user-guide/model-compression/quantization.rst:192
msgid "实例讲解"
msgstr "crwdns45878:0crwdne45878:0"

#: ../../source/user-guide/model-compression/quantization.rst:194
msgid "下面我们以 ResNet18 为例来讲解量化的完整流程，完整代码见 ``MegEngine/Models`` . 主要分为以下几步："
msgstr "crwdns45880:0crwdne45880:0"

#: ../../source/user-guide/model-compression/quantization.rst:196
msgid "修改网络结构，使用已经 Fuse 好的 ConvBn2d、ConvBnRelu2d、ElementWise 代替原先的 Module；"
msgstr "crwdns45882:0crwdne45882:0"

#: ../../source/user-guide/model-compression/quantization.rst:197
msgid "在正常模式下预训练模型，并在每轮迭代保存网络检查点；"
msgstr "crwdns45884:0crwdne45884:0"

#: ../../source/user-guide/model-compression/quantization.rst:198
msgid "调用 :func:`~.quantization.quantize_qat` 转换模型，并进行 finetune；"
msgstr "crwdns45886:0:func:crwdne45886:0"

#: ../../source/user-guide/model-compression/quantization.rst:199
msgid "调用 :func:`~.quantization.quantize` 转换为量化模型，并执行 dump 用于后续模型部署。"
msgstr "crwdns45888:0:func:crwdne45888:0"

#: ../../source/user-guide/model-compression/quantization.rst:201
msgid "网络结构见 ``resnet.py`` ，相比惯常写法，我们修改了其中一些子 Module， 将原先单独的 ``conv``, ``bn``, ``relu`` 替换为 Fuse 过的 Quantable Module。"
msgstr "crwdns45890:0crwdne45890:0"

#: ../../source/user-guide/model-compression/quantization.rst:229
msgid "然后对该模型进行若干轮迭代训练，并保存检查点，这里省略细节："
msgstr "crwdns45892:0crwdne45892:0"

#: ../../source/user-guide/model-compression/quantization.rst:248
msgid "再调用 :func:`~.quantization.quantize_qat` 来将网络转换为 QATModule："
msgstr "crwdns45894:0:func:crwdne45894:0"

#: ../../source/user-guide/model-compression/quantization.rst:259
msgid "这里使用默认的 ``ema_fakequant_qconfig`` 来进行 ``int8`` 量化。"
msgstr "crwdns45896:0crwdne45896:0"

#: ../../source/user-guide/model-compression/quantization.rst:261
msgid "然后我们继续使用上面相同的代码进行 finetune 训练。 值得注意的是，如果这两步全在一次程序运行中执行，那么训练的 trace 函数需要用不一样的， 因为模型的参数变化了，需要重新进行编译。 示例代码中则是采用在新的执行中读取检查点重新编译的方法。"
msgstr "crwdns45898:0crwdne45898:0"

#: ../../source/user-guide/model-compression/quantization.rst:266
msgid "在 QAT 模式训练完成后，我们继续保存检查点，执行 ``inference.py`` 并设置 ``mode`` 为 ``quantized`` ， 这里需要将原始 Float 模型转换为 QAT 模型之后再加载检查点。"
msgstr "crwdns45900:0crwdne45900:0"

#: ../../source/user-guide/model-compression/quantization.rst:281
msgid "模型转换为量化模型包括以下几步："
msgstr "crwdns45902:0crwdne45902:0"

#: ../../source/user-guide/model-compression/quantization.rst:312
msgid "至此便得到了一个可用于部署的量化模型。"
msgstr "crwdns45904:0crwdne45904:0"

