msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-21 10:52+0800\n"
"PO-Revision-Date: 2023-04-21 09:34\n"
"Last-Translator: \n"
"Language: zh_TW\n"
"Language-Team: Chinese Traditional\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-TW\n"
"X-Crowdin-File: /dev/locales/en/LC_MESSAGES/getting-started/beginner/learning-from-linear-regression.po\n"
"X-Crowdin-File-ID: 9847\n"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:9
msgid "一个稍微复杂些的线性回归模型"
msgstr "crwdns112049:0crwdne112049:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:12
msgid "|image0| `在 MegStudio 运行 <https://studio.brainpp.com/project/4642>`__"
msgstr "crwdns112051:0crwdne112051:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:16
msgid "image0"
msgstr "crwdns112053:0crwdne112053:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:12
msgid "|image1| `查看源文件 <https://github.com/MegEngine/Documentation/blob/main/source/getting-started/beginner/learning-from-linear-regression.ipynb>`__"
msgstr "crwdns112055:0crwdne112055:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:17
msgid "image1"
msgstr "crwdns112057:0crwdne112057:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:28
msgid "我们已经学习了如何去训练一个简单的线性回归模型 :math:`y = w * x + b`, 接下来我们将稍微升级一下难度："
msgstr "crwdns112059:0:math:crwdne112059:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:30
msgid "我们对单个样本数据的表示形式将不再是简单的标量 :math:`x`, 而是升级为多维的向量 :math:`\\mathbf {x}` 表示；"
msgstr "crwdns112061:0:math:crwdnd112061:0:math:crwdnd112061:0{x}crwdne112061:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:31
msgid "我们将接触到更多不同形式的梯度下降策略，从而接触一个新的超参数 ``batch_size``;"
msgstr "crwdns112063:0crwdne112063:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:32
msgid "我们将尝试将数据集封装成 MegEngine 支持的 ``Dataset`` 类，方便在 ``Dataloader`` 中进行各种预处理操作；"
msgstr "crwdns112065:0crwdne112065:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:33
msgid "同时，我们的前向计算过程将变成矩阵运算形式，能够帮助你更好地理解向量化实现的优点；"
msgstr "crwdns112067:0crwdne112067:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:34
msgid "在这个过程中，一些遗留问题会得到解答，同时我们将接触到一些新的机器学习概念。"
msgstr "crwdns112069:0crwdne112069:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:36
msgid "本教程的话题将围绕着\\ **“数据（Data）”**\\ 进行，希望你能够有所收获～"
msgstr "crwdns112071:0crwdne112071:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:38
msgid "请先运行下面的代码，检验你的环境中是否已经安装好 MegEngine（\\ `安装教程 <https://megengine.org.cn/doc/stable/zh/user-guide/install/>`__\\ ）："
msgstr "crwdns112073:0crwdne112073:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:80
msgid "接下来，我们将对将要使用的数据集 `波士顿房价数据集 <https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html>`__ 进行一定的了解。"
msgstr "crwdns112075:0crwdne112075:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:92
msgid "波士顿房价数据集的获取和分析"
msgstr "crwdns112077:0crwdne112077:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:94
msgid "获取到真实的数据集原始文件后，往往需要做大量的数据格式处理工作，变成易于计算机理解的形式，才能被各种框架和库使用。"
msgstr "crwdns112079:0crwdne112079:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:96
msgid "但目前我们的重心不在此处，使用 Python 机器学习库 `scikit-learn <https://scikit-learn.org/stable/index.html>`__\\ ，可以快速地获得波士顿房价数据集:"
msgstr "crwdns112081:0crwdne112081:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:151
msgid "我们已经认识过了数据 ``data`` 和标签 ``label``\\ （标签有时也叫目标 ``target`` ），那么 ``feature`` 是什么？"
msgstr "crwdns112083:0crwdne112083:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:153
msgid "我们在描述一个事物的时候，通常会寻找其属性（Attribute）或者说特征（Feature）："
msgstr "crwdns112085:0crwdne112085:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:155
msgid "比如我们描述一只柴犬的长相，会说这只柴的鼻子如何、耳朵如何、毛发如何等等"
msgstr "crwdns112087:0crwdne112087:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:156
msgid "又比如在游戏宠物的属性经常有生命值、魔法值、攻击力、防御力等属性"
msgstr "crwdns112089:0crwdne112089:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:157
msgid "类比到一些支持自定义角色的游戏，这些特征就变成许多可量化和调整的数据"
msgstr "crwdns112091:0crwdne112091:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:159
msgid "实际上，你并不需要在意波士顿房价数据集中选用了哪些特征，我们在本教程中关注的更多是“特征维度变多”这一情况。"
msgstr "crwdns112093:0crwdne112093:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:161
msgid "为了方便后续的交流，我们需要引入一些数学符号来描述它们，不用害怕，理解起来非常简单："
msgstr "crwdns112095:0crwdne112095:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:163
msgid "波士顿房价数据集的样本容量为 506，记为 :math:`n`; （Number）"
msgstr "crwdns112097:0:math:crwdne112097:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:164
msgid "单个的样本可以用一个向量 :math:`\\mathbf {x}=(x_{1}, x_{2}, \\ldots , x_{d})` 来表示，称为“特征向量”，里面的每个元素对应着该样本的某一维特征；"
msgstr "crwdns112099:0:math:crwdnd112099:0{x}crwdnd112099:0{1}crwdnd112099:0{2}crwdnd112099:0{d}crwdne112099:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:165
msgid "波士顿房价数据集的特征向量有 13 个维度，包括住宅平均房间数、城镇师生比例等等，记为 :math:`d`. （Dimensionality）"
msgstr "crwdns112101:0:math:crwdne112101:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:166
msgid "数据集 ``data`` 由 :math:`n` 行特征向量组成，每个特征向量有 :math:`d` 维度特征，因此整个数据集可以用一个形状为 :math:`(n, d)` 的数据矩阵 :math:`X` 来表示；"
msgstr "crwdns112103:0:math:crwdnd112103:0:math:crwdnd112103:0:math:crwdnd112103:0:math:crwdne112103:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:167
msgid "标签 ``label`` 的每个元素是一个标量值，记录着房价值，因此它本身是一个含有 :math:`n` 个元素的标签向量 :math:`\\mathbf {y}`."
msgstr "crwdns112105:0:math:crwdnd112105:0:math:crwdnd112105:0{y}crwdne112105:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:169
msgid "X = \\begin{bmatrix}\n"
"- \\mathbf {x}_1 - \\\\\n"
"- \\mathbf {x}_2 - \\\\\n"
"\\vdots \\\\\n"
"- \\mathbf {x}_n -\n"
"\\end{bmatrix}\n"
"= \\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots  &         &        & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\quad\n"
"\\mathbf {y} =  (y_{1}, y_{2}, \\ldots, y_{n})"
msgstr "crwdns112107:0{bmatrix}crwdnd112107:0{x}crwdnd112107:0{x}crwdnd112107:0{x}crwdnd112107:0{bmatrix}crwdnd112107:0{bmatrix}crwdnd112107:0{1,1}crwdnd112107:0{1,2}crwdnd112107:0{1,d}crwdnd112107:0{2,1}crwdnd112107:0{2,2}crwdnd112107:0{2,d}crwdnd112107:0{n,1}crwdnd112107:0{n,2}crwdnd112107:0{n,d}crwdnd112107:0{bmatrix}crwdnd112107:0{y}crwdnd112107:0{1}crwdnd112107:0{2}crwdnd112107:0{n}crwdne112107:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:187
msgid "其中 :math:`x_{i,j}` 表示第 :math:`i` 个样本 :math:`\\mathbf {x}_i` 的第 :math:`j` 维特征，\\ :math:`y_i` 为样本 :math:`\\mathbf {x}_i` 对应的标签。"
msgstr "crwdns112109:0:math:crwdnd112109:0{i,j}crwdnd112109:0:math:crwdnd112109:0:math:crwdnd112109:0{x}crwdnd112109:0:math:crwdnd112109:0:math:crwdnd112109:0:math:crwdnd112109:0{x}crwdne112109:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:189
msgid "**不同的人会对数学符号的使用有着不同的约定，一定需要搞清楚这些符号的定义，或者在交流时采用比较通用的定义方式**"
msgstr "crwdns112111:0crwdne112111:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:190
msgid "为了简洁美观，我们这里用粗体表示向量 :math:`\\mathbf {x}`; 而在手写时粗细体并不容易辨别，所以板书时常用上标箭头来表示向量 :math:`\\vec {x}`."
msgstr "crwdns112113:0:math:crwdnd112113:0{x}crwdnd112113:0:math:crwdnd112113:0{x}crwdne112113:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:236
msgid "数据集的几种类型"
msgstr "crwdns112115:0crwdne112115:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:238
msgid "在之前的线性回归模型教程中，我们在最后通过可视化效果对模型的拟合程度进行了判断，但这并不是评估模型性能通常的做法。"
msgstr "crwdns112117:0crwdne112117:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:240
msgid "对于常见的机器学习任务，用作基准（Benchmark）的数据集通常会被分为训练集（Training dataset）和测试集（Test dataset）两部分；"
msgstr "crwdns112119:0crwdne112119:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:242
msgid "我们利用训练集的数据，通过优化损失函数的形式将模型训练好，在测试集上对模型进行测试，根据选用的评估指标（Metrics）评价模型性能"
msgstr "crwdns112121:0crwdne112121:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:243
msgid "测试集的数据不能参与模型训练，因为我们希望测试集数据代表着将来会被用于预测的真实数据，它们现在是“不可见的”，当作标签未知"
msgstr "crwdns112123:0crwdne112123:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:245
msgid "训练模型时，通常还会在训练集的基础上划分出验证集（Validation dataset）或者说开发集（Develop dataset），方便进行控制和调试："
msgstr "crwdns112125:0crwdne112125:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:247
msgid "在一些机器学习竞赛中，比赛方会将测试集中一部分拿出来做为 Public Leaderboard 评分和排名，剩下的部分作为 Private Leaderboard 的评分和排名。"
msgstr "crwdns112127:0crwdne112127:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:248
msgid "选手可以根据 Public Leaderboard 上的评估结果及时地对算法和超参数进行调整优化，但最终的排名将以 Private Leaderboard 为准"
msgstr "crwdns112129:0crwdne112129:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:249
msgid "对应地，可以将 Public Leaderboard 所使用的数据集理解为验证集，将 Private Leaderboard 所使用的数据集理解为测试集"
msgstr "crwdns112131:0crwdne112131:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:250
msgid "为了避免在短时间内引入过多密集的新知识，我们目前将不会进行验证集（开发集）的代码实践和讨论"
msgstr "crwdns112133:0crwdne112133:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:252
msgid "数据集该如何划分是一个值得讨论与研究的话题，它是数据预处理的一个环节，目前我们只需要对它有一个基本概念，通过不断实践来加深理解。"
msgstr "crwdns112135:0crwdne112135:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:254
msgid "对于波士顿房价数据集，我们可以将 506 张样本划分为 500 张训练样本，6 张测试样本："
msgstr "crwdns112137:0crwdne112137:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:305
msgid "模型定义、训练和测试"
msgstr "crwdns112139:0crwdne112139:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:307
msgid "对于单个的样本 :math:`\\mathbf {x}`, 想要预测输出 :math:`y`, 即尝试找到映射关系 :math:`f: \\mathbf {x} \\in \\mathbb {R}^{d} \\mapsto y`, 同样可以建立线性模型："
msgstr "crwdns112141:0:math:crwdnd112141:0{x}crwdnd112141:0:math:crwdnd112141:0:math:crwdnd112141:0{x}crwdnd112141:0{R}crwdnd112141:0{d}crwdne112141:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:309
msgid "\\begin{aligned}\n"
"y &= f(\\mathbf {x}) = \\mathbf {w} \\cdot \\mathbf {x} + b \\\\\n"
"& = (w_{1}, w_{2}, \\ldots, w_{13}) \\cdot (x_{1}, x_{2}, \\ldots, x_{13}) + b\\\\\n"
"& = w_{1} x_{1} + w_{2} x_{2} + \\ldots + w_{13} x_{13} + b\n"
"\\end{aligned}"
msgstr "crwdns112143:0{aligned}crwdnd112143:0{x}crwdnd112143:0{w}crwdnd112143:0{x}crwdnd112143:0{1}crwdnd112143:0{2}crwdnd112143:0{13}crwdnd112143:0{1}crwdnd112143:0{2}crwdnd112143:0{13}crwdnd112143:0{1}crwdnd112143:0{1}crwdnd112143:0{2}crwdnd112143:0{2}crwdnd112143:0{13}crwdnd112143:0{13}crwdnd112143:0{aligned}crwdne112143:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:318
msgid "**注意在表示不同的乘积（Product）时，不仅使用的符号有所不同，数学概念和编程代码之间又有所区别** ，以 NumPy 为例："
msgstr "crwdns112145:0crwdne112145:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:320
msgid "上个教程中用的星乘 :math:`*` 符号指的是元素间相乘（有时用 :math:`\\odot` 表示），也适用于标量乘法，编程对应于 ``np.multiply()`` 方法"
msgstr "crwdns112147:0:math:crwdnd112147:0:math:crwdne112147:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:321
msgid "对于点乘我们用 :math:`\\cdot` 符号表示，编程对应于 ``np.dot()`` 方法，在作用于两个向量时它等同于内积 ``np.inner()`` 方法"
msgstr "crwdns112149:0:math:crwdne112149:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:322
msgid "根据传入参数的形状，\\ ``np.dot()`` 的行为也可以等同于矩阵乘法，对应于 ``numpy.matmul()`` 方法或 ``@`` 运算符"
msgstr "crwdns112151:0crwdne112151:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:323
msgid "严谨地说，点积或者数量积只是内积的一种特例，\\ **但我们在编程时，应该习惯先查询文档中的说明，并进行简单验证**"
msgstr "crwdns112153:0crwdne112153:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:325
msgid "比如样本 :math:`\\mathbf {x}` 和参数 :math:`\\mathbf {w}` 都是 13 维的向量，二者的点积结果是一个标量（ ``shape`` 为空元组）："
msgstr "crwdns112155:0:math:crwdnd112155:0{x}crwdnd112155:0:math:crwdnd112155:0{w}crwdne112155:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:372
msgid "我们之前已经实现过类似的训练过程，现在一起来看下面这份新实现的代码："
msgstr "crwdns112157:0crwdne112157:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:456
msgid "模型评估（Model Metric）"
msgstr "crwdns112159:0crwdne112159:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:458
msgid "现在，我们可以使用训练得到的 ``w`` 和 ``b`` 在测试数据上进行评估，一些时候损失函数可以被直接用于评估，比如训练时使用的 MSE."
msgstr "crwdns112161:0crwdne112161:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:460
msgid "但进行评估时，使用的评估指标可以不同，这里我们可以选择使用平均绝对误差（Mean Absolute Error, MAE）作为评估指标："
msgstr "crwdns112163:0crwdne112163:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:462
msgid "\\ell(y_{pred}, y_{real})= \\frac{1}{n }\\sum_{i=1}^{n}\\left | \\hat{y}_{i}-{y}_{i}\\right |"
msgstr "crwdns112165:0{pred}crwdnd112165:0{real}crwdnd112165:0{1}crwdnd112165:0{n }crwdnd112165:0{n}crwdnd112165:0{y}crwdnd112165:0{i}crwdnd112165:0{y}crwdnd112165:0{i}crwdne112165:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:531
msgid "不同的梯度下降形式"
msgstr "crwdns112167:0crwdne112167:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:533
msgid "我们可以发现：上面的代码中，我们每训练一个完整的 ``epoch``, 将根据所有样本的平均梯度进行一次参数更新。"
msgstr "crwdns112169:0crwdne112169:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:535
msgid "这种通过对所有的样本的计算来求解梯度的方法叫做 **批梯度下降法(Batch Gradient Descent)**."
msgstr "crwdns112171:0crwdne112171:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:536
msgid "当碰到样本容量特别大的情况时，可能会导致无法一次性将所有数据给读入内存，遇到内存用尽（Out of memory，OOM）的情况。"
msgstr "crwdns112173:0crwdne112173:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:538
msgid "这时你可能会想：“其实我们完全可以在每个样本经过前向传播计算损失、反向传播计算得到梯度后时，就立即对参数进行更新呀！”"
msgstr "crwdns112175:0crwdne112175:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:540
msgid "Bingo~ 这种思路叫做 **随机梯度下降（Stochastic Gradient Descent，常缩写成 SGD)** ，动手改写后，整体代码实现如下："
msgstr "crwdns112177:0crwdne112177:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:615
msgid "可以看到，在同样的训练周期内，使用随机梯度下降得到的训练损失更低，即损失收敛得更快。这是因为参数的实际更新次数要多得多。"
msgstr "crwdns112179:0crwdne112179:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:617
msgid "接下来我们用随机梯度下降得到的参数 ``w`` 和 ``b`` 进行测试："
msgstr "crwdns112181:0crwdne112181:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:682
msgid "可以看到，虽然我们在训练集上的损失远低于使用批梯度下降的损失，但测试时得到的损失反而略高于批梯度下降的测试结果。为什么会这样？"
msgstr "crwdns112183:0crwdne112183:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:684
msgid "**抛开数据、模型、损失函数和评估指标本身的因素不谈，背后的原因可能是：**"
msgstr "crwdns112185:0crwdne112185:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:686
msgid "使用随机梯度下降时，考虑到噪声数据的存在，并不一定参数每次更新都是朝着模型整体最优化的方向；"
msgstr "crwdns112187:0crwdne112187:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:687
msgid "若样本噪声较多，很容易陷入局部最优解而收敛到不理想的状态；"
msgstr "crwdns112189:0crwdne112189:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:688
msgid "如果更新次数过多，还容易出现在训练数据过拟合的情况，导致泛化能力变差。"
msgstr "crwdns112191:0crwdne112191:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:690
msgid "既然两种梯度下降策略各有千秋，因此有一种折衷的方式，即采用\\ **小批量梯度下降法（Mini-Batch Gradient Descent）**\\ ："
msgstr "crwdns112193:0crwdne112193:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:692
msgid "我们设定一个 ``batch_size`` 值，将数据划分为多个 ``batch``\\ ，每个 ``batch`` 的数据采取批梯度下降策略来更新参数；"
msgstr "crwdns112195:0crwdne112195:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:693
msgid "设置合适的 ``batch_size`` 值，既可以避免出现内存爆掉的情况，也使得损失可能更加平滑地收敛；"
msgstr "crwdns112197:0crwdne112197:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:694
msgid "不难发现 ``batch_size`` 是一个超参数；当 ``batch_size=1`` 时，小批量梯度下降其实就等同于随机梯度下降"
msgstr "crwdns112199:0crwdne112199:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:696
msgid "注意：MegEngine 的优化器 ``Optimizer`` 中实现的 ``SGD`` 优化策略，实际上就是对小批量梯度下降法逻辑的实现。"
msgstr "crwdns112201:0crwdne112201:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:698
msgid "这种折衷方案的效果比“真”随机梯度下降要好得多，\\ **因为可以利用向量化加速批数据的运算，而不是分别计算每个样本。**"
msgstr "crwdns112203:0crwdne112203:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:700
msgid "在这里我们先卖个关子，把向量化的介绍放在更后面，因为我们的当务之急是：获取小批量数据（Mini-batch data）."
msgstr "crwdns112205:0crwdne112205:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:712
msgid "采样器（Sampler）"
msgstr "crwdns112207:0crwdne112207:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:714
msgid "想要从完整的数据集中获得小批量的数据，则需要对已有数据进行采样。"
msgstr "crwdns112209:0crwdne112209:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:716
msgid "在 MegEngine 的 ``data`` 模块中，提供了多种采样器 ``Sampler``\\ ："
msgstr "crwdns112211:0crwdne112211:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:718
msgid "对于训练数据，通常使用顺序采样器 ``SequentialSampler``, 我们即将用到；"
msgstr "crwdns112213:0crwdne112213:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:719
msgid "对于测试数据，通常使用 ``RandomSampler`` 进行随机采样，在本教程的测试部分就能见到；"
msgstr "crwdns112215:0crwdne112215:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:720
msgid "对于 ``dataset`` 的样本容量不是 ``batch_size`` 整数倍的情况，采样器也能进行很好的处理"
msgstr "crwdns112217:0crwdne112217:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:774
msgid "但需要注意到，采样器 ``Sampler`` 返回的是索引值，要得到划分后的批数据，还需要结合使用 MegEngine 中的 ``Dataloader`` 模块。"
msgstr "crwdns112219:0crwdne112219:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:776
msgid "同时也要注意，如果想要使用 ``Dataloader``, 需要先将原始数据集变成 MegEngine 支持的 ``Dataset`` 对象。"
msgstr "crwdns112221:0crwdne112221:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:788
msgid "利用 Dataset 封装一个数据集"
msgstr "crwdns112223:0crwdne112223:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:790
msgid "``Dataset`` 是 MegEngine 中表示数据集最原始的的抽象类，可被其它类继承（如 ``StreamDataset``\\ ），可阅读 ``Dataset`` 模块的文档了解更多的细节。"
msgstr "crwdns112225:0crwdne112225:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:792
msgid "通常我们自定义的数据集类应该继承 ``Dataset`` 类，并重写下列方法："
msgstr "crwdns112227:0crwdne112227:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:794
msgid "``__init__()`` ：一般在其中实现读取数据源文件的功能。也可以添加任何其它的必要功能；"
msgstr "crwdns112229:0__init__crwdne112229:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:795
msgid "``__getitem__()`` ：通过索引操作来获取数据集中某一个样本，使得可以通过 for 循环来遍历整个数据集；"
msgstr "crwdns112231:0__getitem__crwdne112231:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:796
msgid "``__len__()`` ：返回数据集大小"
msgstr "crwdns112233:0__len__crwdne112233:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:851
msgid "其实，对于这种单个或多个 NumPy 数组构成的数据，在 MegEngine 中也可以使用 ``ArrayDataset`` 对它进行初始化，它将自动完成以上方法的重写："
msgstr "crwdns112235:0crwdne112235:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:895
msgid "数据载入器（Dataloader）"
msgstr "crwdns112237:0crwdne112237:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:897
msgid "接下来便可以通过 ``Dataloader`` 来生成批数据，每个元素由对应的 ``batch_data`` 和 ``batch_label`` 组成："
msgstr "crwdns112239:0crwdne112239:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:944
msgid "接下来我们一起来看看，使用批数据为什么能够加速整体的运算效率。"
msgstr "crwdns112241:0crwdne112241:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:947
msgid "通过向量化加速运算"
msgstr "crwdns112243:0crwdne112243:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:949
msgid "在 NumPy 内部，向量化运算的速度是优于 ``for`` 循环的，我们很容易验证这一点："
msgstr "crwdns112245:0crwdne112245:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1007
msgid "重新阅读模型训练的代码，不难发现，每个 ``epoch`` 内部存在着 ``for`` 循环，根据模型定义的 :math:`y_i = \\mathbf {w} \\cdot \\mathbf {x_i} + b` 进行了 :math:`n` 次计算。"
msgstr "crwdns112247:0:math:crwdnd112247:0{w}crwdnd112247:0{x_i}crwdnd112247:0:math:crwdne112247:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1009
msgid "我们在前面已经将数据集表示成了形状为 :math:`(n, d)` 的数据矩阵 :math:`X`, 将标签表示成了 :math:`y` 向量，这启发我们一次性完成所有样本的前向计算过程："
msgstr "crwdns112249:0:math:crwdnd112249:0:math:crwdnd112249:0:math:crwdne112249:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1011
msgid "(y_{1}, y_{2}, \\ldots, y_{n}) =\n"
"\\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots  &         &        & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\cdot (w_{1}, w_{2}, \\ldots, w_{d}) +\n"
"b"
msgstr "crwdns112251:0{1}crwdnd112251:0{2}crwdnd112251:0{n}crwdnd112251:0{bmatrix}crwdnd112251:0{1,1}crwdnd112251:0{1,2}crwdnd112251:0{1,d}crwdnd112251:0{2,1}crwdnd112251:0{2,2}crwdnd112251:0{2,d}crwdnd112251:0{n,1}crwdnd112251:0{n,2}crwdnd112251:0{n,d}crwdnd112251:0{bmatrix}crwdnd112251:0{1}crwdnd112251:0{2}crwdnd112251:0{d}crwdne112251:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1024
msgid "一种比较容易理解的形式是将其看成是矩阵运算 :math:`Y=XW+B`\\ ："
msgstr "crwdns112253:0:math:crwdne112253:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1026
msgid "\\begin{bmatrix}\n"
"y_{1} \\\\\n"
"y_{2} \\\\\n"
"\\vdots \\\\\n"
"y_{n}\n"
"\\end{bmatrix} =\n"
"\\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,d} \\\\\n"
"\\vdots  &         &        & \\vdots \\\\\n"
"x_{n,1} & x_{n,2} & \\cdots & x_{n,d}\n"
"\\end{bmatrix}\n"
"\\begin{bmatrix}\n"
"w_1 \\\\\n"
"w_2 \\\\\n"
"\\vdots \\\\\n"
"w_d\n"
"\\end{bmatrix} +\n"
"\\begin{bmatrix}\n"
"b \\\\\n"
"b \\\\\n"
"\\vdots \\\\\n"
"b\n"
"\\end{bmatrix}"
msgstr "crwdns112255:0{bmatrix}crwdnd112255:0{1}crwdnd112255:0{2}crwdnd112255:0{n}crwdnd112255:0{bmatrix}crwdnd112255:0{bmatrix}crwdnd112255:0{1,1}crwdnd112255:0{1,2}crwdnd112255:0{1,d}crwdnd112255:0{2,1}crwdnd112255:0{2,2}crwdnd112255:0{2,d}crwdnd112255:0{n,1}crwdnd112255:0{n,2}crwdnd112255:0{n,d}crwdnd112255:0{bmatrix}crwdnd112255:0{bmatrix}crwdnd112255:0{bmatrix}crwdnd112255:0{bmatrix}crwdnd112255:0{bmatrix}crwdne112255:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1054
msgid "形状为 :math:`(n,d)` 的矩阵 :math:`X` 和维度为 :math:`(d,)` 的向量 :math:`w` 进行点乘，此时不妨理解成 :math:`w` 变成了一个形状为 :math:`(d, 1)` 的矩阵 :math:`W`"
msgstr "crwdns112257:0:math:crwdnd112257:0:math:crwdnd112257:0:math:crwdnd112257:0:math:crwdnd112257:0:math:crwdnd112257:0:math:crwdnd112257:0:math:crwdne112257:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1055
msgid "两个矩阵进行乘法运算，此时的 ``np.dot(X, w)`` 等效于 ``np.matmul(X, W)``, 底层效率比 ``for`` 循环快许多，得到形状为 :math:`(n, 1)` 的中间矩阵 :math:`P`"
msgstr "crwdns112259:0:math:crwdnd112259:0:math:crwdne112259:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1056
msgid "中间矩阵 :math:`P` 和标量 :math:`b` 相加，此时标量 :math:`b` 广播成 :math:`(n, 1)` 的矩阵 :math:`B` 进行矩阵加法，得到形状为 :math:`(n,1)` 的标签矩阵 :math:`Y`"
msgstr "crwdns112261:0:math:crwdnd112261:0:math:crwdnd112261:0:math:crwdnd112261:0:math:crwdnd112261:0:math:crwdnd112261:0:math:crwdnd112261:0:math:crwdne112261:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1057
msgid "显然，这样的处理逻辑还需要将标签矩阵 :math:`Y` 去掉冗余的那一维，变成形状为 :math:`(n, )` 的标签向量 :math:`y`"
msgstr "crwdns112263:0:math:crwdnd112263:0:math:crwdnd112263:0:math:crwdne112263:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1059
msgid "**矩阵/张量运算的思路在深度学习中十分常见，在后续会被反复提及。**"
msgstr "crwdns112265:0crwdne112265:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1061
msgid "NumPy 的\\ `官方文档 <https://numpy.org/doc/stable/reference/generated/numpy.dot.html>`__\\ 中写着此时 ``np.dot()`` 的真实逻辑："
msgstr "crwdns112267:0crwdne112267:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1063
msgid "If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b."
msgstr "crwdns112269:0crwdne112269:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1064
msgid "如果 ``a`` 是 N 维度数组且 ``b`` 是 1 维数组，将对 ``a`` 和 ``b`` 最后一轴上对元素进行乘积并求和。"
msgstr "crwdns112271:0crwdne112271:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1066
msgid "这样的计算逻辑也可以由爱因斯坦求和约定 ``np.einsum('ij,j->i', X, w)`` 来实现，感兴趣的读者可查阅 ``np.einsum()`` 文档。"
msgstr "crwdns112273:0crwdne112273:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1068
msgid "我们可以设计一套简单的测试样例，来看一下 ``np.dot()``, ``np.einsum()`` 和 ``np.matmul()`` 相较于 ``for`` 循环是否能得到同样的结果："
msgstr "crwdns112275:0crwdne112275:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1126
msgid "接下来加大数据规模，测试向量化是否起到了加速效果："
msgstr "crwdns112277:0crwdne112277:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1202
msgid "可以发现 ``dot()`` 和 ``matmul()`` 的向量化实现都有明显的加速效果，\\ ``einsum()`` 虽然是全能型选手，但也以更多的开销作为了代价，一般不做考虑。"
msgstr "crwdns112279:0crwdne112279:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1205
msgid "NumPy 实现"
msgstr "crwdns112281:0crwdne112281:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1207
msgid "我们先尝试使用 NumPy 的 ``dot()`` 实现一下向量化的批梯度下降的代码："
msgstr "crwdns112283:0crwdne112283:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1276
msgid "上面的 ``loss`` 收敛状况与我们最开始实现的 ``sum_grad`` 的数值一致，可以自行测试一下 ``for`` 循环写法和向量化写法的用时差异。"
msgstr "crwdns112285:0crwdne112285:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1278
msgid "同时我们也可以发现，当前向传播使用批数据变成矩阵计算带来极大加速的同时，也为我们的反向传播梯度计算提出了更高的要求："
msgstr "crwdns112287:0crwdne112287:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1280
msgid "尽管存在着链式法则，但是对于不熟悉矩阵求导的人来说，还是很难理解一些梯度公式推导的过程，比如为什么出现了转置 ``data.T``"
msgstr "crwdns112289:0crwdne112289:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1281
msgid "当前向传播的计算变得更加复杂，由框架实现自动求导 ``autograde`` 机制就显得更加必要和方便"
msgstr "crwdns112291:0crwdne112291:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1284
msgid "MegEngine 实现"
msgstr "crwdns112293:0crwdne112293:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1286
msgid "最后，让我们利用上小批量（Mini-batch）的数据把完整的训练流程代码在 MegEngine 中实现："
msgstr "crwdns112295:0crwdne112295:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1288
msgid "为了后续教程理解的连贯性，我们做一些改变，使用 ``F.matmul()`` 来代替 ``np.dot()``\\ ，前面已经证明了这种情况下的计算结果值是等价的，但形状不同"
msgstr "crwdns112297:0crwdne112297:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1289
msgid "对应地，我们的 ``w`` 将由向量变为矩阵，\\ ``b`` 在进行加法操作时将自动进行广播变成矩阵，输出 ``pred`` 也是一个矩阵，即 2 维 Tensor"
msgstr "crwdns112299:0crwdne112299:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1290
msgid "在计算单个 ``batch`` 的 ``loss`` 时，内部通过 ``sum()`` 计算已经去掉了冗余的维度，最终得到的 ``loss`` 是一个 0 维 Tensor"
msgstr "crwdns112301:0crwdne112301:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1387
msgid "对我们训练好的模型进行测试："
msgstr "crwdns112303:0crwdne112303:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1450
msgid "可以发现，使用小批量梯度下降策略更新参数，最终在测试时得到的平均绝对值损失比单独采用批梯度下降和随机梯度下降策略时还要好一些。"
msgstr "crwdns112305:0crwdne112305:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1462
msgid "总结回顾"
msgstr "crwdns112307:0crwdne112307:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1464
msgid "有些时候，添加一些额外的条件，看似简单的问题就变得更加复杂有趣起来了，启发我们进行更多的思考和探索。"
msgstr "crwdns112309:0crwdne112309:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1466
msgid "我们需要及时总结学习过的知识，并在之后的时间内不断通过实践来强化记忆，这次的教程中出现了以下新的概念："
msgstr "crwdns112311:0crwdne112311:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1468
msgid "数据集（Dataset）：想要将现实中的数据集变成能够被 MegEngine 使用的格式，必须将其处理成 ``MapDataset`` 或者 ``StreamDateset`` 类"
msgstr "crwdns112313:0crwdne112313:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1469
msgid "继承基类的同时，还需要实现 ``__init__()``, ``__getitem__`` 和 ``__len__()`` 三种内置方法"
msgstr "crwdns112315:0__init__crwdnd112315:0__getitem__crwdnd112315:0__len__crwdne112315:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1470
msgid "我们对事物的抽象——特征（Feature）有了一定的认识，并接触了一些数学形式的表达如数据矩阵 :math:`X` 和标签向量 :math:`y` 等"
msgstr "crwdns112317:0:math:crwdnd112317:0:math:crwdne112317:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1471
msgid "我们接触到了训练集（Trainining dataset）、测试集（Test dataset）和验证集（Validation dataset）/ 开发集（Develop dataset）的概念"
msgstr "crwdns112319:0crwdne112319:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1472
msgid "除了训练时我们会设计一个损失函数外，在测试模型性能时，我们也需要设计一个评估指标（Metric），不同任务的指标不尽相同"
msgstr "crwdns112321:0crwdne112321:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1473
msgid "采样器（Sampler）：可以帮助我们自动地获得一个采样序列，常见的有顺序采样器 ``SequentialSampler`` 和随机采样器 ``RandomSampler``"
msgstr "crwdns112323:0crwdne112323:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1474
msgid "数据载入器（Dataloader）：根据 ``Dataset`` 和 ``Sampler`` 来获得对应的小批量数据（Mini-batch data）"
msgstr "crwdns112325:0crwdne112325:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1475
msgid "我们认识了一个新的超参数 ``batch_size``\\ ，并且对不同形式的梯度下降各自的优缺点有了一定的认识"
msgstr "crwdns112327:0crwdne112327:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1476
msgid "我们理解了批数据向量化计算带来的好处，能够大大加快计算效率；同时为了避免推导向量化的梯度，实现自动求导机制是相当必要的"
msgstr "crwdns112329:0crwdne112329:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1478
msgid "我们要养成及时查阅文档的好习惯，比如 NumPy 的 ``np.dot()`` 在接受的 ndarray 形状不同时，运算逻辑也将发生变化"
msgstr "crwdns112331:0crwdne112331:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1480
msgid "在写代码时，Tensor 的维度形状变化是尤其需要关注的地方，理清楚逻辑可以减少遇到相关报错的可能"
msgstr "crwdns112333:0crwdne112333:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1481
msgid "不同库和框架之间，相同 API 命名背后的实现可能完全不同"
msgstr "crwdns112335:0crwdne112335:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1493
msgid "问题思考"
msgstr "crwdns112337:0crwdne112337:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1495
msgid "旧问题的解决往往伴随着新问题的诞生，让我们一起来思考一下："
msgstr "crwdns112339:0crwdne112339:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1497
msgid "我们在标记和收集数据的时候，特征的选取是否科学，数据集的规模应该要有多大？数据集的划分又应该选用什么样的比例？"
msgstr "crwdns112341:0crwdne112341:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1498
msgid "这次见着了 ``batch_size``, 它的设定是否有经验可循？接触到的超参数越来越多了，验证集和开发集是如何帮助我们对合适的超参数进行选取的？"
msgstr "crwdns112343:0crwdne112343:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1499
msgid "我们对 NumPy 支持的 ndarray 的数据如何导入 MegEngine 已经有了经验，更复杂的数据集（图片、视频、音频）要如何处理呢？"
msgstr "crwdns112345:0crwdne112345:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1500
msgid "噪声数据会对参数的学习进行干扰，\\ ``Dataloader`` 中是否有对应的预处理方法来做一些统计意义上的处理？"
msgstr "crwdns112347:0crwdne112347:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1501
msgid "我们知道了向量化可以加速运算，在硬件底层是如何实现“加速”这一效果的？"
msgstr "crwdns112349:0crwdne112349:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1504
msgid "关于数学表示的习惯"
msgstr "crwdns112351:0crwdne112351:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1506
msgid "由于数据矩阵 :math:`X` 的形状排布为 :math:`(n, d)`, 因此 MegEngine 中实现的矩阵形式的线性回归为 :math:`\\hat {Y} = f(X) = XW+B` （这里将 :math:`B` 视为 :math:`b` 广播后的矩阵）"
msgstr "crwdns112353:0:math:crwdnd112353:0:math:crwdnd112353:0:math:crwdnd112353:0{Y}crwdnd112353:0:math:crwdnd112353:0:math:crwdne112353:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1508
msgid "然而在线性代数中为了方便运算表示，向量通常默认为列向量 :math:`\\mathbf {x} = (x_1; x_2; \\ldots x_n)`, 单样本线性回归为 :math:`f(\\mathbf {x};\\mathbf {w},b) = \\mathbf {w}^T\\mathbf {x} + b`\\ ，"
msgstr "crwdns112355:0:math:crwdnd112355:0{x}crwdnd112355:0:math:crwdnd112355:0{x}crwdnd112355:0{w}crwdnd112355:0{w}crwdnd112355:0{x}crwdne112355:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1510
msgid "对应有形状为 :math:`(d, n)` 的数据矩阵："
msgstr "crwdns112357:0:math:crwdne112357:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1512
msgid "X = \\begin{bmatrix}\n"
"| & | & & | \\\\\n"
"\\mathbf {x}_1 & \\mathbf {x}_2 & \\cdots & \\mathbf {x}_n  \\\\\n"
"| & | & & |\n"
"\\end{bmatrix}\n"
"= \\begin{bmatrix}\n"
"x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n"
"x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n"
"\\vdots  &         &        & \\vdots \\\\\n"
"x_{d,1} & x_{d,2} & \\cdots & x_{d,n}\n"
"\\end{bmatrix}"
msgstr "crwdns112359:0{bmatrix}crwdnd112359:0{x}crwdnd112359:0{x}crwdnd112359:0{x}crwdnd112359:0{bmatrix}crwdnd112359:0{bmatrix}crwdnd112359:0{1,1}crwdnd112359:0{1,2}crwdnd112359:0{1,n}crwdnd112359:0{2,1}crwdnd112359:0{2,2}crwdnd112359:0{2,n}crwdnd112359:0{d,1}crwdnd112359:0{d,2}crwdnd112359:0{d,n}crwdnd112359:0{bmatrix}crwdne112359:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1527
msgid "因此可以得到 :math:`\\hat {Y} = f(X) = W^{T}X+B`, 这反而是比较常见的形式（类似于 :math:`y=Ax+b`\\ ）。"
msgstr "crwdns112361:0:math:crwdnd112361:0{Y}crwdnd112361:0{T}crwdnd112361:0:math:crwdne112361:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1529
msgid "为什么我们在教程中要使用 :math:`\\hat {Y} = f(X) = XW+B` 这种不常见的表述形式呢？数据布局不同会有什么影响？"
msgstr "crwdns112363:0:math:crwdnd112363:0{Y}crwdne112363:0"

#: ../../source/getting-started/beginner/learning-from-linear-regression.ipynb:1531
msgid "深度学习，简单开发。我们鼓励你在实践中不断思考，并启发自己去探索直觉性或理论性的解释。"
msgstr "crwdns112365:0crwdne112365:0"

#~ msgid ""
#~ "|image0| `在官网查看 <https://megengine.org.cn/doc/stable/zh"
#~ "/getting-started/beginner/learning-from-linear-"
#~ "regression.html>`__"
#~ msgstr ""

#~ msgid "|image1| `在 MegStudio 运行 <https://studio.brainpp.com/project/4642>`__"
#~ msgstr ""

#~ msgid ""
#~ "|image2| `在 GitHub 查看 "
#~ "<https://github.com/MegEngine/Documentation/blob/main/source"
#~ "/getting-started/beginner/learning-from-linear-"
#~ "regression.ipynb>`__"
#~ msgstr ""

#~ msgid "NumPy 的官方文档中写着此时 ``np.dot()`` 的真实逻辑："
#~ msgstr ""
#~ "NumPy's official document states that "
#~ "the real logic of ``np.dot()'' at "
#~ "this time is："

