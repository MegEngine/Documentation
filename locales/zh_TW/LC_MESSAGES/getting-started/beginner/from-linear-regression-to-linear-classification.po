msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-12-28 09:14+0000\n"
"PO-Revision-Date: 2023-04-21 09:34\n"
"Last-Translator: \n"
"Language: zh_TW\n"
"Language-Team: Chinese Traditional\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-TW\n"
"X-Crowdin-File: /dev/locales/en/LC_MESSAGES/getting-started/beginner/from-linear-regression-to-linear-classification.po\n"
"X-Crowdin-File-ID: 9841\n"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:9
msgid "从线性回归到线性分类"
msgstr "crwdns111359:0crwdne111359:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:12
msgid "|image0| `在 MegStudio 运行 <https://studio.brainpp.com/project/4643>`__"
msgstr "crwdns111361:0crwdne111361:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:16
msgid "image0"
msgstr "crwdns111363:0crwdne111363:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:12
msgid "|image1| `查看源文件 <https://github.com/MegEngine/Documentation/blob/main/source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb>`__"
msgstr "crwdns111365:0crwdne111365:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:17
msgid "image1"
msgstr "crwdns111367:0crwdne111367:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:28
msgid "回归和分类问题在机器学习中十分常见，我们接触过了线性回归，那么线性模型是否可用于分类任务呢？本次教程中，我们将："
msgstr "crwdns111369:0crwdne111369:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:30
msgid "接触经典的 MNIST 数据集和对应的分类任务，对计算机视觉领域的图像编码有一个基础认知；"
msgstr "crwdns111371:0crwdne111371:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:31
msgid "将线性回归经过“简单改造”，则可以用来解决分类问题——我们将接触到 Logistic 回归和 Softmax 回归；"
msgstr "crwdns111373:0crwdne111373:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:32
msgid "结合之前的学习，\\ **实现一个最简单的线性分类器，并尝试用它来识别手写数字。**"
msgstr "crwdns111375:0crwdne111375:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:34
msgid "请先运行下面的代码，检验你的环境中是否已经安装好 MegEngine（\\ `安装教程 <https://megengine.org.cn/doc/stable/zh/user-guide/install/>`__\\ ）："
msgstr "crwdns111377:0crwdne111377:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:76
msgid "接下来，我们将先了解一下这次要使用到的分类问题经典数据集：\\ `MNIST 手写数字数据集 <http://yann.lecun.com/exdb/mnist/>`__\\ 。"
msgstr "crwdns111379:0crwdne111379:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:79
msgid "MNIST 手写数字数据集"
msgstr "crwdns111381:0crwdne111381:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:81
msgid "MNIST 的训练数据集中存在着 60000 张（测试集 10000 张）手写数字 0～9 的黑白图片样例，每张图片的长和宽均为 28 像素。"
msgstr "crwdns111383:0crwdne111383:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:83
msgid "在 MegEngine 的 ``dataset`` 模块中内置了 MNIST 等经典数据集的接口，方便初学者进行相关调用："
msgstr "crwdns111385:0crwdne111385:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:144
msgid "以训练集为例，你最终将得到一个长度为 60000 的 ``train_dataset`` 列表，其中的每个元素是一个包含样本和标签的元组："
msgstr "crwdns111387:0crwdne111387:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:190
msgid "为了方便理解，我们这里选择将数据集拆分为样本和标签，处理成 Numpy 的 ndarray 格式："
msgstr "crwdns111389:0crwdne111389:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:232
msgid "经过上面的整理，我们得到了训练数据 ``train_data`` 和对应的标签 ``train_label``:"
msgstr "crwdns111391:0crwdne111391:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:234
msgid "可以发现此时的训练数据的形状是 :math:`(60000, 28, 28, 1)`, 分别对应数据量（Number）、高度（Height）、宽度（Width）和通道数（Channel），简记为 NHWC；"
msgstr "crwdns111393:0:math:crwdne111393:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:235
msgid "其中通道（Channel）是图像领域常见的概念，对计算机而言，1 通道通常表示灰度图（Grayscale），即将黑色到白色之间分为 256 阶表示；"
msgstr "crwdns111395:0crwdne111395:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:236
msgid "布局（Layout）表示了数据在内存中的表示方式，在 MegEngine 中，通常以 NCHW 作为默认的数据布局，我们在将来会接触到布局的转换。"
msgstr "crwdns111397:0crwdne111397:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:248
msgid "理解黑白（灰度）图像数据"
msgstr "crwdns111399:0crwdne111399:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:250
msgid "我们先尝试对数据进行随机抽样，并进行可视化显示："
msgstr "crwdns111401:0crwdne111401:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:294
msgid "挑选出一张图片（下方的 ``idx`` 可修改），进行二维和三维视角的可视化："
msgstr "crwdns111403:0crwdne111403:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:348
msgid "在灰度图中，每个像素值用 0（黑色）~ 255（白色）进行表示，即一个 ``int8`` 所能表示的范围。"
msgstr "crwdns111405:0crwdne111405:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:351
msgid "图像数据的特征向量表示"
msgstr "crwdns111407:0crwdne111407:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:353
msgid "回想一下我们在使用波士顿房价数据集时，单个样本的特征通常以特征向量 :math:`\\mathbf {x} \\in \\mathbb R^D` 的形式进行表示，而图像的样本特征空间为 :math:`\\mathbf {x} \\in \\mathbb R ^{H \\times W \\times C}`\\ 。"
msgstr "crwdns111409:0:math:crwdnd111409:0{x}crwdnd111409:0:math:crwdnd111409:0{x}crwdne111409:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:355
msgid "对于图像类型的数据输入，在没有想到更好的特征抽取和表征形式时，不妨也粗线条一些，考虑直接将像素点“铺”成向量的形式，即 :math:`\\mathbb R ^{H \\times W \\times C} \\mapsto \\mathbb R^D`\\ ："
msgstr "crwdns111411:0:math:crwdne111411:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:396
msgid "上面是一个简单的例子，这种扁平化（Flatten）的操作同样也可以对整个 ``train_data`` 使用："
msgstr "crwdns111413:0crwdne111413:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:438
msgid "在 MegEngine 的 ``functional`` 模块中，提供了 ``flatten()`` 方法："
msgstr "crwdns111415:0crwdne111415:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:486
msgid "这样的话，就可以把问题和我们所了解的线性回归模型 :math:`f(\\mathbf {x}) = \\mathbf {w} \\cdot \\mathbf {x} + b` 联系起来了，但情况有那么一些不同："
msgstr "crwdns111417:0:math:crwdnd111417:0{x}crwdnd111417:0{w}crwdnd111417:0{x}crwdne111417:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:488
msgid "线性回归的输出值在 :math:`\\mathbb R` 上连续，并不能很好的处理标签值离散的多分类问题，因此需要寻找新的解决思路。"
msgstr "crwdns111419:0:math:crwdne111419:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:491
msgid "线性分类模型"
msgstr "crwdns111421:0crwdne111421:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:493
msgid "我们先从较为简单的二分类问题，即标签 :math:`y \\in \\{0, 1\\}` 情况开始讨论，再将情况推广到多分类。"
msgstr "crwdns111423:0:math:crwdne111423:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:495
msgid "对于离散的标签，我们可以引入非线性的决策函数 :math:`g(\\cdot)` 来预测输出类别（决策边界依然是线性超平面，依旧是线性模型）。"
msgstr "crwdns111425:0:math:crwdne111425:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:497
msgid "举个例子，可以将 :math:`f(\\mathbf {x}) = \\mathbf {x} \\cdot \\mathbf {w} + b` 的输出以 0 为阈值（threshold）做划分（即此时决策边界为 :math:`f(\\mathbf {x}) = 0`\\ ），决策函数为指示函数 :math:`\\mathbb{I}(\\cdot)`\\ ："
msgstr "crwdns111427:0:math:crwdnd111427:0{x}crwdnd111427:0{x}crwdnd111427:0{w}crwdnd111427:0:math:crwdnd111427:0{x}crwdnd111427:0:math:crwdnd111427:0{I}crwdne111427:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:499
msgid "\\hat {y} = \\mathbb{I}(f(\\mathbf {x}) - \\text{threshold})=\\left\\{\\begin{array}{lll}\n"
"1 & \\text {if} & f(\\mathbf {x})>\\text{threshold} \\\\\n"
"0 & \\text {if} & f(\\mathbf {x})<\\text{threshold}\n"
"\\end{array}\\right."
msgstr "crwdns111429:0{y}crwdnd111429:0{I}crwdnd111429:0{x}crwdnd111429:0{threshold}crwdnd111429:0{array}crwdnd111429:0{lll}crwdnd111429:0{if}crwdnd111429:0{x}crwdnd111429:0{threshold}crwdnd111429:0{if}crwdnd111429:0{x}crwdnd111429:0{threshold}crwdnd111429:0{array}crwdne111429:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:507
msgid "这种决策方法虽然简单直观，但缺点也很明显："
msgstr "crwdns111431:0crwdne111431:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:509
msgid "如果看成是一个优化问题，它的数学性质导致其不适合用于梯度下降算法；"
msgstr "crwdns111433:0crwdne111433:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:510
msgid "如果数据集不同的类别之间没有明确的关系，分段决策在多分类的情况下不适用；"
msgstr "crwdns111435:0crwdne111435:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:512
msgid "比如手写数字分类，将输出值平均到 0～9 附近，依据四舍五入分类，不同分类之间存在着“距离度量”；"
msgstr "crwdns111437:0crwdne111437:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:513
msgid "这暗示着不同的分类之间也有相似度/连续性，则等同于假设图像 1 和图像 2 的相似度会比 1 和 7 的相似度更高"
msgstr "crwdns111439:0crwdne111439:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:525
msgid "Logistic 回归"
msgstr "crwdns111441:0crwdne111441:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:527
msgid "Logistic 回归是一种常见的处理二分类问题的线性模型，使用 Sigmoid 函数 :math:`\\sigma (\\cdot)` 作为决策函数（其中 ``exp()`` 指以自然常数 :math:`e` 为底的指数函数）："
msgstr "crwdns111443:0:math:crwdnd111443:0:math:crwdne111443:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:529
msgid "\\sigma (x) = \\frac{1}{1+\\exp( -x)}"
msgstr "crwdns111445:0{1}crwdne111445:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:534
msgid "Sigmoid 这样的 S 型函数最早被人们设计出来并使用，它有如下优点："
msgstr "crwdns111447:0crwdne111447:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:536
msgid "Sigmoid 函数的导数非常容易求出：\\ :math:`\\sigma '(x) = \\sigma (x)(1- \\sigma (x))`, 其处处可微的性质保证了在优化过程中梯度的可计算性；"
msgstr "crwdns111449:0:math:crwdne111449:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:537
msgid "Sigmoid 函数还可以将值压缩到 :math:`(0, 1)` 范围内，很适合用来表示预测结果是某个分类的概率："
msgstr "crwdns111451:0:math:crwdne111451:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:572
msgid "历史小知识：Logistic 是比利时数学家 Pierre François Verhulst 在 1844 或 1845 年在研究人口增长的关系时命名的，和 Sigmoid 一样指代 S 形函数：起初阶段大致是指数增长；然后随着开始变得饱和，增加变慢最后，达到成熟时增加停止。以下是 Verhulst 对命名的解释："
msgstr "crwdns111453:0crwdne111453:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:574
msgid "“We will give the name logistic [logistique] to the curve” (1845 p.8). Though he does not explain this choice, there is a connection with the logarithmic basis of the function. Logarithm was coined by John Napier (1550-1617) from Greek logos (ratio, proportion, reckoning) and arithmos (number). Logistic comes from the Greek logistikos (computational). In the 1700’s, logarithmic and logistic were synonymous. Since computation is needed to predict the supplies an army requires, logistics has come to be also used for the movement and supply of troops. So it appears the other meaning of “logistics” comes from the same logic as Verhulst terminology, but is independent (?). Verhulst paper is accessible; the definition is on page 8 (page 21 in the volume), and the picture is after the article (page 54 in the volume). ” —— `Why logistic (sigmoid) ogive and not autocatalytic curve? <https://rasch.org/rmt/rmt64k.htm>`__"
msgstr "crwdns111455:0[logistique]crwdne111455:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:577
msgid "在 MegEngine 的 ``functional`` 模块中，提供了 ``sigmoid()`` 方法："
msgstr "crwdns111457:0crwdne111457:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:623
msgid "根据二分类问题 :math:`y \\in \\{0, 1\\}` 的特性，标签值不是 :math:`1` 即是 :math:`0`\\ ，可以使用如下形式表示预测分类为 :math:`1` 或 :math:`0` 的概率："
msgstr "crwdns111459:0:math:crwdnd111459:0:math:crwdnd111459:0:math:crwdnd111459:0:math:crwdnd111459:0:math:crwdne111459:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:625
msgid "\\begin{aligned}\n"
"p(y=1 \\mid \\mathbf {x}) &= \\sigma (f(\\mathbf {x})) = \\frac{1}{1+\\exp \\left( -f(\\mathbf {x}) \\right)} \\\\\n"
"p(y=0 \\mid \\mathbf {x}) &= 1 - p(y=1 \\mid \\mathbf {x})\n"
"\\end{aligned}"
msgstr "crwdns111461:0{aligned}crwdnd111461:0{x}crwdnd111461:0{x}crwdnd111461:0{1}crwdnd111461:0{x}crwdnd111461:0{x}crwdnd111461:0{x}crwdnd111461:0{aligned}crwdne111461:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:633
msgid "在处理二分类问题时，我们进行优化的最终目的是，希望最终预测的概率值 :math:`\\hat{y}=\\sigma(f(\\mathbf x))` 尽可能地接近真实标签 :math:`y`;"
msgstr "crwdns111463:0:math:crwdnd111463:0{y}crwdnd111463:0:math:crwdne111463:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:635
msgid "为了能够正确地优化我们的任务目标，需要选用合适的损失（目标）函数。有了线性回归的经验，不妨用均方误差 MSE 试一下："
msgstr "crwdns111465:0crwdne111465:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:637
msgid "\\begin{aligned}\n"
"\\ell_{\\operatorname{MSE}} &= \\frac {1}{2} \\sum_{n} \\left( \\hat{y} - y \\right )^2 \\\\\n"
"\\frac {\\partial (\\hat{y} - y)^2}{\\partial \\mathbf {w}} &=\n"
"2 (\\hat{y} - y) \\cdot \\frac {\\partial \\hat{y}}{\\partial f(\\mathbf {x})}\n"
"\\cdot \\frac {\\partial f(\\mathbf {x})}{\\partial w} \\\\\n"
"&= 2 (\\hat{y} - y) \\cdot \\color{blue} {\\hat{y} \\cdot (1 - \\hat{y})} \\cdot \\color{black} {\\mathbf {x}}\n"
"\\end{aligned}"
msgstr "crwdns111467:0{aligned}crwdnd111467:0{MSE}crwdnd111467:0{1}crwdnd111467:0{2}crwdnd111467:0{n}crwdnd111467:0{y}crwdnd111467:0{y}crwdnd111467:0{w}crwdnd111467:0{y}crwdnd111467:0{y}crwdnd111467:0{x}crwdnd111467:0{x}crwdnd111467:0{y}crwdnd111467:0{blue}crwdnd111467:0{y}crwdnd111467:0{y}crwdnd111467:0{black}crwdnd111467:0{x}crwdnd111467:0{aligned}crwdne111467:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:648
msgid "虽然可以求得相应的梯度，但随着参数的更新，单样本上求得的梯度越来越接近 :math:`0`, 即出现了梯度消失（Gradient Vanishing）的情况；"
msgstr "crwdns111469:0:math:crwdne111469:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:649
msgid "另外使用 Sigmoid + MSE 得到的是一个非凸函数（可通过求二阶导证明），使用梯度下降算法容易收敛到局部最小值点，而非全局最优点。"
msgstr "crwdns111471:0crwdne111471:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:651
msgid "所以应该设计什么样的损失函数，才是比较合理的呢？在揭晓答案之前，我们先直接将二分类问题推广到多分类的情况。"
msgstr "crwdns111473:0crwdne111473:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:663
msgid "Softmax 回归"
msgstr "crwdns111475:0crwdne111475:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:665
msgid "Softmax 回归可以看成是 Logistic 回归在多分类问题上的推广，也称为多项（Multinomial）Logistic 回归，或多类（Muti-Class）Logistic 回归。"
msgstr "crwdns111477:0crwdne111477:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:667
msgid "对于多分类问题，类别标签 :math:`y \\in \\{1, 2, \\ldots, C \\}` 可以有 :math:`C` 个取值 —— 前面提到，如何将我们的输出和这几个值较好地对应起来，是需要解决的难题之一。"
msgstr "crwdns111479:0:math:crwdnd111479:0:math:crwdne111479:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:669
msgid "在处理二分类问题时，我们使用 Sigmoid 函数将输出变成了 :math:`(0,1)` 范围内的值，将其视为概率。对于多分类问题的标签，我们也可以进行形式上的处理。"
msgstr "crwdns111481:0:math:crwdne111481:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:671
msgid "我们可以将真实的类别标签值 :math:`y` 处理成 One-hot 编码的形式进行表示：只在对应的类别标签位置为 1，其它位置为 0."
msgstr "crwdns111483:0:math:crwdne111483:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:673
msgid "在 MegEngine 的 ``functional`` 模块中，提供了 ``one_hot()`` 方法："
msgstr "crwdns111485:0crwdne111485:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:721
msgid "不难发现，One-hot 编码以向量 :math:`\\mathbf y` 的形式给出了样本属于每一个类别的概率。"
msgstr "crwdns111487:0:math:crwdne111487:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:723
msgid "自然地，我们希望线性分类器最终输出的预测值是一个形状为 :math:`(C,)` 的向量 :math:`\\hat {\\mathbf y}`\\ ，这样方便设计和计算损失函数。"
msgstr "crwdns111489:0:math:crwdnd111489:0:math:crwdne111489:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:725
msgid "我们已经知道线性输出 :math:`\\hat y = f(\\mathbf x; \\mathbf w, b) = \\mathbf x \\cdot \\mathbf w + b \\in \\mathbb R` , 现在我们希望能够得到 :math:`\\hat {\\mathbf y} \\in \\mathbb R^C` 这样的输出;"
msgstr "crwdns111491:0:math:crwdnd111491:0:math:crwdne111491:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:726
msgid "可以设计 :math:`C` 个不同的 :math:`f(\\mathbf x; \\mathbf w, b`) 分别进行计算得到 :math:`C` 个输出，也可以直接利用矩阵 :math:`W` 和向量 :math:`b` 的形式直接计算："
msgstr "crwdns111493:0:math:crwdnd111493:0:math:crwdnd111493:0:math:crwdnd111493:0:math:crwdnd111493:0:math:crwdne111493:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:788
msgid "得到的输出虽然有 :math:`C` 个值了，但仍然不是概率的形式，这时我们希望设计这样一个决策函数："
msgstr "crwdns111495:0:math:crwdne111495:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:790
msgid "通过这个函数，可以将输出的 :math:`C` 个值转换为样本 :math:`\\mathbf{x}` 属于某一类别 :math:`c` 的概率 :math:`p(y=c | \\mathbf{x}) \\in (0,1)`\\ ；"
msgstr "crwdns111497:0:math:crwdnd111497:0:math:crwdnd111497:0{x}crwdnd111497:0:math:crwdnd111497:0:math:crwdnd111497:0{x}crwdne111497:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:791
msgid "满足不同预测标签类别的概率和为 1."
msgstr "crwdns111499:0crwdne111499:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:793
msgid "一个直接的想法是使用 :math:`\\operatorname{ArgMax}` 函数，将值最大位置的概率设置为 1，其它位置为 0"
msgstr "crwdns111501:0:math:crwdnd111501:0{ArgMax}crwdne111501:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:795
msgid "这样也可以得到作为最终预测的 One-hot 编码形式的向量，但问题在于："
msgstr "crwdns111503:0crwdne111503:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:796
msgid "和分段函数类似，此时的 :math:`\\operatorname{ArgMax}` 函数是不可导的，对梯度下降法不友好；"
msgstr "crwdns111505:0:math:crwdnd111505:0{ArgMax}crwdne111505:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:797
msgid "这种处理方式过于简单粗暴，没有考虑到在其它类别上预测的概率值所带来的影响。"
msgstr "crwdns111507:0crwdne111507:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:799
msgid "在 MegEngine 的 ``functional`` 模块中，提供了 ``argmax()`` 方法："
msgstr "crwdns111509:0crwdne111509:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:846
msgid "为了解决上面的问题，人们设计出了 :math:`\\operatorname{Softmax}` 归一化指数函数（也叫 :math:`\\operatorname{SoftArgmax}`, 即柔和版 :math:`\\operatorname{Argmax}`\\ ）："
msgstr "crwdns111511:0:math:crwdnd111511:0{Softmax}crwdnd111511:0:math:crwdnd111511:0{SoftArgmax}crwdnd111511:0:math:crwdnd111511:0{Argmax}crwdne111511:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:848
msgid "p(y=c | \\mathbf {x}) = \\operatorname {Softmax}(y_c)=\\frac{\\exp y_c}{\\sum_{i} \\exp y_i}"
msgstr "crwdns111513:0{x}crwdnd111513:0{Softmax}crwdnd111513:0{i}crwdne111513:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:853
msgid "将线性输出 :math:`y_c = f(\\mathbf x)` 经过指数归一化计算后得到一个概率 :math:`p(y=c | \\mathbf{x})`, 我们通常经过类似处理后得到的分类概率值叫做 Logits."
msgstr "crwdns111515:0:math:crwdnd111515:0:math:crwdnd111515:0{x}crwdne111515:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:854
msgid "由于指数 :math:`e^x` 容易随着输入值 :math:`x` 的变大发生指数爆炸，真正的 :math:`\\operatorname {Softmax}` 实现中还会有一些额外处理来增加数值稳定性，我们不在这里介绍。"
msgstr "crwdns111517:0:math:crwdnd111517:0:math:crwdnd111517:0:math:crwdnd111517:0{Softmax}crwdne111517:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:856
msgid "在 MegEngine 的 ``functional`` 模块中，提供了 ``softmax()`` 方法："
msgstr "crwdns111519:0crwdne111519:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:902
msgid "我们为什么不采用均值归一化或其它的的方法，而要引入指数的形式呢？可以这样解释： - 指数函数具有“马太效应”：我们认为较大的原始值在归一化后得到的概率值也应该更大； - 与 :math:`\\operatorname{ArgMax}` 相比，使用 :math:`\\operatorname{SoftMax}` 还可以找到 Top-K 候选项，即前 :math:`k` 大概率的分类，有利于模型性能评估；"
msgstr "crwdns111521:0:math:crwdnd111521:0{ArgMax}crwdnd111521:0:math:crwdnd111521:0{SoftMax}crwdnd111521:0:math:crwdne111521:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:904
msgid "分类问题中 :math:`\\operatorname{SoftMax}` 函数的选用也和影响了所对应的损失函数的设计，即下面即将介绍的交叉熵。"
msgstr "crwdns111523:0:math:crwdnd111523:0{SoftMax}crwdne111523:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:916
msgid "交叉熵（Cross Entropy）"
msgstr "crwdns111525:0crwdne111525:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:918
msgid "我们已经得到了预测的类别标签向量 :math:`\\mathbf {\\hat{y}}`, 也已经使用 One-hot 编码表示了真实的类别标签向量 :math:`\\mathbf{y}`. 二者各自代表一种概率分布。"
msgstr "crwdns111527:0:math:crwdnd111527:0{y}crwdnd111527:0:math:crwdnd111527:0{y}crwdne111527:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:920
msgid "在信息论中，如果对同一个随机变量 :math:`x` 有两个单独的概率分布 :math:`p(x)` 和 :math:`q(x)`\\ ，可以使用相对熵（KL 散度）来表示两个分布的差异："
msgstr "crwdns111529:0:math:crwdnd111529:0:math:crwdnd111529:0:math:crwdne111529:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:922
msgid "\\begin{aligned}\n"
"\\mathrm{KL}(p \\| q) &=-\\int p(x) \\ln q(x) d x-\\left(-\\int p(x) \\ln p(x) d x\\right) \\\\\n"
"&= H(p,q) - H(p)\n"
"\\end{aligned}"
msgstr "crwdns111531:0{aligned}crwdnd111531:0{KL}crwdnd111531:0{aligned}crwdne111531:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:930
msgid "相对熵的特点是，两个概率分布完全相同时，其值为零。二者分布之间的差异越大，相对熵值越大。"
msgstr "crwdns111533:0crwdne111533:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:931
msgid "由公式可知，\\ :math:`\\mathrm{KL}(p \\| q) \\neq \\mathrm{KL}(q \\| p)`. 不具对称性，所以其表示的不是严格意义上的“距离”，适合分类任务。"
msgstr "crwdns111535:0:math:crwdnd111535:0{KL}crwdnd111535:0{KL}crwdne111535:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:932
msgid "感兴趣的读者可以自行了解信息论中有关熵（Entropy）的概念，目前我们只要知道这些东西能做什么就行。"
msgstr "crwdns111537:0crwdne111537:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:934
msgid "我们希望设计一个损失函数，可用来评估当前训练得到的概率分布 :math:`q(x)`\\ ，与真实分布 :math:`p(x)` 之间有多么大的差异，同时整体要是凸函数。"
msgstr "crwdns111539:0:math:crwdnd111539:0:math:crwdne111539:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:936
msgid "而在训练的过程中，代表真实类别概率的 :math:`p(x)` 固定，是一个确定的常数，其 :math:`H(p)` 值不会随着训练而改变，也不会影响梯度计算，所以可省略。"
msgstr "crwdns111541:0:math:crwdnd111541:0:math:crwdne111541:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:938
msgid "剩下的 :math:`H(p,q)` 部分则被定义为我们常用的交叉熵（Cross Entropy, CE），用我们现在的例子中的离散概率值来表示则为："
msgstr "crwdns111543:0:math:crwdne111543:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:940
msgid "\\ell_{\\operatorname{CE}} = H(\\mathbf{y}, \\hat{\\mathbf{y}})=-\\sum_{i=1}^C y_i \\ln \\hat{y}_i"
msgstr "crwdns111545:0{CE}crwdnd111545:0{y}crwdnd111545:0{y}crwdnd111545:0{y}crwdne111545:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:945
msgid "这即是 Softmax 分类器需要优化的损失函数，对应于 MegEngine 中的 ``cross_entropy()`` 损失函数："
msgstr "crwdns111547:0crwdne111547:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:947
msgid "在 MegEngine 的 ``cross_entropy()`` 函数中，会自动对原始标签 :math:`y` 进行 One-hot 编码得到 :math:`\\mathbf{y}`\\ ，所以 ``pred`` 应该比 ``label`` 多一个 :math:`\\mathbb R^C` 维度；"
msgstr "crwdns111549:0:math:crwdnd111549:0:math:crwdnd111549:0{y}crwdnd111549:0:math:crwdne111549:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:948
msgid "设置 ``with_logits=True`` 时，将使用 Softmax 函数把分类输出标准化成概率分布，下面的代码示例中 ``pred`` 已经为概率分布的形式；"
msgstr "crwdns111551:0crwdne111551:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:949
msgid "二分类问题使用的 Sigmoid 函数其实是 Sotfmax 函数的一个特例（读者可尝试证明），对应 MegEngine 中的 ``binary_cross_entropy()`` 方法。"
msgstr "crwdns111553:0crwdne111553:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1012
msgid "我们还可以发现，使用 Softmax + Cross Entropy 结合的形式，二者的 :math:`\\exp` 和 :math:`\\ln` 计算一定程度上可以相互抵消，简化计算流程。"
msgstr "crwdns111555:0:math:crwdnd111555:0:math:crwdne111555:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1014
msgid "现在我们找到了 MNIST 手写图像分类任务的决策函数和损失函数，是时候尝试自己利用 MegEngine 构建一个线性分类器了。"
msgstr "crwdns111557:0crwdne111557:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1017
msgid "练习：线性分类"
msgstr "crwdns111559:0crwdne111559:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1019
msgid "我们使用 MegEngine 对线性分类器进行实现："
msgstr "crwdns111561:0crwdne111561:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1140
msgid "接着我们要实现测试部分，分类问题可使用预测精度（Accuracy）来评估模型性能，即被正确预测的比例："
msgstr "crwdns111563:0crwdne111563:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1216
msgid "这意味着我们只训练了 5 个周期的线性分类器，在测试集的 10,000 张图片中，就有 9,170 张图片被正确地预测了分类。"
msgstr "crwdns111565:0crwdne111565:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1218
msgid "你可以尝试增大上方的 ``epochs`` 超参数，最终结果不会提升很多，说明我们现在所实现的线性分类器存在一定的局限性，需要寻找更好的方法。"
msgstr "crwdns111567:0crwdne111567:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1230
msgid "总结回顾"
msgstr "crwdns111569:0crwdne111569:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1232
msgid "我们依据对线性回归的认知发展衍生出了线性分类器，并完成了 MNIST 手写数字识别任务，请回忆一下整个探索的过程。"
msgstr "crwdns111571:0crwdne111571:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1234
msgid "理解机器学习知识的角度有很多种，不同的角度之间存在着千丝万缕的联系："
msgstr "crwdns111573:0crwdne111573:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1236
msgid "预测（Predict）：线性回归输出的标量是 :math:`\\mathbb R` 连续值，使用 Sigmoid 将其映射到 :math:`(0, 1)`, 而 Softmax 可将 :math:`C` 个输出值变成对应分类上的概率；"
msgstr "crwdns111575:0:math:crwdnd111575:0:math:crwdnd111575:0:math:crwdne111575:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1237
msgid "优化（Optimize）：线性回归模型通常选择均方误差（MSE）作为损失（目标）函数，而线性分类模型通常使用交叉熵（CE）；"
msgstr "crwdns111577:0crwdne111577:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1238
msgid "评估（Evaluate）：线性回归模型可以选者平均误差或总误差作为评估指标，而线性分类模型可以使用精度（Accuracy）；"
msgstr "crwdns111579:0crwdne111579:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1240
msgid "对于特征的处理：我们接触了计算机领域的灰度栅格图，并尝试使用 ``flatten()`` 方法将每个样本的高维特征处理为特征向量。"
msgstr "crwdns111581:0crwdne111581:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1252
msgid "问题思考"
msgstr "crwdns111583:0crwdne111583:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1254
msgid "旧问题的解决往往伴随着新问题的诞生，让我们一起来思考一下："
msgstr "crwdns111585:0crwdne111585:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1256
msgid "对于同样的任务（如 MNIST 手写数字识别），我们可以选用不同的模型和算法（比如 K 近邻算法）来解决，我们只接触了机器学习算法的冰山一角；"
msgstr "crwdns111587:0crwdne111587:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1257
msgid "对于具有 HWC 属性的图像，教程中展平成特征向量的处理方式似乎有些直接（比如忽视掉了像素点在平面空间上的临接性），是否有更好的方法？"
msgstr "crwdns111589:0crwdne111589:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1258
msgid "我们在处理分类问题的输出时使用了 Softmax 达到了归一化的效果，那么对于输入数据的特征，是否也可以进行归一化呢？"
msgstr "crwdns111591:0crwdne111591:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1259
msgid "MegEngine 的自动求导机制帮助我们省掉了很多计算，可以尝试推导 Softmax + Cross Entropy 的反向传播过程，感受框架的便捷之处。"
msgstr "crwdns111593:0crwdne111593:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1261
msgid "我们已经训练了一个识别手写数字的分类器，可以尝试用它在一些实际的数据上进行测试（而不仅仅是官方提供的测试集），可以尝试写代码测试一下："
msgstr "crwdns111595:0crwdne111595:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1263
msgid "如何将常见的 jpg, png, gif 等图像格式处理成 NumPy 的 ndarray 格式？（提示：可使用 OpenCV 或 Pillow 等库）"
msgstr "crwdns111597:0crwdne111597:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1264
msgid "我想要测试的图像的长和宽和 MNIST 所使用的 :math:`28 \\times 28` 形状不一致，此时要如何处理？"
msgstr "crwdns111599:0:math:crwdne111599:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1265
msgid "我想要测试的图像是一张三通道的彩色图片，MNIST 数据集中是黑白图，此时又要如何处理？"
msgstr "crwdns111601:0crwdne111601:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1267
msgid "你有思路了吗？我们在本次教程结尾的地方进行了一项有趣的测试。"
msgstr "crwdns111603:0crwdne111603:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1279
msgid "机器学习背后的数学知识"
msgstr "crwdns111605:0crwdne111605:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1281
msgid "MegEngine 教程内容以相关代码实践为主，为了避免引入过多的理论导致“学究”，我们对一些数学细节的介绍比较笼统，感兴趣读者可进行拓展性的阅读："
msgstr "crwdns111607:0crwdne111607:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1283
msgid "斯坦福 `CS 229 <http://cs229.stanford.edu/>`__ 是非常好的机器学习课程，可以在课程主页找到有关广义线性模型、指数族分布和最大熵模型的讲义"
msgstr "crwdns111609:0crwdne111609:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1284
msgid "对于线性回归问题的最小二乘估计，可以尝试推导正规方程得到其解析解（Analytical solution）或者说闭式解（Closed-form solution）"
msgstr "crwdns111611:0crwdne111611:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1285
msgid "对于均方误差（MSE）和交叉熵（CE）的使用，可以用使用极大似然估计（Maximum likelihood estimation）进行推导和解释"
msgstr "crwdns111613:0crwdne111613:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1286
msgid "对支持向量机（Support Vector Machine）和感知机模型（Perceptron）等经典二分类模型，属于机器学习领域，本教程中也没有进行介绍"
msgstr "crwdns111615:0crwdne111615:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1288
msgid "至于统计机器学习的理论解释，也分为频率派和贝叶斯派两大类，二者探讨「不确定性」这件事时的出发点与立足点不同，均作了解有助于拓宽视野。"
msgstr "crwdns111617:0crwdne111617:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1290
msgid "对于新知识，我们提倡先建立直觉性的解释，搞清楚它如何生效（How it works），优缺点在哪里（Why we use it），人们如何定义（What is it），从而建立共同的认知；"
msgstr "crwdns111619:0crwdne111619:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1291
msgid "当你不断进步，达到当前阶段的认知边界后，自然会对更深层次的解释好奇，这个时候不妨利用数学的严谨性探索一番，最终恍然大悟：“原来如此。”；"
msgstr "crwdns111621:0crwdne111621:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1292
msgid "数学的严谨性让人赞叹，对于解释性不够强的理论，我们要勇于质疑，大胆探索，就像前人从地心说到日心说所付出的一样，不懈追求。"
msgstr "crwdns111623:0crwdne111623:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1294
msgid "在机器学习领域，线性模型比较适合初学者入门，过渡到神经网络模型也比较自然，接下来我们就要打开深度学习的大门。"
msgstr "crwdns111625:0crwdne111625:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1306
msgid "线性模型就够了吗？"
msgstr "crwdns111627:0crwdne111627:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1308
msgid "既然线性模型在分类任务上可以取得如此好的准确率，是不是意味着线性模型就能够解决所有的问题呢？"
msgstr "crwdns111629:0crwdne111629:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1310
msgid "我们将前面使用的 MNIST 数据集换成 CIFAR10 数据集试一下（只改动了数据集处的代码），看看是否有一样的效果："
msgstr "crwdns111631:0crwdne111631:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1446
msgid "有没有感觉到一丝疑惑呢？之前大显神通的线性分类器，为何在换用了 CIFAR10 数据集后就判若两人了？随机猜测的正确率也会有 10% 的啊！"
msgstr "crwdns111633:0crwdne111633:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1448
msgid "此时可以给你一些信息："
msgstr "crwdns111635:0crwdne111635:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1450
msgid "在 CIFAR10 数据集中，一共有 10 类共 60000 张 32x32 **彩色** 图像，每类有 6000 张图片，其中 50000 个训练集，10000 张为测试集。"
msgstr "crwdns111637:0crwdne111637:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1452
msgid "容易发现，将数据集换用后，图像的颜色由黑白变为了彩色，这意味着图像的通道数量由原本灰度图的 1 变成了由 RGB 表示的 3 通道图。"
msgstr "crwdns111639:0crwdne111639:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1453
msgid "还记得我们对图像进行的 ``flatten`` 操作吗？这个特征处理操作对于简单的灰度图而言尚可接受，可用在 RGB 图像身上，似乎更加不合理了。"
msgstr "crwdns111641:0crwdne111641:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1454
msgid "另外，与 MNIST 数据集中全是黑底白字的手写数字不同，CIFAR10 数据集中有着包括 airplane(飞机)，automobile（汽车），bird（鸟），cat（猫），deer（鹿），dog（狗），frog（青蛙），horse（马），ship（船）和 truck（卡车）在内的 10 类物品，图像中蕴含的信息将更加复杂。"
msgstr "crwdns111643:0crwdne111643:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1456
msgid "我想你现在应该发现了：想要用线性模型来处理稍微复杂一些的图片分类任务，就开始显得力不从心了。此时，你会尝试怎么做呢？"
msgstr "crwdns111645:0crwdne111645:0"

#: ../../source/getting-started/beginner/from-linear-regression-to-linear-classification.ipynb:1458
msgid "深度学习，简单开发。我们鼓励你在实践中不断思考，并启发自己去探索直觉性或理论性的解释。"
msgstr "crwdns111647:0crwdne111647:0"

#~ msgid ""
#~ "|image0| `在官网查看 <https://megengine.org.cn/doc/stable/zh"
#~ "/getting-started/beginner/from-linear-regression-"
#~ "to-linear-classification.html>`__"
#~ msgstr ""

#~ msgid "|image1| `在 MegStudio 运行 <https://studio.brainpp.com/project/4643>`__"
#~ msgstr ""

#~ msgid ""
#~ "|image2| `在 GitHub 查看 "
#~ "<https://github.com/MegEngine/Documentation/blob/main/source"
#~ "/getting-started/beginner/from-linear-regression-"
#~ "to-linear-classification.ipynb>`__"
#~ msgstr ""

#~ msgid "起初阶段大致是指数增长；然后随着开始变得饱和，增加变慢最后，达到成熟时增加停止。"
#~ msgstr ""
#~ "The initial stage is roughly exponential"
#~ " growth; then as it begins to "
#~ "become saturated, the increase slows "
#~ "down and finally, the increase stops "
#~ "when it reaches maturity."

#~ msgid "以下是 Verhulst 对命名的解释："
#~ msgstr "The following is an explanation of the naming Verhulst："

