msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-06-03 10:50+0800\n"
"PO-Revision-Date: 2021-07-07 18:38\n"
"Last-Translator: \n"
"Language-Team: Chinese Traditional\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-TW\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/zh_CN/LC_MESSAGES/reference/api/megengine.core.tensor.megbrain_graph.optimize_for_inference.po\n"
"X-Crowdin-File-ID: 6900\n"
"Language: zh_TW\n"

#: ../../source/reference/api/megengine.core.tensor.megbrain_graph.optimize_for_inference.rst:2
msgid "megengine.core.tensor.megbrain\\_graph.optimize\\_for\\_inference"
msgstr "crwdns62516:0crwdne62516:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:1 of
msgid "Applies optimize_for_inference pass for computing graph."
msgstr "crwdns62518:0crwdne62518:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference of
msgid "param dest_vars"
msgstr "crwdns62520:0crwdne62520:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:3 of
msgid "list of output vars in the computing graph"
msgstr "crwdns62522:0crwdne62522:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference of
msgid "Keyword Arguments"
msgstr "crwdns62524:0crwdne62524:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:9 of
msgid "enable_io16xc32 --"
msgstr "crwdns62526:0crwdne62526:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:8 of
msgid "whether to use float16 for I/O between oprs and use float32 as internal computation precision. Note the output var would be changed to float16."
msgstr "crwdns62528:0crwdne62528:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:13 of
msgid "enable_ioc16 --"
msgstr "crwdns62530:0crwdne62530:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:12 of
msgid "whether to use float16 for both I/O and computation precision."
msgstr "crwdns62532:0crwdne62532:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:16 of
msgid "enable_hwcd4 --"
msgstr "crwdns62534:0crwdne62534:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:16 of
msgid "whether to use NHWCD4 data layout. This is faster on some OpenCL backend."
msgstr "crwdns62536:0crwdne62536:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:19 of
msgid "enable_nchw88 --"
msgstr "crwdns62538:0crwdne62538:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:19 of
msgid "whether to use NCHW88 data layout, currently used in X86 AVX backend."
msgstr "crwdns62540:0crwdne62540:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:22 of
msgid "enable_nchw44 --"
msgstr "crwdns62542:0crwdne62542:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:22 of
msgid "whether to use NCHW44 data layout, currently used in arm backend."
msgstr "crwdns62544:0crwdne62544:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:25 of
msgid "enable_nchw44_dot --"
msgstr "crwdns62546:0crwdne62546:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:25 of
msgid "whether to use NCHW44_dot data layout, currently used in armv8.2+dotprod backend."
msgstr "crwdns62548:0crwdne62548:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:28 of
msgid "enable_nchw4 --"
msgstr "crwdns62550:0crwdne62550:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:28 of
msgid "whether to use NCHW4 data layout, currently used in nvidia backend(based on cudnn)."
msgstr "crwdns62552:0crwdne62552:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:31 of
msgid "enable_nchw32 --"
msgstr "crwdns62554:0crwdne62554:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:31 of
msgid "whether to use NCHW32 data layout, currently used in nvidia backend with tensorcore(based on cudnn)."
msgstr "crwdns62556:0crwdne62556:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:35 of
msgid "enable_chwn4 --"
msgstr "crwdns62558:0crwdne62558:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:34 of
msgid "whether to use CHWN4 data layout, currently used in nvidia backend with tensorcore."
msgstr "crwdns62560:0crwdne62560:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:37 of
msgid "enable_fuse_conv_bias_nonlinearity: whether to fuse conv+bias+nonlinearty"
msgstr "crwdns62562:0crwdne62562:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:38 of
msgid "into one opr."
msgstr "crwdns62564:0crwdne62564:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:41 of
msgid "enable_fuse_conv_bias_with_z: whether to fuse conv_bias with z"
msgstr "crwdns62566:0crwdne62566:0"

#: megengine.core.tensor.megbrain_graph.optimize_for_inference:40 of
msgid "input for inference on nvidia backend(this optimization pass will result in mismatch of the precision of output of training and inference)"
msgstr "crwdns62568:0crwdne62568:0"

