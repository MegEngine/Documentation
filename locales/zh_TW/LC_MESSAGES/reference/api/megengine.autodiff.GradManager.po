msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-06-03 10:50+0800\n"
"PO-Revision-Date: 2021-07-07 10:51\n"
"Last-Translator: \n"
"Language-Team: Chinese Traditional\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-TW\n"
"X-Crowdin-File: /[MegEngine.Documentation] main/locales/zh_CN/LC_MESSAGES/reference/api/megengine.autodiff.GradManager.po\n"
"X-Crowdin-File-ID: 6588\n"
"Language: zh_TW\n"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:2
msgid "megengine.autodiff.GradManager"
msgstr "crwdns61504:0crwdne61504:0"

#: megengine.autodiff.grad_manager.GradManager:1 of
msgid "GradManager computes gradients or more generally, vector-Jacobian product, by reverse mode automatic differentiation (a.k.a. back propagation)."
msgstr "crwdns61506:0crwdne61506:0"

#: megengine.autodiff.grad_manager.GradManager:4 of
msgid "Reverse mode autodiff normally reuses many intermediate tensors for best computation efficiency. In a read-eval-print-loop (REPL) environment however, it is impossible to known how the user would take gradients later thus which tensors to keep. To solve this problem, the user must somehow declare beforehand which gradient could possibly be taken. With GradManager, users are required to call the :meth:`attach` method on a tensor if they want to take gradients with respect to it later. Furthermore, any computation on a tensor before it is attached is completely ignored from the autodiff perspective, so :meth:`attach` must be called before any computation that needs differentiation."
msgstr "crwdns61508:0:meth:crwdnd61508:0:meth:crwdne61508:0"

#: megengine.autodiff.grad_manager.GradManager:13 of
msgid "For example, the following symbolic differentiation code"
msgstr "crwdns61510:0crwdne61510:0"

#: megengine.autodiff.grad_manager.GradManager:22 of
msgid "can be rewriten using GradManager for REPL environment as"
msgstr "crwdns61512:0crwdne61512:0"

#: megengine.autodiff.grad_manager.GradManager:34 of
msgid "A more realistic example of training a neural network would be like"
msgstr "crwdns61514:0crwdne61514:0"

#: megengine.autodiff.grad_manager.GradManager:47 of
msgid "You can also use ``record()`` and ``release()`` method instead of ``with`` context:"
msgstr "crwdns61516:0crwdne61516:0"

#: megengine.autodiff.grad_manager.GradManager:62 of
msgid "For your convenience, GradManager may (not must) be reused. As shown in the examples, you only need to attach a tensor once and GradManager will remember it afterwards. However, a single GradManager can record only one computation history at a time. To run multiple differentiations simultaneously or perform high order differentiation, create as many GradManager as you need."
msgstr "crwdns61518:0crwdne61518:0"

#: megengine.autodiff.grad_manager.GradManager:70 of
msgid "Mutable tensors introduce ambiguities when doing symbolic differentiation: which version of the tensor are we referring to? For attached tensors, GradManager resolves this ambiguity by \"snapshoting\" them on first encounter, either on :meth:`record` (or entering with statement) if tensor is attached before :meth:`record`, or on :meth:`attach` if GradManager is already recording. Attached tensors will then be interpreted as their snapshotted version for differentiation purpose. The same ambiguity on the first parameter of :meth:`backward` is simply resolved by using the latest version."
msgstr "crwdns61520:0:meth:crwdnd61520:0:meth:crwdnd61520:0:meth:crwdnd61520:0:meth:crwdne61520:0"

#: megengine.autodiff.grad_manager.GradManager:78 of
msgid "Typically, in data parallel, we would like to average the gradients across processes. Users will finally get the averaged gradients if an \"AllReduce\" callback is registered as follows:"
msgstr "crwdns61522:0crwdne61522:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:15
msgid "Methods"
msgstr "crwdns61524:0crwdne61524:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:30:<autosummary>:1
msgid ":obj:`attach <megengine.autodiff.GradManager.attach>`\\ \\(tensors\\[\\, callbacks\\]\\)"
msgstr "crwdns61526:0:obj:crwdne61526:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:30:<autosummary>:1
msgid "Instruct GradManager to track operations on tensors, so that gradients with respect to those tensors could be evaluated later."
msgstr "crwdns61528:0crwdne61528:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:30:<autosummary>:1
msgid ":obj:`backward <megengine.autodiff.GradManager.backward>`\\ \\(\\[y\\, dy\\]\\)"
msgstr "crwdns61530:0:obj:crwdne61530:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:30:<autosummary>:1
msgid "Compute gradients (or vector-Jacobian product) for all attached tensors, accumulate to corresponding .grad attribute, and release resources along the way."
msgstr "crwdns61532:0crwdne61532:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:30:<autosummary>:1
msgid ":obj:`record <megengine.autodiff.GradManager.record>`\\ \\(\\)"
msgstr "crwdns61534:0:obj:crwdne61534:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:30:<autosummary>:1
msgid "Start recording operations"
msgstr "crwdns61536:0crwdne61536:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:30:<autosummary>:1
msgid ":obj:`release <megengine.autodiff.GradManager.release>`\\ \\(\\)"
msgstr "crwdns61538:0:obj:crwdne61538:0"

#: ../../source/reference/api/megengine.autodiff.GradManager.rst:30:<autosummary>:1
msgid "Stop recording operations and release resources kept for gradient computation"
msgstr "crwdns61540:0crwdne61540:0"

