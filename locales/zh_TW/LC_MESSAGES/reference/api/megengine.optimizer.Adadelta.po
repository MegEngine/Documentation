msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-09-21 10:50+0000\n"
"PO-Revision-Date: 2023-09-21 10:56\n"
"Last-Translator: \n"
"Language: zh_TW\n"
"Language-Team: Chinese Traditional\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-TW\n"
"X-Crowdin-File: /dev/locales/zh_CN/LC_MESSAGES/reference/api/megengine.optimizer.Adadelta.po\n"
"X-Crowdin-File-ID: 9639\n"

#: ../../source/reference/api/megengine.optimizer.Adadelta.rst:5
msgid "Adadelta"
msgstr "crwdns105623:0crwdne105623:0"

#: megengine.optimizer.adadelta.Adadelta:1 of
msgid "Implements Adadelta algorithm proposed in `\"ADADELTA: An Adaptive Learning Rate Method\" <https://arxiv.org/abs/1212.5701>`_."
msgstr "crwdns122445:0crwdne122445:0"

#: megengine.optimizer.adadelta.Adadelta of
msgid "参数"
msgstr "crwdns105629:0crwdne105629:0"

#: megengine.optimizer.adadelta.Adadelta:4 of
msgid "iterable of parameters to optimize or dicts defining parameter groups."
msgstr "crwdns105631:0crwdne105631:0"

#: megengine.optimizer.adadelta.Adadelta:8 of
msgid "coefficient that scales delta before it is applied to the parameters. Default: 1.0."
msgstr "crwdns122447:0crwdne122447:0"

#: megengine.optimizer.adadelta.Adadelta:12 of
msgid "coefficient used for computing a running average of squared gradients. Default: 0.9."
msgstr "crwdns122449:0crwdne122449:0"

#: megengine.optimizer.adadelta.Adadelta:16 of
msgid "term added to the denominator to improve numerical stability. Default: 1e-6."
msgstr "crwdns122451:0crwdne122451:0"

#: megengine.optimizer.adadelta.Adadelta:20 of
msgid "weight decay (L2 penalty). Default: 0."
msgstr "crwdns122453:0crwdne122453:0"

#: megengine.optimizer.adadelta.Adadelta of
msgid "返回"
msgstr "crwdns122455:0crwdne122455:0"

#: megengine.optimizer.adadelta.Adadelta:23 of
msgid "An instance of the Adadelta optimizer."
msgstr "crwdns122457:0crwdne122457:0"

#~ msgid "基类：:class:`megengine.optimizer.optimizer.Optimizer`"
#~ msgstr "基类：:class:`megengine.optimizer.optimizer.Optimizer`"

#~ msgid ""
#~ ":obj:`__init__ <megengine.optimizer.Adadelta.__init__>`\\ "
#~ "\\(params\\[\\, lr\\, rho\\, eps\\, "
#~ "weight\\_decay\\]\\)"
#~ msgstr ""
#~ ":obj:`__init__ <megengine.optimizer.Adadelta.__init__>`\\ "
#~ "\\(params\\[\\, lr\\, rho\\, eps\\, "
#~ "weight\\_decay\\]\\)"

#~ msgid "Initialize self."
#~ msgstr "初始化方法。"

#~ msgid "megengine.optimizer.Adadelta"
#~ msgstr "megengine.optimizer.Adadelta"

#~ msgid "Methods"
#~ msgstr "方法"

#~ msgid ""
#~ ":obj:`add_param_group "
#~ "<megengine.optimizer.Adadelta.add_param_group>`\\ "
#~ "\\(param\\_group\\)"
#~ msgstr ""
#~ ":obj:`add_param_group "
#~ "<megengine.optimizer.Adadelta.add_param_group>`\\ "
#~ "\\(param\\_group\\)"

#~ msgid ""
#~ "Add a param group to ``param_groups``"
#~ " of the :class:`~megengine.optim.optimizer.Optimizer`."
#~ msgstr ""
#~ "向 :class:`~megengine.optim.optimizer.Optimizer` 的 "
#~ "``param_groups`` 中添加一组参数。"

#~ msgid ":obj:`backward <megengine.optimizer.Adadelta.backward>`\\ \\(loss\\)"
#~ msgstr ":obj:`backward <megengine.optimizer.Adadelta.backward>`\\ \\(loss\\)"

#~ msgid ":obj:`bcast_param <megengine.optimizer.Adadelta.bcast_param>`\\ \\(\\)"
#~ msgstr ":obj:`bcast_param <megengine.optimizer.Adadelta.bcast_param>`\\ \\(\\)"

#~ msgid ":obj:`clear_grad <megengine.optimizer.Adadelta.clear_grad>`\\ \\(\\)"
#~ msgstr ":obj:`clear_grad <megengine.optimizer.Adadelta.clear_grad>`\\ \\(\\)"

#~ msgid "Set the grad attribute to None for all parameters."
#~ msgstr "把所有参数的梯度属性设置为 None。"

#~ msgid ""
#~ ":obj:`load_state_dict "
#~ "<megengine.optimizer.Adadelta.load_state_dict>`\\ \\(state\\)"
#~ msgstr ""
#~ ":obj:`load_state_dict "
#~ "<megengine.optimizer.Adadelta.load_state_dict>`\\ \\(state\\)"

#~ msgid "Loads the optimizer state."
#~ msgstr "加载优化器状态。"

#~ msgid ""
#~ ":obj:`state_dict <megengine.optimizer.Adadelta.state_dict>`\\"
#~ " \\(\\[keep\\_var\\]\\)"
#~ msgstr ""
#~ ":obj:`state_dict <megengine.optimizer.Adadelta.state_dict>`\\"
#~ " \\(\\[keep\\_var\\]\\)"

#~ msgid "Export the optimizer state."
#~ msgstr "导出优化器状态。"

#~ msgid ":obj:`step <megengine.optimizer.Adadelta.step>`\\ \\(\\)"
#~ msgstr ":obj:`step <megengine.optimizer.Adadelta.step>`\\ \\(\\)"

#~ msgid "Performs a single optimization step."
#~ msgstr "执行单一优化步骤。"

#~ msgid ":obj:`zero_grad <megengine.optimizer.Adadelta.zero_grad>`\\ \\(\\)"
#~ msgstr ":obj:`zero_grad <megengine.optimizer.Adadelta.zero_grad>`\\ \\(\\)"

#~ msgid "Implements Adadelta algorithm."
#~ msgstr "实现Adadelta算法。"

