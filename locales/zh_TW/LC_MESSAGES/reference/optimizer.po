msgid ""
msgstr ""
"Project-Id-Version: megengine\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-09-21 10:50+0000\n"
"PO-Revision-Date: 2023-09-21 10:57\n"
"Last-Translator: \n"
"Language: zh_TW\n"
"Language-Team: Chinese Traditional\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: megengine\n"
"X-Crowdin-Project-ID: 450980\n"
"X-Crowdin-Language: zh-TW\n"
"X-Crowdin-File: /dev/locales/zh_CN/LC_MESSAGES/reference/optimizer.po\n"
"X-Crowdin-File-ID: 9807\n"

#: ../../source/reference/optimizer.rst:6
msgid "megengine.optimizer"
msgstr "crwdns110515:0crwdne110515:0"

#: ../../source/reference/optimizer.rst:15:<autosummary>:1
msgid ":py:obj:`Optimizer <megengine.optimizer.Optimizer>`"
msgstr "crwdns110517:0crwdne110517:0"

#: ../../source/reference/optimizer.rst:15:<autosummary>:1
msgid "Base class for all optimizers."
msgstr "crwdns110519:0crwdne110519:0"

#: ../../source/reference/optimizer.rst:17
msgid "常见优化器"
msgstr "crwdns110521:0crwdne110521:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid ":py:obj:`SGD <megengine.optimizer.SGD>`"
msgstr "crwdns110523:0crwdne110523:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid "Implements stochastic gradient descent."
msgstr "crwdns110525:0crwdne110525:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid ":py:obj:`AdamW <megengine.optimizer.AdamW>`"
msgstr "crwdns110527:0crwdne110527:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid "Implements the AdamW algorithm proposed in `\"Decoupled Weight Decay Regularization\" <https://arxiv.org/abs/1711.05101>`_."
msgstr "crwdns122601:0crwdne122601:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid ":py:obj:`Adam <megengine.optimizer.Adam>`"
msgstr "crwdns110531:0crwdne110531:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid "Implements Adam algorithm proposed in `\"Adam: A Method for Stochastic Optimization\" <https://arxiv.org/abs/1412.6980>`_."
msgstr "crwdns110533:0crwdne110533:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid ":py:obj:`Adagrad <megengine.optimizer.Adagrad>`"
msgstr "crwdns110535:0crwdne110535:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid "Implements Adagrad algorithm proposed in `\"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\" <http://jmlr.org/papers/v12/duchi11a.html>`_."
msgstr "crwdns122603:0crwdne122603:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid ":py:obj:`Adadelta <megengine.optimizer.Adadelta>`"
msgstr "crwdns110539:0crwdne110539:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid "Implements Adadelta algorithm proposed in `\"ADADELTA: An Adaptive Learning Rate Method\" <https://arxiv.org/abs/1212.5701>`_."
msgstr "crwdns122605:0crwdne122605:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid ":py:obj:`LAMB <megengine.optimizer.LAMB>`"
msgstr "crwdns110543:0crwdne110543:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid "Implements LAMB algorithm."
msgstr "crwdns110545:0crwdne110545:0"

#: ../../source/reference/optimizer.rst:29:<autosummary>:1
msgid ":py:obj:`LAMBFp16 <megengine.optimizer.LAMBFp16>`"
msgstr "crwdns110547:0crwdne110547:0"

#: ../../source/reference/optimizer.rst:31
msgid "学习率调整"
msgstr "crwdns110549:0crwdne110549:0"

#: ../../source/reference/optimizer.rst:38:<autosummary>:1
msgid ":py:obj:`LRScheduler <megengine.optimizer.LRScheduler>`"
msgstr "crwdns110551:0crwdne110551:0"

#: ../../source/reference/optimizer.rst:38:<autosummary>:1
msgid "Base class for all learning rate based schedulers."
msgstr "crwdns110553:0crwdne110553:0"

#: ../../source/reference/optimizer.rst:38:<autosummary>:1
msgid ":py:obj:`MultiStepLR <megengine.optimizer.MultiStepLR>`"
msgstr "crwdns110555:0crwdne110555:0"

#: ../../source/reference/optimizer.rst:38:<autosummary>:1
msgid "Decays the learning rate of each parameter group by gamma once the"
msgstr "crwdns110557:0crwdne110557:0"

#: ../../source/reference/optimizer.rst:40
msgid "梯度处理"
msgstr "crwdns110559:0crwdne110559:0"

#: ../../source/reference/optimizer.rst:47:<autosummary>:1
msgid ":py:obj:`clip_grad_norm <megengine.optimizer.clip_grad_norm>`"
msgstr "crwdns110561:0crwdne110561:0"

#: ../../source/reference/optimizer.rst:47:<autosummary>:1
msgid "Clips gradient norm of an iterable of parameters."
msgstr "crwdns110563:0crwdne110563:0"

#: ../../source/reference/optimizer.rst:47:<autosummary>:1
msgid ":py:obj:`clip_grad_value <megengine.optimizer.clip_grad_value>`"
msgstr "crwdns110565:0crwdne110565:0"

#: ../../source/reference/optimizer.rst:47:<autosummary>:1
msgid "Clips gradient of an iterable of parameters to a specified lower and upper."
msgstr "crwdns110567:0crwdne110567:0"

#~ msgid ":obj:`Optimizer.step <megengine.optimizer.Optimizer.step>`"
#~ msgstr ":obj:`Optimizer.step <megengine.optimizer.Optimizer.step>`"

#~ msgid "Performs a single optimization step."
#~ msgstr "执行单一优化步骤。"

#~ msgid ":obj:`Optimizer.clear_grad <megengine.optimizer.Optimizer.clear_grad>`"
#~ msgstr ":obj:`Optimizer.clear_grad <megengine.optimizer.Optimizer.clear_grad>`"

#~ msgid "Set the grad attribute to None for all parameters."
#~ msgstr "把所有参数的梯度属性设置为 None。"

#~ msgid ""
#~ ":obj:`Optimizer.add_param_group "
#~ "<megengine.optimizer.Optimizer.add_param_group>`"
#~ msgstr ""
#~ ":obj:`Optimizer.add_param_group "
#~ "<megengine.optimizer.Optimizer.add_param_group>`"

#~ msgid ""
#~ "Add a param group to ``param_groups``"
#~ " of the :class:`~megengine.optim.optimizer.Optimizer`."
#~ msgstr ""
#~ "向 :class:`~megengine.optim.optimizer.Optimizer` 的 "
#~ "``param_groups`` 中添加一组参数。"

#~ msgid ":obj:`Optimizer.state_dict <megengine.optimizer.Optimizer.state_dict>`"
#~ msgstr ":obj:`Optimizer.state_dict <megengine.optimizer.Optimizer.state_dict>`"

#~ msgid "Export the optimizer state."
#~ msgstr "导出优化器状态。"

#~ msgid ""
#~ ":obj:`Optimizer.load_state_dict "
#~ "<megengine.optimizer.Optimizer.load_state_dict>`"
#~ msgstr ""
#~ ":obj:`Optimizer.load_state_dict "
#~ "<megengine.optimizer.Optimizer.load_state_dict>`"

#~ msgid "Loads the optimizer state."
#~ msgstr "加载优化器状态。"

#~ msgid "优化器（Optimizer）"
#~ msgstr "优化器（Optimizer）"

#~ msgid ":obj:`SGD <megengine.optimizer.SGD>`"
#~ msgstr ":obj:`SGD <megengine.optimizer.SGD>`"

#~ msgid ":obj:`AdamW <megengine.optimizer.AdamW>`"
#~ msgstr ":obj:`AdamW <megengine.optimizer.AdamW>`"

#~ msgid "Implements Adagrad algorithm."
#~ msgstr "实现Adagrad算法。"

#~ msgid "Implements Adadelta algorithm."
#~ msgstr "实现Adadelta算法。"

